"metadata__created_at","metadata__modified_at","metadata__owner","metadata__path","metadata__seen_at","rets__001"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","inal CLIP visual encoder. Similarly, CogAgent [36] and\nMini-Gemini [48] also separate HR and LR images using\ndistinct vision encoders, subsequently merging their fea-\ntures using a cross-attention module. In contrast, our ap-\nproach offers a more simplified solution and shows advan-\ntages for varying resolutions and aspect ratio inputs. (2)\nCropped image patches [37, 46, 50, 51, 59, 99, 101]. For\nexample, Monkey [50] employs sliding windows to seg-\nment images into patches, subsequently processing them\nwith LoRA fine-tuning. TextMonkey [59] further proposes\nshifted window attention and token resampler to consider\nthe connections among different patches. These approaches\nare confined to either a few predefined high-resolution set-\ntings [36, 46, 48, 50, 51, 55, 59, 66, 97] or a limited range of\nresolutions [37, 99]. Conversely, our method devises a dy-\nnamic image partition strategy to support the scaling from\n336 pixels to 4K resolution, and the maximum resolution is\nlarger than previous approaches (*e.g*., 1.5k for Monkey [50]\nand 2k for UReader [101])."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,\nJiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi\nWang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong\nXiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,\nXiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing\nYu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,\nPeng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\nWenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue\nZhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe\nZhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng\nQiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024. 1, 2<br><br>[11] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Ji-\naqi Wang. DualFocus: Integrating macro and micro per-\nspectives in multi-modal large language models. *arXiv*\n*preprint arXiv:2402.14767*, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","90\n85\n80\n75\n70\n65\n60\n55\n50\n*\n*Val*\n*Val*\n*Val*\n*Test*\n*Test*\n*EN-Test*\nInfoVQA\nDocVQA\nTextVQA\nChartQA\nMMBench\nMME\nSeed\nAI2D\nHD-9 (1561 Tokens)\nHD-16 (2653 Tokens)\nHD-25 (4057 Tokens)\n4K HD (8737 Tokens)<br><br>Figure 5. **Influence of Training Resolution. **High-resolution training is critical for HD-OCR tasks, while its gain on other tasks is minor."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","here*pw*and*ph*represent the number of patches in each row\nand column, respectively. We then split the ˆ*x*into*ph ×*\n*pw*non-overlapped patches. Each patch is a small image\nwith 336*×*336 size and we treat these patches as individual\ninputs for the ViT. In the following, we use ‘HD-*H*’ to represent our high-\nresolution setting with the constraint of*H*patches. For ex-\nample, the ’HD-9’ allows up to 9 patches, including a range\nof resolutions such as 1008*×*1008, 672*×*1344, 336*×*3024,\n*etc*. **Global-Local Format. **For each input image, we present it\nto the model with two views. The first is the global view,\nwhere the image is resized to a fixed size (in our case, 336\n× 336). This provides a macro understanding of the image. Empirically, we have found this to be crucial for the LVLM\nto correctly understand the image."
"","","","","","The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","der a similar model scale. As shown in Table 4, our\nmodel significantly outperforms existing open-source mod-\nels, achieving competitive results across all benchmarks. Notably, the InternLM-XComposer2 series is the only\nmethod that achieves a higher than 50% score on the chal-\nlenging MMStar benchmark. **High-resolution Understanding Evaluation. **\nThen we\ncompare IXC2-4KHD with models that are specifically de-\nsigned for high-resolution understanding tasks. We report\nthe results of 5 high-resolution benchmarks in Table 5, as a\ngeneral LVLM, IXC2-4KHD shows superb performance on\nthese tasks and outperforms competitors with a large mar-\ngin. For example, IXC2-4KHD gets 68*. *6% on Infograph-\nicVQA, surpassing recent DocOwl 1.5 with +17*.*9%. For\nthe OCRBench, IXC2-4KHD gets 67*. *5%, outperforms Co-\ngAgent with +8*.*5%.<br><br>**4.2."
"","","","","","Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*∗*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","3\nFresh, frozen, canned and dried all count\n‘toward your daily servings, so you have plenty<br><br>of options. ‘\n,\n9\n:\n2\nAlways reserve half of your plate for fruits\n‘and vegetables. 3\nChoose whole fruits and vegetables. 4 &<br><br>4\nTr tocata variety of vegetables, instead of the\n3\n‘same thing all the time. 2\n. 6\n4g\nLook for fruit packed in its own fruit juice and\ns\n100% fruit juice, with no added sugar. §\nLook for low/no-sodium options for canned\nvegetables, and 100% vegetable juice."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[43] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are\nyou smarter than a sixth grader? textbook question answer-\ning for multimodal machine comprehension. In*Proceed-*\n*ings of the IEEE Conference on Computer Vision and Pat-*\n*tern recognition*, pages 4999–5007, 2017. 6<br><br>[44] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv ́e\nLe Borgne, Romaric Besanc ̧on, Jos ́e G Moreno, and Jes ́us\nLov ́on Melgarejo. Viquae, a dataset for knowledge-based\nvisual question answering about named entities. In*Pro-*\n*ceedings of the 45th International ACM SIGIR Conference*\n*on Research and Development in Information Retrieval*,\npages 3108–3120, 2022. 6<br><br>[45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","| 86% Public Health England<br><br>ee\nae\nMS aay\n“et? OWHAT IS\nn/a\n2\nwna? <abie“y wy. contact tracing «\n|\n@\n=\n©<br><br>Contact tracing is a fundamental part of outbreak control\nthat’s used by public health professionals around the world\nto prevent the spread of infections<br><br>FEN\nPerson<br><br>|\nfor coronavirus\n|\n(COVID-19)\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Task\nDataset<br><br>General Semantic Alignment\nShareGPT4V-PT [14], COCO [17], Nocaps [1], TextCaps [86], LAION400M [80], SBU [75], CC 3M [83]\nWorld Knowledge Alignment\nConcept Data [110]\nVision Capability Enhancement\nWanJuan [35], Flicker[103], MMC-Inst[54], RCTW-17[84], CTW[106], LSVT[88], ReCTs[111], ArT[22]<br><br>Table 1. **Datasets used for Pre-Training**. The data are collected from diverse sources for the three objectives. The newly added data is\nhighlighted with red.<br><br>image has a 2D structure and the image ratio is dynamic,\nthe number of tokens for each row can vary across dif-\nferent images. This variation can potentially confuse the\nLVLM, making it difficult to determine which tokens be-\nlong to the same row of the image and which ones belong\nto the next row. This confusion may hinder the LVLM’s\nability to understand the 2D structure of the image, which\nis crucial for comprehending structural image content such\nas documents, charts, and tables. To address this issue, we\nintroduce a learnable newline (‘*\*n’) token at the end of each\nrow of the image features before the flattening. Finally, we\nconcatenate the global and local views, inserting a special\n‘separate’ token between them to distinguish the two views."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Xiaoyi Dong*∗*1*,*2, Pan Zhang*∗*1, Yuhang Zang*∗*1, Yuhang Cao1*,*2, Bin Wang1, Linke Ouyang1,\nSongyang Zhang1, Haodong Duan1, Wenwei Zhang1, Yining Li1, Hang Yan1, Yang Gao1, Zhe Chen1<br><br>Xinyue Zhang1, Wei Li1, Jingwen Li1, Wenhai Wang1*,*2, Kai Chen1, Conghui He3, Xingcheng Zhang3,\nJifeng Dai4*,*1, Yu Qiao1, Dahua Lin1*,*2, Jiaqi Wang1*,*�<br><br>arXiv:2404.06512v1  [cs.CV]  9 Apr 2024<br><br>1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n3SenseTime Group, 4Tsinghua University<br><br>internlm@pjlab.org.cn<br><br>Figure 1. Overview of InternLM-XComposer2-4KHD perfor-\nmance on benchmarks with different resolutions. Our model based\non InternLM2-7B [91]**matches or even surpasses GPT-4V [74]**\n**and Gemini Pro [90] in 10 of the 16 benchmarks**."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**4. Experiments**<br><br>In this section, we validate the benchmark performance\nof our InternLM-XComposer2-4KHD (IXC2-4KHD in the\nfollowing for simplicity) after supervised fine-tuning.<br><br>**4.1. LVLM Benchmark results. **<br><br>In Table 3 and Table 4,\nwe compare our IXC2-\n4KHD\non\na\nlist\nof\nbenchmarks\nwith\nboth\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","is assessed to see how close\nthey were to aconfirmed case,\n2\noH)\nwhich could include someone\nliving with the case, or\nspeak to the patient to identify\n_someone who has been in\n_\nanyone who has had close contact\n(Aaee ee\nwith them during the time they are infectious<br><br>A contact<br><br>Following this assessment, we can categorise\nthem into low or high risk and contact them to\nprovide advice on what they should do<br><br>ge\nchante\nIf we believe a contact is at\ni\nlow risk\n14\nhigher risk of infection\nG contact\ndays\nthey may be asked to\n“an doesn’t require\nGW,\nself-isolate, remaining in\nHy]\nself-isolation\ntheir home for 14 days and\n4\nstaying away from work,\nschool or public places<br><br>a\nWe contact them daily\npassive follow up\nLa)\nprovide them with advice\nwhich means person being\noF\non what to do if they\nmonitored but we don’t necessarily\nx4]\nbecome unwell until they\ncontact them every day\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[86] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In*Computer Vision–*\n*ECCV 2020: 16th European Conference, Glasgow, UK, Au-*\n*gust 23–28, 2020, Proceedings, Part II 16*, pages 742–758. Springer, 2020. 6<br><br>[87] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In*Proceed-*\n*ings of the IEEE/CVF conference on computer vision and*\n*pattern recognition*, pages 8317–8326, 2019. 1, 2, 6, 7<br><br>[88] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\nCanjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo\nLiu, Dimosthenis Karatzas, et al. Icdar 2019 competition on\nlarge-scale street view text with partial labeling-rrc-lsvt. In\n*2019 International Conference on Document Analysis and*\n*Recognition (ICDAR)*, pages 1557–1562."
"","","","","","IEEE, 2019. 6"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[102] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. *arXiv.org*, 2023. 2<br><br>[103] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. *Transactions of the Association for Computa-*\n*tional Linguistics*, 2:67–78, 2014."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[24] OpenCompass Contributors. Opencompass:\nA univer-\nsal evaluation platform for foundation models. https:\n//github.com/open- compass/opencompass,\n2023. 7<br><br>[25] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 2<br><br>[26] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos ́e M.F. Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In*Proceedings of the IEEE Confer-*\n*ence on Computer Vision and Pattern Recognition (CVPR)*,\n2017. 6<br><br>[27] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\nBin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,\nHaodong Duan, Maosong Cao, Wenwei Zhang, Yining\nLi, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-\nwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu\nQiao, Dahua Lin, and Jiaqi Wang."
"","","","","","Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Model\n‘*\*n’\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*∗*<br><br>HD9\n*×*\n79.5\n50.3\n74.0\n78.2\n79.1\n2206\n75.9\nHD9\n✓\n79.4\n50.5\n73.8\n78.2\n79.5\n2201\n**76.6**<br><br>4KHD\n*×*\n88.1\n67.4\n75.9\n80.4\n79.9\n**2232**\n76.4\n4KHD\n✓\n**89.0**\n**69.3**\n**77.2**\n**81.0**\n**80.2**\n2205\n76.2<br><br>Table 8. **Influence of Indicator ‘***\***n’ in the Image Features. **‘*\*n’\nhelps LVLM understand structural images when the input resolu-\ntion is dynamic and large.<br><br>Strategy\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*∗*<br><br>Re-Sampler\n86.2\n67.1\n75.3\n78.8\n79.6\n2124\n74.2\nC-Abstractor\n88.6\n69.5\n77.1\n80.6\n80.4\n2236\n76.7\nConcat\n89.0\n69.3\n77.2\n81.0\n80.2\n2205\n76.2<br><br>Table 9. **Ablation on Token Merging Strategy. **Both the simple\nconcatenation operation and the C-Abstractor works well."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Task\nDataset<br><br>Caption\nShareGPT4V [14], COCO [17],Nocaps [1]<br><br>General QA\nVQAv2 [3], GQA [38], OK-VQA [67]\nVD [26], RD[13], VSR[53],<br><br>Science QA\nAI2D [42], SQA [63], TQA[43], IconQA[65]<br><br>Chart QA\nDVQA [40], ChartQA, ChartQA-AUG [68]<br><br>Math QA\nMathQA [104], Geometry3K[62], TabMWP[64],\nCLEVR-MATH[52]/Super[49]<br><br>World Knowledge QA A-OKVQA [81],KVQA [82], ViQuAE[44]<br><br>OCR QA\nTextVQA[87], OCR-VQA[72], ST-VQA[8]<br><br>HD-OCR QA\nInfoVQA[69], DocVQA[70]<br><br>Conversation\nLLaVA-150k [56], LVIS-Instruct4V [94]\nShareGPT-en&zh [21], InternLM-Chat[91]<br><br>Table 2. **Datasets used for Supervised Fine-Tuning**. We collect\ndata from diverse sources to empower the model with different\ncapabilities. The newly added data is highlighted with red.<br><br>**3.3."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[83] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In*Pro-*\n*ceedings of the 56th Annual Meeting of the Association for*\n*Computational Linguistics (Volume 1: Long Papers)*, pages\n2556–2565, 2018. 6<br><br>[84] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\nPei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\nBai. Icdar2017 competition on reading chinese text in the\nwild (rctw-17). In*2017 14th iapr international conference*\n*on document analysis and recognition (ICDAR)*, volume 1,\npages 1429–1434. IEEE, 2017. 6<br><br>[85] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu,\nand Diyi Yang. Design2code: How far are we from au-\ntomating front-end engineering?, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","a>\nAnalyze this figure in detail<br><br>7)<br><br>The image is a scientific paper titled ""ShareGPT4V: Improving \nLarge Multi-Modal Models with Better Captions"" from the \nUniversity of Science and Technology of China and Shanghai \nAI Laboratory. The paper discusses the comparison between the \ncaptions in their proposed ShareGPT4V dataset and those \nutilized by recent large multi-modal models (LMMs). The \ncaptions are compared in terms of their quality and \nperformance. The paper also highlights the remarkable \nperformance of the proposed LMMs based on a simple \narchitecture that has remarkable performance across a majority \nof the multi-modal benchmarks.<br><br>0.6<br><br>64,\n68.8\n6.7,Lm\nMMBench\n623<br><br>MMBench-CN<br><br>@ ShareGPT4V-7B<br><br>a>\nWhich model has the best performance on the \nMMBench and what is its exact number? Qwen-VL-Chat-7B<br><br>7)\nAccording to the paper, ShareGPT4V-7B has the best \nperformance on the MMBench with a score of 68.8.<br><br>a>\nFigure 1. (a) We showease a comparison between the caption in our proposed ShareGPT4V dataset and those utilized by recent large\nmulti-modal models (LMMs). Unlike COCO-Caption [7] involves brief human-made captions on the main subject. LLaVA-Instruct\n(31) combines human-made captions, bounding boxes, and GPT4 [39] to ‘imagine’ the image details, which leads to inevitable er-\nror/hallucination description (marked in red), Our approach involves feeding carefully designed prompts along with images directly into\nthe advanced GPT4-Vision [40] and the descriptions are more detailed and accurate (marked in blue). (b) We highlight the remarkable\nperformance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset.<br><br>Where can I get this project?<br><br>7) You can get this project from the following GitHub link: \nhttps://ShareGPT4V.github.io\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**5. Conclusion**<br><br>In this paper, we propose the InternLM-Xcomposer2-\n4KHD that exceeds the performance of previous open-\nsource models on OCR-related tasks and also achieves\ncompetitive results on general-purpose LVLM benchmarks. Thanks to our dynamic resolution and automatic patch con-\nfiguration, our model supports a maximum training resolu-\ntion of up to 4K HD. We also integrate a global view patch\nto support the macro understanding and a learnable newline\ntoken to handle the various input image resolutions. Our\nmodel’s performance continues to improve as the training\nresolution increases for HD-OCR tasks. Notably, we do\nnot observe any performance saturation even for the 4KHD\nsetting, and we have not explored the upper bound due to\nthe computational burden increasing with higher-resolution\ninputs. In future work, we plan to explore efficient solu-\ntions for accurate LVLM training and inference, enabling\nour model to handle even higher resolutions while main-\ntaining computational efficiency."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[32] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. *arXiv preprint*\n*arXiv:2312.12436*, 2023. 2<br><br>[33] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a seed of vision in large language model. *arXiv preprint arXiv:2307.08041*, 2023. 1, 5<br><br>[34] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou. Hallusionbench: An advanced diagnostic suite for\nentangled language hallucination & visual illusion in large\nvision-language models, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","During the pre-training phase, the LLM is frozen while\nboth the vision encoder and Partial LoRA are fine-tuned to\nalign the visual tokens with the LLM. The pre-training data\nmainly follow the design in XComposer2 which is curated\nwith**three objectives**in mind: 1) general semantic align-\nment, 2) world knowledge alignment, 3) vision capability\nenhancement. In this paper, we focus on high-resolution\nand structural image understanding. Therefore, we have\ncollected more related data to enhance this specific capa-\nbility. As shown in Table.1, we have utilized a diverse OCR\ndataset for this purpose.<br><br>**3.4."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","and Pete Florence. Palm-e: An embodied multimodal lan-\nguage model. In*arXiv preprint arXiv:2303.03378*, 2023. 2<br><br>[29] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. In\n*Proceedings of the 60th Annual Meeting of the Association*\n*for Computational Linguistics (Volume 1: Long Papers)*,\npages 320–335, 2022. 1<br><br>[30] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang\nLi, and Can Huang. DocPedia: Unleashing the power\nof large multimodal model in the frequency domain\nfor versatile document understanding. *arXiv preprint*\n*arXiv:2311.11810*, 2023. 5<br><br>[31] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji."
"","","","","","Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","In many communities, fruits and vegetables are limited/expensive and only\navailable in corner stores, convenience stores, bodegas and gas stations.<br><br>Lack of Access Can Profoundly Impact Diet\nThere are ways that you can actively support improved access to\nhigher quality foods for neighbourhoods. Here's how:<br><br>Contact your city and state\nFind ways to spread the word\nCreate a petition for more\nleaders to let them know\nabout nutrition assistance\nvariety, improve affordability\nwhat food access is like in\nprograms, such as SNAP, WIC and\nor advocate for better\nyour community\nschool meals. signage/placement.<br><br>¢\nS<br><br>Meet with an after-school or\nOrganize a letter-writing\n‘Sign up for “You're the Cure”\ndaycare program representative\n‘campaign and set up a meeting\nand then send a note to your\nto discuss serving more fruits and\n_—_with state leaders. Congressperson advocating for\nveggies for snacks. For example, ask for funding to\nhealthier meals at school\nhost a farmers’ market in an\nunderserved community."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","After the pre-training, we empower the model to understand\nhigh-resolution images and solve diverse challenges. Dif-\nferent from previous perception tasks (*e.g*., VQAv2, GQA)\nwhich typically answer questions based on the noticeable\nobject in the image. OCR-related tasks depend on a de-\ntailed understanding of text within a high-resolution image. For instance, in InfoVQA, the length of the longer side of\n50% of the images exceeds 2000 pixels. Low-resolution\ninputs can distort the dense text information, causing the\nmodel to fail in its understanding. However, we have ob-\nserved a resolution saturation problem with the aforemen-\ntioned perception tasks, where the influence of resolution\nbecomes negligible. To address this, we introduce a mixed-resolution train-\ning strategy for more efficient training. For tasks requir-\ning high resolution, we employ the ‘HD-55’ setting during\ntraining. This allows for the input of 4K (3840*×*1600) im-\nages without necessitating additional image compression. These tasks are referred to as the HD-OCR QA tasks in Ta-\nble 2."
"","","","","","For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the ‘HD25’ set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","*arXiv:2209.14610*, 2022. 6<br><br>[65] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram under-\nstanding and visual language reasoning. *arXiv preprint*\n*arXiv:2110.13214*, 2021. 6<br><br>[66] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\ning Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,\nLi Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha\nZhang, and Furu Wei. Kosmos-2.5: A multimodal literate\nmodel, 2023. 2, 5<br><br>[67] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In*Proceedings*\n*of the IEEE/cvf conference on computer vision and pattern*\n*recognition*, pages 3195–3204, 2019."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**LVLMs for Document Understanding. **\nDocument un-\nderstanding involves analyzing and comprehending various\ndigital documents, such as figures, tables, and academic pa-\npers. Many document understanding tasks require models\nto handle high-resolution inputs, complex layouts, various\naspect ratios, and diverse document formats. To enhance the\ncapabilities of LVLMs for document understanding, sev-\neral works have collected and constructed high-quality doc-\nument instruction tuning data, including LLaVAR [112],\nmPLUG-DocOwl [100] and TGDoc [96]. DocPediaDoc-\nPedia [30] processes document inputs in the frequency do-\nmain. Some previous works have improved document un-\nderstanding ability by designing special modules for high-\nresolution inputs, such as HR and LR encoders [36, 97]\nor cropped image patches [59, 99, 101]. Our InternLM-\nXComposer2-4KHD first scales to 4K resolution inputs\nand demonstrates strong document understanding ability on\nOCR-related benchmarks. Also, our approach also achieves\ncomparable results on other general LVLM benchmarks like\nperception and reasoning [15, 33, 57, 61]."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Which department has the highest cost in most cases?"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[60] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-\naoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. RAR: Retrieving and ranking augmented mllms for visual\nrecognition. *arXiv preprint arXiv:2403.13805*, 2024. 2<br><br>[61] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. In*International Conference on Learning Represen-*\n*tations (ICLR)*, 2024. 5, 7<br><br>[62] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\nterpretable geometry problem solving with formal language\nand symbolic reasoning. In*The 59th Annual Meeting of the*\n*Association for Computational Linguistics (ACL)*, 2021."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**The Role of Global-View. **We first examine the impact\nof the global view in our Global-Local Format. As indi-\ncated in Table 7, we find that the global view is essential\nfor the LVLM to accurately comprehend the input image. When it is removed, the model performs worse across all\nbenchmarks. For instance, the model experiences a*−*4*. *4%\ndrop in performance on the MMBench EN-Test without the\nglobal view. We contend that the global view offers a gen-\neral macro understanding of the image, which the model\nstruggled to derive from the large number of tokens in the\nlocal view. **The Role of the Newline Token. **We incorporate a special\nnewline token at the end of each row of the image features\nbefore the flattening operation. This token serves as an in-\ndicator of the image’s 2D structure."
"","","","","","We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM’s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","tical applicability in real-world scenarios. Recent advancements have aimed at enhancing the\nresolution of Large Vision-Language Models (LVLMs). Some approaches [36, 48, 66, 97] involve adapting high-\nresolution vision encoders directly. However, the Vi-\nsion Transformer (ViT) architecture falls short when deal-\ning with images of varying resolutions and aspect ratios,\nthereby restricting its ability to handle diverse inputs ef-\nfectively. Alternatively, some methods [37, 46, 50, 51, 55,\n59, 99] maintain the vision encoder’s resolution, segment-\ning high-resolution images into multiple low-resolution\npatches. Yet, these methods are constrained by an inad-\nequate resolution, typically around 1500*×*1500, which\ndoes not satisfy the demands of daily content,*e.g*., website\nscreenshots [85], document pages [70], and blueprints [69]. Furthermore, they are confined to either a few predefined\nhigh-resolution settings [36, 46, 48, 50, 51, 55, 59, 66, 97]\nor a limited range of resolutions [37, 99], thereby restricting\ntheir utility across a variety of applications. In this work, we introduce InternLM-XComposer2-\n4KHD, a pioneering model that for the first time expands\nthe resolution capabilities of Large Vision-Language Mod-\nels (LVLMs) to 4K HD and even higher, thereby setting\na new standard in high-resolution vision-language under-\nstanding. Designed to handle a broad range of resolutions,\nInternLM-XComposer2-4KHD supports images with any\naspect ratio from 336 pixels up to 4K HD, facilitating its\ndeployment in real-world contexts. InternLM-XComposer2-4KHD\nfollows\npatch\ndivi-\nsion [46, 50] paradigm and enhances it by incorporating an\ninnovative extension: dynamic resolution with automatic\npatch configuration."
"","","","","","To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*×*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[89] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\nShu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. *arXiv preprint arXiv:2312.03818*, 2023. 2<br><br>[90] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2<br><br>[91] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Muted Color Palettes\nMuted colors are colors that have alow\nsaturation (as opposed to vivid colors). Muted colors feel safe and secure, even\nnostalgic. They can also feel natural and\norganic. That’s why many health and\nwellness brands have been using muted\ncolor palettes this year.<br><br>Simple Data\n|\nVisualizations\nThe goal of any data visualization should<br><br>be to make the complex\ndata easy to\nunderstand. We are living in a time where<br><br>a lot of data is constantly being circulated. Simple data visualizations\ncanmake\ncommunication more effective.<br><br>Geometric Shapes\nEverywhere<br><br>Last year, we saw designers usinga lot\nofflowing and abstract shapes in their\ndesigns. This year, they have been\nreplaced with rigid, hard-edged geo-\nmetric shapes and patterns. The hard\nedges of a geometric shape create a\ngreat contrast against muted colors.<br><br>Flat Icons and\nIllustrations<br><br>Many brands are using flat icons and\nillustrationsin their social media graphics,\nwebsite design, and more. Icons can be a<br><br>powerful tool for visual communication. With\na simple icon, you can communicate meaning\nin less space than words."
"","","","","","Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[38] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. *Conference on Computer Vision and*\n*Pattern Recognition (CVPR)*, 2019. 6<br><br>[39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**Large Vision-Language Models (LVLMs). **Large Lan-\nguage Models (LLMs) [6, 9, 10, 23, 39, 41, 73, 76, 78, 91–\n93, 108] have gained significant attention due to their\nimpressive performance in various language-related tasks\nsuch as text generation and question answering. Follow-\ning this enthusiasm, recent Large Vision-Language Mod-\nels (LVLMs) have emerged[4, 7, 16, 18, 19, 25, 28, 32,\n47, 74, 77, 102, 110, 113], combining LLMs with vi-\nsion encoders [79, 89, 109] to leverage the complemen-\ntary strengths of language and vision modalities. By fusing\ntextual and visual representations, LVLMs can ground lan-\nguage in visual contexts, enabling a more comprehensive\nunderstanding and generation of multimodal content [5, 11,\n14, 20, 27, 51, 60, 95]. **LVLMs for High-Resolution Understanding. **\nLarge\nVision-Language Models (LVLMs) often employ CLIP-\nViT as the visual encoder for vision-dependent tasks. How-\never, the visual encoder’s reliance on low resolutions,\nsuch as 224*×*224 or 336*×*336 pixels, limits its ef-\nfectiveness for high-resolution tasks like OCR and docu-\nment/chart perception. To enhance high-resolution under-\nstanding, recent works have primarily employed the fol-\nlowing strategies: (1) High-resolution (HR) visual encoders\nor dual encoders catering to HR and low-resolution (LR)\ninputs [36, 48, 66, 97]. For instance, Vary [97] intro-\nduces a new image encoder supporting HR inputs, which\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Figure 6. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","open-source LVLMs and closed-source APIs. Here\nwe\nreport\nresults\nin\nDocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87],\nOCRBench[58],\nMMStar[15], MathVista[61], MMMU[107], AI2D[42],\nMME [31], MMBench (MMB) [57], MMBench-Chinese\n(MMB*CN*) [57], SEED-Bench Image Part (SEED*I*)[45],\nQBench-Testset (QBench*T*)[98],\nMM-Vet [105],\nHal-\nlusionBench (HallB)[34]. The evaluation is mainly\nconducted on the OpenCompass VLMEvalKit[24] for the\nunified reproduction of the results.<br><br>**Comparison with Closed-Source APIs. **As demonstrated\nin Table 3, IXC2-4KHD exhibits competitive performance\nacross a variety of benchmarks, rivaling that of Closed-\nSource APIs. Owing to its high-resolution input, IXC2-\n4KHD achieves a score of 90*. *0% on DocVQA and 81*. *0%\non ChartQA, thereby surpassing GPT-4V and Gemini-Pro\nwith a non-trivial margin. In the challenging Infograph-\nicVQA task, our model is the first open-source model that is\nclose to the performance of Closed-Source APIs, exceeding\nthe performance of previous open-source models by nearly\n20%. In addition to OCR-related tasks, IXC2-4KHD is a\ngeneral-purpose Large Vision-Language Modal that excels\nin semantic-level tasks, demonstrating competitive results."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","switch to the HD-16 setting, we observe a performance gain\nof +10*.*2%. The performance continues to improve as the\nresolution increases, with saturation not observed even for\nthe 4KHD setting. Due to computational constraints, we\ndefer the exploration of the upper bound of improvement\nto future work. In terms of other OCR-related tasks, the\nperformance gain attributable to increased resolution is rel-\natively minor. For the perception-related benchmarks, per-\nformance is saturated on the resolution that only has negli-\ngible difference between the four settings. **Higher Inference Resolution Leads to better results on**\n**Text-related Tasks. **An intriguing observation from our ex-\nperiments is that our model, when inferring with a slightly\nhigher resolution, tends to yield improved results on text-\nrelated tasks. We present the results of HD-9, HD-16,\nand HD-25 in Table 6. For instance, IXC2-HD9 achieves\na 50*. *5% score on InfographicVQA."
"","","","","","When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In*Proceedings of the International Conference on*\n*Machine learning (ICML)*, pages 8748–8763. PMLR, 2021. 2<br><br>[80] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. *arXiv preprint arXiv:2111.02114*, 2021. 6<br><br>[81] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In*European Conference on Computer Vision*,\npages 146–162."
"","","","","","Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[35] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\njuan: A comprehensive multimodal dataset for advancing\nenglish and chinese large models. *ArXiv*, abs/2308.10755,\n2023. 6<br><br>[36] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\nWenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, et al. Cogagent: A visual language\nmodel for gui agents. *arXiv preprint arXiv:2312.08914*,\n2023. 2, 5, 7, 8<br><br>[37] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\nZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei\nHuang, et al. mplug-docowl 1.5: Unified structure learn-\ning for ocr-free document understanding. *arXiv preprint*\n*arXiv:2403.12895*, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[12] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and\nByungseok Roh. Honeybee: Locality-enhanced projec-\ntor for multimodal llm. *arXiv preprint arXiv:2312.06742*,\n2023. 9<br><br>[13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm’s referential dialogue magic. *arXiv.org*, 2023. 6<br><br>[14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. *arXiv preprint arXiv:2311.12793*, 2023. 1, 2, 6<br><br>[15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, and Feng Zhao."
"","","","","","Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","In practice, we employ the OpenAI CLIP ViT-L-14-336\nas the vision encoder. Different from XComposer2, We\nkeep the ViT resolution as 336*×*336 and increase the input\nresolution with more patches. For the Dynamic Image Par-\ntition strategy, we use ‘HD-25’ for the pertaining. For each\nimage or patch, the image token number is decreased to 1*/*4\nwith a simple**merge operation**. We concatenate the nearby\n4 tokens into a new token through the channel dimension,\nthen align it with the LLM by an MLP. The ‘separate’ and\n‘*\*n’ token are randomly initialized. For the Partial LoRA,\nwe set a rank of 256 for all the linear layers in the LLM de-\ncoder block. Our training process involves a batch size of\n4096 and spans across 2 epochs. The learning rate linearly\nincreases to 2*×*10*−*4 within the first 1% of the training\nsteps. Following this, it decreases to 0 according to a co-\nsine decay strategy."
"","","","","","To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[28] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","0.18 # 1 @ Payal # Support © Engnering © Faces © Finance\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[50] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important\nthings for large multi-modal models. *arXiv preprint*\n*arXiv:2311.06607*, 2023. 2, 5, 7<br><br>[51] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[104] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengy-\ning Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. Metamath: Bootstrap your own\nmathematical questions for large language models. *arXiv*\n*preprint arXiv:2309.12284*, 2023. 6<br><br>[105] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. *arXiv preprint arXiv:2308.02490*, 2023. 7<br><br>[106] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in\nthe wild. *Journal of Computer Science and Technology*,\n34(3):509–521, 2019. 6<br><br>[107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\nYang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\nWenhu Chen."
"","","","","","Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Model\nDoc Info Text Chart MMB MME SEED*∗*<br><br>HD9\n79.4 50.5 73.8\n78.2\n79.5\n2201\n76.6\n+ w/o global-view 78.1 47.9 71.2\n77.9\n75.1\n2019\n76.2<br><br>Table 7. **Influence of Global-View in the Input. **Global-view is\ncritical for most benchmarks.<br><br>namic image token length used in training enhances the ro-\nbustness of the LVLM, leading to better results when the\ntext in the image is more ‘clear’ in the higher resolution\ninput. Conversely, the results on ChartQA consistently de-\ngrade under this setting. This could be due to the model be-\ncoming confused about the chart structure when the resolu-\ntion is altered. Additionally, similar to the observation from\nFigure 5, the impact of resolution on perception-related\nbenchmarks appears to be quite minor. **Visualization Results. **We provide the visualization results\non ultra-high HD images in Figure 2 and Figure 3. Please\nrefer to the appendix for more results.<br><br>found this approach to be effective in reducing the num-\nber of image tokens efficiently. Here we study the influence\nof different token-merging strategies under the 4KHD set-\nting."
"","","","","","In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>IXC2-VL</th><th>InernLM2-7B</th><th>55.4</th><th>57.6</th><th>81.2</th><th>1,712.0</th><th>530.7</th><th>80.7</th><th>79.4</th><th>74.9</th><th>72.5</th><th>46.7</th></tr>\n<tr><td>IXC2-VL</td><td>InernLM2-7B</td><td>55.4</td><td>57.6</td><td>81.2</td><td>1,712.0</td><td>530.7</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td></tr>\n<tr><td>IXC2-4KHD</td><td>InernLM2-7B</td><td>54.1</td><td>57.8</td><td>80.9</td><td>1,655.9</td><td>548.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>HD25\nHD25\nHD30</th><th>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</th></tr>\n<tr><td>HD25\nHD25\nHD30</td><td>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</td></tr>\n</table><br><br>Table 6. **Influence of Inference Resolution. **The model achieves\nbetter performance on text-related tasks when the inference reso-\nlution is higher than its training resolution."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","“aencare |<br><br>Figure 4. **The illustration of processing high-resolution input.**<br><br>**3.2. High-Resolution Input. **<br><br>**Dynamic Image Partition. **\nUtilizing a static input im-\nage size for processing high-resolution images, particularly\nthose with varying aspect ratios, is neither efficient nor ef-\nfective. To overcome this limitation, we introduce a dy-\nnamic image partitioning approach, as shown in Figure 4. Our method strategically segments the image into smaller\npatches, while maintaining the integrity of the original im-\nage’s aspect ratio. Given a maximum partition number*H*, the image*x*with\nsize [*h, w*] is resized and padded to the new image ˆ*x*with\nsize [*ph ×*336*, pw ×*336]. This process is subject to the\nfollowing constraints:<br><br>*p_w  \*t*im*e*s p_h \leq \mathcal {H}; \; p_h = \lceil p_w \times h / w \rceil*\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","a>\n@ HR @ IT @ Payroll @ Support @ Engineering @ Facilities @ Finance @ Sales\nConduct an in-depth analysis of the chart.<br><br>7)<br><br>The line chart presents the operations cost by department in US \ndollars for various categories from May 2018 to Mar 2019. The \ncategories include Human Resources (HR), Information \nTechnology (IT), Payroll, Support, Engineering, Facilities, \nFinance, and Sales. Each category is represented by a different \ncolor and line on the graph. The y-axis represents the cost in \nthousands of US dollars, while the x-axis represents the months \nfrom May 2018 to Mar 2019. The graph shows fluctuations in \nthe cost for each category throughout the months, with some \ncategories experiencing peaks and troughs at different times. 80xee\nSKS\n~\nZg\n=\n=\nee\n—~\na,\n—=\n“=\nSS==a\nway 2018\nsut20t8\nsep 2018\nNow 2018\nJon 2019\nWar 20\nMonths<br><br>a><br><br>a\n0\nMay 2018\nMonths\nJan 2019\nMar 29\n7) Payroll has the highest cost in most cases, as indicated by \nthe purple line on the graph."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","wid©a4“<\nie\nete|\nlate<br><br>Social Media\nSlide Decks\nAlot of people have been sharing slide decks<br><br>on Instagram and Linkedin. They're a visual\nway of communicating longer messages than\na single image post. That’s why we've seen<br><br>so many social justice, political and wellness\naccounts sharing slide decks. On top of that,\nthe algorithms\non Instagram and Linkedin. seem to promote these slide decks a lot\nmore thana single image.<br><br>Text Heavy Videos\nPeople will likely continue to work\nremotely well into 2021. That means\nshooting new video content is going to\nbe difficult. Videos that make use of text\non-screen to communicate messages\nare a way of getting around that.<br><br>Instead of needing a whole production\nteam to create a video, brands can\ncreate a simple text heavy video in\na fraction ofthe time.<br><br>Read the full guide: venngage."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Amputee\npaneer\n7<br><br>initialize\namoot\n= feat\neumemtoy O04)<br><br>goo (ins ian si44)<br><br>é\nere\nainda aren\na quuatontion\nhoe\n'\n2<br><br>3\nmort= cummed\n7\nab\nith, Yndew\n:\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","PROTECTING OUR PLANET STARTS WITH\nad<br><br><table border=""1"">\n<tr><th></th><th></th><th>\n</th></tr>\n<tr><td> </td><td> </td><td>\n</td></tr>\n<tr><td> </td><td> </td><td>\n</td></tr>\n</table><br><br>BIKE MORE | @B reduce\n{choose sustainable’ rueyon\neR | Gi recy\n=oGeil\noy'>) creme, e -§ SHES"" paw\nEncode Resize :\nSEEETON | cercemesaaes | Seale, A TREE\niliNg\nDon’t send\n& Merge EDUCATE r ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW curs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","a>\nDetect all the word in the given image<br><br>7)\nAmerican Heart Association. VennGAGE. <br><br>How to Develop Healthy Eating Habits. And Reduce the Risk of Heart Disease. <br><br>Your best defense is a healthy lifestyle filled with physical activity and \nnutritious meals, while cutting down or avoiding drinking and smoking. <br><br>The Current Trend<br><br>75% of the US population is low in its fruit and vegetables consumption. 1/10 \nadults meets the recommendation for fruit and vegetable consumption. Men and \nyoung adults in general eat the fewest fruits and vegetables compared to any \nother group. <br><br>3 Steps to Eating Healthy 1. Add produce of various colors to your diet to \ndiversify your produce consumption."
"","","","","","2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</th><th>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</th><th>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</th></tr>\n<tr><td>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</td><td>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</td><td>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>GPT-4V\nGemini-Pro</th><th>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</th></tr>\n<tr><td>GPT-4V\nGemini-Pro</td><td>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","75%\n1/10\n(*)\nMen and young adults\nof the US population is\nadults meets the\nin general eat the fewest\nlow in its fruit and\nrecommendation for fruit and\nfruits and vegetables,\nvegetables consumption. vegetable consumption. compared to any other group.<br><br>3 Steps to Eating Healthy\n1\n2\n3\n‘Add produce of varying colors\nUse hacks to incorporate more\nTake steps to improve\nproduce consumption.to your diet to diversify your\nveggies and fruits to your meals,\naccessto fruits and\nand throughout the day."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Source: https://ww.empoweredtoserve.rg/-/media/ETS-files/Community Resources/Health-Lessons\nAmgen Branded/Fruits-and-Veggies/DS18477_ETS_Amgen_Presentation_FruitVeg_OTkk_\nNOTES. pdf<br><br>The HD image (816 x 3813)<br><br>Figure 7. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In*ICDAR*, 2019. 6<br><br>[73] OpenAI. Chatgpt. https://openai.com/blog/\nchatgpt, 2022. 1, 2<br><br>[74] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[75] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text:\nDescribing images using 1 million captioned\nphotographs."
"","","","","","In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730–27744, 2022."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Overall, the infographic provides a \ncomprehensive overview of the 7 graphic design \ntrends that are expected to shape the design \nlandscape in 2021.<br><br>=\nWhat is the detailed explanation of the \nthird part?<br><br>*<br><br>The third part of the image is titled ""Geometric \nShapes Everywhere"". It explains that last year, \ndesigners used a lot of flowing and abstract \nshapes in their designs. However, this year, they \nhave been replaced with rigid, hard-edged \ngeometric shapes and patterns. The hard edges of \na geometric shape create a great contrast against \nmuted colors.<br><br>®\n(@)<br><br>=<br><br>*\nYou can find the full guide on \nvennage.com/blog/graphic-design-trends.<br><br>The HD image (816 x 5133)<br><br>Figure 3. **Chat with InternLM-XComposer2-4KHD on ultra-high HD image with the 816***×***5133 resolution**. Some regions of the\ninput HD images are zoomed in for better visualization."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[63] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal rea-\nsoning via thought chains for science question answer-\ning. *Advances in Neural Information Processing Systems*,\n35:2507–2521, 2022. 6<br><br>[64] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","github.com/InternLM/InternLM, 2023. 1, 2, 6<br><br>[92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth ́ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. *arXiv.org*, 2023.<br><br>[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models,\n2023. 1, 2<br><br>[94] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. *arXiv preprint*\n*arXiv:2311.07574*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[111] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\nLi, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\nMingkun Yang, et al. Icdar 2019 robust reading challenge\non reading chinese text on signboard. In*2019 international*\n*conference on document analysis and recognition (ICDAR)*,\npages 1577–1581. IEEE, 2019. 6<br><br>[112] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: En-\nhanced visual instruction tuning for text-rich image under-\nstanding. *arXiv preprint arXiv:2306.17107*, 2023. 5<br><br>[113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. Minigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language\nmodels. *arXiv.org*, 2023."
"","","","","","2"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In*International*\n*Conference on Computer Vision (ICCV)*, 2015. 6<br><br>[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt. Openflamingo:\nAn open-\nsource framework for training large autoregressive vision-\nlanguage models. *arXiv.org*, 2023. 2<br><br>[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. *arXiv.org*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Find ways to spread the word about nutrition assistance programs, such as \nSNAP, WIC and school meals. <br><br>Create a petition for more variety, improve affordability and advocate for better \nsignage/placement. <br><br>Meet with an after-school or daycare program representative to discuss serving \nmore fruits and vegetables for snacks. Organize a letter-writing campaign and \nset up a meeting with state leaders. <br><br>For example, ask for funding to host a farmers market in an underserved \ncommunity. Sign up for ""You're the Cure"" and send a note to your \nCongressperson advocating for healthier meals at school. <br><br>It's American Heart Month! Share this infographic with your family, friends and \nnetwork today. You can carry out all of these actions or just a few. But \nwhichever actions you choose, they pave the way for greater access to nutritious \nfood for your community."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","‘Adding a variety of produce to your diet has a number of health benefits. It'll help you:<br><br>Manage your weight\nControl your blood pressure\nSupport healthy digestion<br><br>Reduce the risk of some\nReduce the risk of chronic health\ncancers, such as colon cancer\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**1. Introduction**<br><br>In recent years, the progress in Large Language Models\n(LLMs) [10, 21, 29, 39, 73, 78, 91–93] has provoked the\ndevelopment of Large Vision-Language Models (LVLMs). These models have demonstrated proficiency in tasks such\nas image captioning [14, 17] and visual-question-answering\n(VQA) [31, 33, 57, 107]. Nevertheless, due to their limited\nresolution, they struggle with processing images containing\nfine details, such as charts [68], tables [87], documents [70],\nand infographics [69]."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth ́ee Lacroix, and William El Sayed. Mistral 7b, 2023. 1, 2<br><br>[40] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. Dvqa: Understanding data visualizations via ques-\ntion answering. In*Proceedings of the IEEE conference on*\n*computer vision and pattern recognition*, pages 5648–5656,\n2018. 6<br><br>[41] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. *arXiv preprint arXiv:2001.08361*,\n2020. 2<br><br>[42] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images."
"","","","","","In*Computer Vision–ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11–14, 2016, Proceedings, Part IV 14*, pages 235–\n251. Springer, 2016."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Figure 2. **Chat with InternLM-XComposer2-4KHD**. Some regions of the input HD images are zoomed in for better visualization."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","§} ShareGPT4¥: Improving Large Multi-Modal Models with Better Captions<br><br>Lin Chen**! ?, Jinsong Li*®?, Xiaoyi Dong?, Pan Zhang”, Conghui He?, Jiaqi Wang?,\nFeng Zhao! ', Dahua Lin’? 1University of Science and Technology of China\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**3. Method**<br><br>**3.1. Model Architecture. **<br><br>The model architecture of InternLM-XComposer2-4KHD\nmainly follows the design of InternLM-XComposer2[27]\n(XComposer2 in the following for simplicity. ), including a\nlight-weight Vision Encoder OpenAI ViT-Large/14, Large\nLanguage Model InternLM2-7B, and Partial LoRA for effi-\ncient alignment. We recommend the readers to the XCom-\nposer2 paper for more details."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","And Reduce the Risk of Heart Disease\na\n. *\n%\nw\nRs\n=\n~~\nY<br><br>Your best defense is a healthy lifestyle filled with physical activity and\nnutritious meals, while cutting down or avoiding drinking and smoking."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","a>\nWhat is the algorithm of this flow chart?<br><br>7)\nThe flow chart in the image is an algorithm for finding the maximum element in an array. Here is a \ndetailed explanation of the algorithm:<br><br>1. Declare an integer array having size m.\n2. Input the m numbers into the array. 3. Initialize max as the first element of the array. 4. For each element in the array, starting from the second element:\nIf the current element is greater than max, set max to the current element. 5. At the end of the loop, max will hold the maximum element in the array."
"","","","","","6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Adding a variety of produce to your diet has a number of health benefits. It \nhelps you: Manage your weight, Control your blood pressure, Support healthy \ndigestion, Reduce the risk of some cancers, such as colon cancer, Reduce the \nrisk of chronic health problems, such as diabetes. <br><br>2. Use Healthy Eating Hacks <br><br>1. Fresh, frozen, canned and dried all count toward your daily servings, so you \nhave plenty of options. 2. Always reserve half of your plate for fruits and \nvegetables. 3. Choose whole fruits and vegetables. 4."
"","","","","","Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[18] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut. Pali-3 vision language models: Smaller, faster,\nstronger, 2023. 2<br><br>[19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","PROTECTING OUR PLANET STARTS WITH\nad<br><br>BIKE MORE | @B reduce\n{choose sustainable’ rueyon\neR | Gi recy\n=oGeil\noy'>)\ncreme, e -§\nSHES"" paw\n:\nSEEETON | cercemesaaes | Seale, A TREE\nili Ng\nDon’t send\nEDUCATE\nr ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW\ncurs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc<br><br>Dynamic Image \nPartition\nR PLANET STARTS PROTECTING OU<br><br><table border=""1"">\n<tr><th></th><th></th><th></th><th></th><th></th><th></th><th>\n</th></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\n</td></tr>\n</table><br><br>Dalve vess\n¢ Freocte<br><br>Encode\nWA\n= ane\n& Merge<br><br>ae<br><br>ae\n«(1\nEDUCATE.<br><br>es énour\nee Y\nVolunteer! | 2\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Chen, et al. Sphinx: The joint mixing of weights, tasks, and\nvisual embeddings for multi-modal large language models. *arXiv preprint arXiv:2311.07575*, 2023. 2, 5<br><br>[52] Adam Dahlgren Lindstr ̈om and Savitha Sam Abra-\nham. Clevr-math:\nA dataset for compositional lan-\nguage, visual and mathematical reasoning. *arXiv preprint*\n*arXiv:2208.05358*, 2022. 6<br><br>[53] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. *Transactions of the Association*\n*for Computational Linguistics*, 2023. 6<br><br>[54] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu."
"","","","","","Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2023. 2<br><br>[20] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and\nJifeng Dai. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. *arXiv*\n*preprint arXiv:2312.14238*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**References**<br><br>[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. Nocaps: Novel object\ncaptioning at scale. In*Proceedings of the IEEE/CVF inter-*\n*national conference on computer vision*, pages 8948–8957,\n2019. 6<br><br>[2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,\nGe Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jian-\nqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu,\nShawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen\nXie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yux-\nuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi:\nOpen foundation models by 01.ai, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Table 3. **Comparison with closed-source APIs and previous open-source SOTAs. **Our InternLM-XComposer2-4KHD gets SOTA\nresults in 6 of the 16 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs. The best results\nare**bold**and the second-best results are underlined.<br><br><table border=""1"">\n<tr><th>Method</th><th>LLM</th><th>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</th></tr>\n<tr><td>Method</td><td>LLM</td><td>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[99] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\nNi, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\nSun, and Gao Huang. Llava-uhd: an lmm perceiving any\naspect ratio and high-resolution images. *arXiv preprint*\n*arXiv:2403.11703*, 2024. 2, 5, 8<br><br>[100] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Jun-\nfeng Tian, et al. mPLUG-DocOwl: Modularized multi-\nmodal large language model for document understanding. *arXiv preprint arXiv:2307.02499*, 2023. 5<br><br>[101] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming\nYan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji\nZhang, et al. Ureader: Universal ocr-free visually-situated\nlanguage understanding with multimodal large language\nmodel. *arXiv preprint arXiv:2310.05126*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[77] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world. *arXiv.org*,\n2023. 2<br><br>[78] Qwen. Introducing qwen-7b: Open foundation and human-\naligned models (of the state-of-the-arts), 2023. 1, 2<br><br>[79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,\nYang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia\nGuo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang,\nTao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiax-\ning Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\ning Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-\nwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun\nLv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang\nNing, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%*chatgpt quality, March 2023. 1, 6<br><br>[22] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\nNg, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\nZhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\ning challenge on arbitrary-shaped text-rrc-art. In*2019 In-*\n*ternational Conference on Document Analysis and Recog-*\n*nition (ICDAR)*, pages 1571–1576. IEEE, 2019. 6<br><br>[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. *arXiv.org*, 2022."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>Train Eval</th><th>Doc Info Text Chart MMB MME SEED∗</th></tr>\n<tr><td>Train Eval</td><td>Doc Info Text Chart MMB MME SEED∗</td></tr>\n</table><br><br><table border=""1"">\n<tr><th>HD9\nHD9\nHD16</th><th>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</th></tr>\n<tr><td>HD9\nHD9\nHD16</td><td>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</td></tr>\n</table><br><br><table border=""1"">\n<tr><th>HD16\nHD16\nHD25</th><th>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</th></tr>\n<tr><td>HD16\nHD16\nHD25</td><td>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>Gemini-Pro</th><th>88.1</th><th>74.1</th><th>75.2</th><th>74.6</th><th>68.0</th><th>42.6</th><th>45.8</th><th>70.2</th><th>47.9</th><th>1,933.3</th><th>73.6</th><th>74.3</th><th>70.7</th><th>70.6</th><th>59.2</th><th>45.2</th></tr>\n<tr><td>IXC2-VL</td><td>57.7</td><td>72.6</td><td>34.4</td><td>70.1</td><td>53.2</td><td>55.4</td><td>57.6</td><td>81.2</td><td>41.4</td><td>2,220.4</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td><td>41.0</td></tr>\n<tr><td>IXC2-4KHD</td><td>90.0</td><td>81.0</td><td>68.6</td><td>77.2</td><td>67.5</td><td>54.1</td><td>57.8</td><td>80.9</td><td>39.7</td><td>2,204.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td><td>40.9</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[16] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali-x: On scaling up a multilingual vision\nand language model, 2023. 2<br><br>[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server, 2015."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","multi-modal benchmarks. This. project\nis available at\nhttps: //ShareGPT4V. github."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","& shareGPTWV: Improving Large Mul-Modal Me with BeterCaptions\nsca onme ee\nnye\nnc gg\naa aay<br><br>Zz\ni\n——— ee\nS\nES\nSSeeeass) 4oes<br><br>5 spmamilcamees ene\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","up to 4K resolution, the model can achieve additional\nperformance improvements during inference by processing\nimages at higher resolutions. Furthermore, scaling the training resolution up to 4K\nstandard results in a consistent improvement in perfor-\nmance, highlighting the potential for training even beyond\n4K resolution. This underscores the capacity for further en-\nhancing model capabilities and suggests a promising tra-\njectory for advancing the frontiers of high-resolution image\nprocessing within the domain of large vision-language mod-\nels. We evaluate our InternLM-XComposer2-4KHD on\n16 diverse benchmarks spanning various domains, in-\ncluding 5 challenging HD-OCR datasets (DocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87] and\nOCRBench[58]). Compared to previous open-source\nLVLM models and closed-source APIs, our approach\nachieves SOTA results in 6 of 16 benchmarks, demon-\nstrating competitive performance despite only 7B parame-\nters. As shown in Figure 1, InternLM-XComposer2-4KHD\neven surpasses the performance of GPT4V [74] and Gemini\nPro [90] across ten benchmarks. Notably, our method ex-\nhibits excellent performance on 5 HD-OCR datasets, over\nexisting open-source LVLMs by a substantial margin."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","’ Recovery\n&\ntime<br><br>Finally, the infographic emphasizes the \nimportance of recovery time, as contact displays \nno symptoms of coronavirus (COVID-19) the \npatient makes a full recovery, they can go back \nto daily lives as normal and pose no risk to \nothers. The HD image (986 x 2560)<br><br>@\nContact displays no symptoms\nof coronavirus (COVID-19)\nthe patient makesa full recovery,\nthey can go back to daily lives as\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","DocVQA\n**Abstract** OCRBench ChartQA<br><br>81.0\n90.0 67.5\nMathVista TextVQA<br><br>57.8\n77.2<br><br>AI2D InfoVQA\n80.9\n68.6<br><br>2204.9 74.7\nMME\nSEEDI<br><br>71.8\n77.7<br><br>QBenchT MMBenchCN\n54.9 39.9<br><br>40.9 80.2<br><br>MM-Vet MMBench\n54.1<br><br>HallB MMMU\nMMStar<br><br>GPT-4V\nGemini-Pro\nIXC2-4KHD\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","a>\nGive an in-depth description of the visual.<br><br>7)<br><br>The image is an infographic from Public Health \nEngland that explains the concept of contact \ntracing in the context of preventing the spread of \ncoronavirus (COVID-19). Contact tracing is a \nfundamental part of outbreak control used by \npublic health professionals around the world to \nprevent the spread of infections.<br><br>The infographic is divided into several sections, \neach providing a different aspect of the contact \ntracing process. The first section explains that a \ncontact is assessed to see how close they were to \na confirmed case, which could include someone \nliving with the case, or someone who has been \nin direct contact with them or their bodily fluids. Following this assessment, the contacts are \ncategorised into low or high risk and provided \nwith advice on what they should do.<br><br>The second section explains that if a contact is at \nhigh risk of infection, they may be asked to self-\nisolate, remaining in their home for 14 days and \nstaying away from work, school or public \nplaces. If a contact is at low risk, they don't \nrequire self-isolation.<br><br>The third section explains that if the person \ndevelops symptoms, they would be tested and \nprovided with specialist care if they have the \nnovel coronavirus (COVID-19)."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Table 5. **High-resolution Evaluation. **IntenrLM-XComposer2-4KHD has the largest input resolution and outperforms open-source\nLVLMs which are specifically tuned for document understanding."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","=\nIllustrate the fine details present in the image.<br><br>*<br><br>The image is a vibrant and colorful infographic \nthat provides an overview of 7 graphic design \ntrends that are expected to dominate in 2021. The \ninfographic is divided into 7 sections, each \nrepresenting a different trend.<br><br>At the top, there's a section titled ""Muted Color \nPalettes"", which introduces the concept of muted \ncolors as a trend for 2021. <br><br>This is followed by a section on ""Simple Data \nVisualizations"", highlighting the goal of making \ncomplex data easy to understand.<br><br>The third section focuses on ""Geometric Shapes \nEverywhere"", showcasing how designers are \nusing geometric shapes in their designs. <br><br>The fourth section delves into ""Flat Icons and \nIllustrations"", explaining how brands are using \nflat icons and illustrations in their social media \ngraphics, website design, and more.<br><br>The fifth section introduces ""Classic Serif Fonts"", \ndiscussing how these fonts are seen as classic, \nelegant, and trustworthy. <br><br>The sixth section explores ""Social Media Slide \nDecks"", illustrating how slide decks have become \na visual way of communicating longer messages \non Instagram and LinkedIn.<br><br>Finally, the seventh section discusses ""Text \nHeavy Videos"", predicting that people will likely \ncontinue to work remotely well into 2021, leading \nto an increase in video content that uses text on-\nscreen to communicate messages."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","**InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model**\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[46] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang,\nFanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-\nmodality model, 2023. 2, 5<br><br>[47] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. *arXiv.org*, 2023. 2<br><br>[48] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-Gemini: Mining the potential of multi-modality\nvision language models. *arXiv preprint arXiv:2403.18814*,\n2024. 2, 5<br><br>[49] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam\nKortylewski, Wufei Ma, Benjamin Van Durme, and Alan L\nYuille. Super-clevr: A virtual benchmark to diagnose do-\nmain robustness in visual reasoning."
"","","","","","In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963–14973, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[68] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. *arXiv preprint arXiv:2203.10244*, 2022. 1, 2, 6, 7<br><br>[69] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In*Proceedings of the IEEE/CVF Winter Conference on Ap-*\n*plications of Computer Vision*, pages 1697–1706, 2022. 1,\n2, 6, 7<br><br>[70] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In*Pro-*\n*ceedings of the IEEE/CVF winter conference on applica-*\n*tions of computer vision*, pages 2200–2209, 2021. 1, 2, 6,\n7<br><br>[71] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al."
"","","","","","Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","=<br><br>If the person does\n|(@)a\nwe would test\nanaes\n—\nthem and provide them with\n=== Gas\n|\nspecialist care if they have\nwe O|e=\n|\nthe novel coronavirus\n———_\n(Cow\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","*The Large Vision-Language Model (LVLM) field has*\n*seen significant advancements, yet its progression has been*\n*hindered by challenges in comprehending fine-grained vi-*\n*sual content due to limited resolution. Recent efforts have*\n*aimed to enhance the high-resolution understanding ca-*\n*pabilities of LVLMs, yet they remain capped at approxi-*\n*mately 1500 × 1500 pixels and constrained to a relatively*\n*narrow resolution range. This paper represents InternLM-*\n*XComposer2-4KHD, a groundbreaking exploration into el-*\n*evating LVLM resolution capabilities up to 4K HD (3840*\n*× 1600) and beyond. Concurrently, considering the ultra-*\n*high resolution may not be necessary in all scenarios, it*\n*supports a wide range of diverse resolutions from 336 pix-*\n*els to 4K standard, significantly broadening its scope of ap-*\n*plicability. Specifically, this research advances the patch*\n*division paradigm by introducing a novel extension: dy-*\n*namic resolution with automatic patch configuration. *\n*It*\n*maintains the training image aspect ratios while automati-*\n*cally varying patch counts and configuring layouts based on*\n*a pre-trained Vision Transformer (ViT) (336 × 336), lead-*\n*ing to dynamic training resolution from 336 pixels to 4K*\n*standard. *\n*Our research demonstrates that scaling train-*\n*ing resolution up to 4K HD leads to consistent perfor-*\n*mance enhancements without hitting the ceiling of poten-*\n*tial improvements. *\n*InternLM-XComposer2-4KHD shows*\n*superb capability that matches or even surpasses GPT-*\n*4V and Gemini Pro in 10 of the 16 benchmarks. *\n*The*\n*InternLM-XComposer2-4KHD model series with 7B pa-*\n*rameters are publicly available at https://github. *\n*com/InternLM/InternLM-XComposer."
"","","","","","*<br><br>*indicates equal contribution."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[95] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models, 2023. 2, 7<br><br>[96] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and\nHouqiang Li. Towards improving document understanding:\nAn exploration on text-grounding via mllms. *arXiv preprint*\n*arXiv:2311.13194*, 2023. 5<br><br>[97] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang. Vary:\nScaling up the vision vocab-\nulary for large vision-language models. *arXiv preprint*\n*arXiv:2312.06109*, 2023. 2, 5<br><br>[98] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, et al. Q-bench: A benchmark for\ngeneral-purpose foundation models on low-level vision."
"","","","","","*arXiv preprint arXiv:2309.14181*, 2023."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>Open-Source\nPrevious SOTA</th><th>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</th></tr>\n<tr><td>Open-Source\nPrevious SOTA</td><td>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Table 4. **Comparison with open-source SOTA methods. **IXC2-4KHD outperforms competitors in most benchmarks. The best results\nare**bold**and the second-best results are underlined.<br><br>setting yields better results on most OCR-related tasks when\nthe LVLM is trained under the ‘HD25’ setting. In practice, we jointly train all the components with a\nbatch size of 2048 over 3500 steps. Data from multiple\nsources are sampled in a weighted manner, with the weights\nbased on the number of data from each source. As the ‘HD-\n55’ setting has double image tokens than the ‘HD-25’, we\nadjust the data loader to enable different batch sizes for\nthem and adjust their weight accordingly. The maximum\nlearning rate is set to 5*×*10*−*5, and each component has its\nown unique learning strategy. For the vision encoder, we set\nthe LLDR to 0*. *9, which aligns with the pretraining strategy."
"","","","","","For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables each may\nreduce your risk for cardiovascular disease, including heart attack and stroke. Examples of a single serving of fruit:\nExamples of a single serving of vegetables:\n©\n1 medium apple\n©\n5-8 broccoli florets\n©\n1small banana\n+\nYa\nlarge sweet potato\n*% medium avocado\n©\n6 baby carrots\n«© large strawberries\n©\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","<table border=""1"">\n<tr><th>Method</th><th>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</th></tr>\n<tr><td>Method</td><td>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","RA a\nos\n@)<br><br>5= 9)<br><br>a\n:\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Operations Cost by Department by Month<br><br>160K<br><br>120K"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","Model\nModel Size\nMax Resolution\nDocVQA*T est*\nChartQA*T est*\nInfoVQA*T est*\nTextVQA*V al*\nOCRBench<br><br>TextMonkey[59]\n9B\n896x896\n73.0\n66.9\n28.6\n65.6\n55.8\nLLaVA-UHD [99]\n13B\n1008x672\n—\n—\n—\n67.7\n—\nCogAgent [36]\n17B\n1024x1024\n81.6\n68.4\n44.5\n76.1\n59.0\nUReader [101]\n7B\n2240x2240\n65.4\n59.3\n42.2\n57.6\n—\nDocOwl 1.5 [37]\n8B\n1344x1344\n82.2\n70.2\n50.7\n68.6\n—<br><br><table border=""1"">\n<tr><th>IXC2-4KHD</th><th>8B</th><th>3840x1600</th><th>90.0 (+7.8)</th><th>81.0 (+10.8)</th><th>68.6 (+17.9)</th><th>77.2 (+1.2)</th><th>67.5 (+8.5)</th></tr>\n<tr><td>IXC2-4KHD</td><td>8B</td><td>3840x1600</td><td>90.0 (+7.8)</td><td>81.0 (+10.8)</td><td>68.6 (+17.9)</td><td>77.2 (+1.2)</td><td>67.5 (+8.5)</td></tr>\n"
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[6] Baichuan. Baichuan 2: Open large-scale language models. *arXiv.org*, 2023. 2<br><br>[7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa ̆gnak Tas ̧ırlar. Introducing our multimodal models, 2023. 2<br><br>[8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc ̧al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\n*Proceedings of the IEEE/CVF international conference on*\n*computer vision*, pages 4291–4301, 2019. 6<br><br>[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners."
"","","","","","*Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877–\n1901, 2020."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. *arXiv.org*, 2023. 6<br><br>[57] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player? *arXiv:2307.06281*, 2023. 1, 5, 7<br><br>[58] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\nYin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the\nhidden mystery of ocr in large multimodal models, 2024. 2,\n7<br><br>[59] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,\nShuo Zhang, and Xiang Bai. Textmonkey: An ocr-free\nlarge multimodal model for understanding document."
"","","","","","*arXiv*\n*preprint arXiv:2403.04473*, 2024."
"1732283109","1732283109","null","../test/InternLM-4kimages.pdf","1732285996","[108] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\nZhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130b: An open bilingual pre-\ntrained model. In*The Eleventh International Conference*\n*on Learning Representations (ICLR)*, 2023. 2<br><br>[109] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang,\nand Jiaqi Wang. Long-CLIP: Unlocking the long-text capa-\nbility of clip. *arXiv preprint arXiv:2403.15378*, 2024. 2<br><br>[110] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al. Internlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. *arXiv preprint*\n*arXiv:2309.15112*, 2023."
