{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from autogen import ConversableAgent, UserProxyAgent\n",
    "from autogen.coding import LocalCommandLineCodeExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = \"AIzaSyD939q3PbECaSJO1IAzRbmpqlREgJteLKg\"\n",
    "if not os.environ.get('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = google_api_key\n",
    "\n",
    "if not os.environ.get('AUTOGEN_USE_DOCKER'):\n",
    "    os.environ['AUTOGEN_USE_DOCKER'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_system_prompt = \"\"\"You are a helpful AI assistant with expertise in legal and financial matters and general knowledge of the world. Additionally, you have the ability to write code when required to assist with queries. You will be provided with a query and context relevant to the query.\n",
    "\n",
    "Your task is to:\n",
    "1. Provide a concise and informative response to the query, using only the context provided.\n",
    "2. If you are not confident about the answer based on the given context, politely state that you are not sure about the answer.\n",
    "3. Avoid phrases like \"based on the context provided\" or \"based on the documents\" in your response; just directly answer the query.\n",
    "4. If the context contains contradictory or non-entailing pieces of information relevant to the query:\n",
    "   - Summarize the response incorporating all the relevant information.\n",
    "   - Ask the user if they would like an answer based on any specific piece of information.\n",
    "5. If the query is vague (e.g., \"Tell me about Python syntax\"):\n",
    "   - Provide a summary response based on the entire context.\n",
    "   - Politely ask the user if they can be more specific about what they are looking for.\n",
    "6. If the query involves coding, generate accurate and efficient code solutions and nothing else. Ensure your code is well-commented and tailored to the user's request.\n",
    "\n",
    "Be clear, accurate, and avoid unnecessary elaboration.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy_prompt = \"\"\"If you have more information to share about the same, state it; otherwise, strictly respond with 'done' and nothing else.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_system_prompt = \"Respond concisely and accurately, using the conversation provided and the context specified in the query. The user may reference documents they provided, which will be given to you as context.\\\n",
    "#     You also have a web search tool and a code exeuction tool which can be used to retrieve real-time information or draw insights when necessary.\\\n",
    "#         If extra information is needed to answer the question, use a web search.\"\n",
    "# user_proxy_prompt = \"If you have any new information, state it, otherwise indicate that you are ready for the next query.\"\n",
    "auto_agent = ConversableAgent(name=\"assistant\", human_input_mode=\"NEVER\", system_message=agent_system_prompt,\n",
    "                                llm_config={\"config_list\": [{\"model\": \"gemini-1.5-flash\", \"temperature\": 0.5, \"api_key\": os.environ.get(\"GOOGLE_API_KEY\"), \"api_type\": \"google\"}]},\n",
    "                                code_execution_config=False)\n",
    "executor = LocalCommandLineCodeExecutor(work_dir=\"coding\")\n",
    "user_proxy = UserProxyAgent(name=\"user_proxy\", human_input_mode=\"NEVER\", max_consecutive_auto_reply=1, code_execution_config={'executor': executor},\n",
    "                            default_auto_reply=user_proxy_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "Write a function to print the first 6 alphabets each 3 times.\n",
      "Context:\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "```python\n",
      "def print_alphabets():\n",
      "  \"\"\"Prints the first 6 alphabets each 3 times.\"\"\"\n",
      "  alphabets = \"abcdef\"\n",
      "  for char in alphabets:\n",
      "    print(char * 3)\n",
      "\n",
      "print_alphabets()\n",
      "```\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK (inferred language is python)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: aaa\n",
      "bbb\n",
      "ccc\n",
      "ddd\n",
      "eee\n",
      "fff\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Okay.  Is there anything else I can help you with?\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"Write a function to print the first 6 alphabets each 3 times.\"\n",
    "\n",
    "combined_context = \"\"\n",
    "\n",
    "chat_result = user_proxy.initiate_chat(recipient=auto_agent, message=f\"{query}\\nContext:\\n{combined_context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is the valuation of NVIDIA?\n",
      "Context:\n",
      "\n",
      "## Article 1\n",
      "Uploaded at: 2024-01-01\n",
      "Valuation of NVIDIA is $500 billion.\n",
      "\n",
      "## Article 2\n",
      "Uploaded at: 2023-01-01\n",
      "Valuation of NVIDIA is $400 billion.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "The valuation of NVIDIA is reported as $500 billion (2024) and $400 billion (2023).\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "If you have more information to share about the same, state it; otherwise, strictly respond with 'done' and nothing else.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "done\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the valuation of NVIDIA?\"\n",
    "\n",
    "combined_context = \\\n",
    "\"\"\"\n",
    "## Article 1\n",
    "Uploaded at: 2024-01-01\n",
    "Valuation of NVIDIA is $500 billion.\n",
    "\n",
    "## Article 2\n",
    "Uploaded at: 2023-01-01\n",
    "Valuation of NVIDIA is $400 billion.\n",
    "\"\"\"\n",
    "chat_result = user_proxy.initiate_chat(recipient=auto_agent, message=f\"{query}\\nContext:\\n{combined_context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is the valuation of NVIDIA?\n",
      "Context:\n",
      "\n",
      "## Article 1\n",
      "Uploaded at: 2024-01-01\n",
      "Valuation of NVIDIA is $500 billion.\n",
      "\n",
      "## Article 2\n",
      "Uploaded at: 2023-01-01\n",
      "Valuation of NVIDIA is $400 billion.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Based on the provided articles, NVIDIA's valuation was $500 billion as of January 1st, 2024, and $400 billion as of January 1st, 2023.  A current valuation requires a real-time search.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "If you have any new information, state it, otherwise indicate that you are ready for the next query.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "Ready for the next query.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'What is the valuation of NVIDIA?\\nContext:\\n\\n## Article 1\\nUploaded at: 2024-01-01\\nValuation of NVIDIA is $500 billion.\\n\\n## Article 2\\nUploaded at: 2023-01-01\\nValuation of NVIDIA is $400 billion.\\n', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Based on the provided articles, NVIDIA's valuation was $500 billion as of January 1st, 2024, and $400 billion as of January 1st, 2023.  A current valuation requires a real-time search.\\n\", 'role': 'user', 'name': 'assistant'}, {'content': 'If you have any new information, state it, otherwise indicate that you are ready for the next query.', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Ready for the next query.\\n', 'role': 'user', 'name': 'assistant'}], summary='Ready for the next query.\\n', cost={'usage_including_cached_inference': {'total_cost': 0.004172, 'gemini-1.5-flash': {'cost': 0.004172, 'prompt_tokens': 395, 'completion_tokens': 67, 'total_tokens': 462}}, 'usage_excluding_cached_inference': {'total_cost': 0.00182, 'gemini-1.5-flash': {'cost': 0.00182, 'prompt_tokens': 239, 'completion_tokens': 7, 'total_tokens': 246}}}, human_input=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided articles, NVIDIA's valuation was $500 billion as of January 1st, 2024, and $400 billion as of January 1st, 2023.  A current valuation requires a real-time search.\n",
      "\n",
      "\n",
      "Okay.  I'm ready for your next request.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assistant_responses = []\n",
    "\n",
    "for message in chat_result.chat_history:\n",
    "    if message['name'] == \"assistant\":\n",
    "        assistant_responses.append(message['content'])\n",
    "\n",
    "# Combine all assistant responses into one message\n",
    "unified_response = \"\\n\\n\".join(assistant_responses)\n",
    "print(unified_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_classification_prompt = \"\"\"You are an intelligent AI assistant. You will be provided with a user query. You will have to classify the query into one of the following categories:\n",
    "- **code_execution**: The query requires a code to be executed, such as plotting, performing numerical calculations, or verifying the output of a code. Note: If the user only asks to write the code and not execute it, do not classify it as code_execution.\n",
    "- **summary**: The query is either asking for a summary or it requires a large amount of information to be retrieved and summarized.\n",
    "- **search**: The query is asking for a specific information which can be answered with a single piece of information.\n",
    "- **analysis**: The query is asking for a thorough analysis of every part of some document or text, which may require reasoning and understanding of the text.\n",
    "- **comparison**: The query is asking for a comparison between two or more entities, which may require multiple sources of information.\n",
    "\n",
    "### Examples\n",
    "\n",
    "**Example 1:**\n",
    "User Query: Plot a sine wave.\n",
    "Answer: code_execution\n",
    "**Example 2:**\n",
    "User Query: In how many cricket world cups, India made it to the top 7?\n",
    "Answer: summary\n",
    "**Example 3:**\n",
    "User Query: Who was the captain of the Indian cricket team in the 2011 world cup?\n",
    "Answer: search\n",
    "**Example 4:**\n",
    "User Query: What are the grammatical and logical errors in this document?\n",
    "Answer: analysis\n",
    "**Example 5:**\n",
    "User Query: What are the differences between the election systems of India and the United States?\n",
    "Answer: comparison\n",
    "**Example 6:**\n",
    "User Query: Write a Python function to calculate the factorial of a number.\n",
    "Answer: search\n",
    "\n",
    "User Query: {user_query}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from datetime import datetime\n",
    "llm = Gemini(model=\"models/gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code_execution\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"Analyze this file and plot the data.\"\n",
    "prompt = PromptTemplate((query_classification_prompt))\n",
    "prompt = prompt.format(user_query=user_query)\n",
    "resp = llm.complete(prompt).text\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'search\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"Write a CUDA code for parallel matrix multiplication\"\n",
    "prompt = PromptTemplate((query_classification_prompt))\n",
    "prompt = prompt.format(user_query=user_query)\n",
    "resp = llm.complete(prompt).text\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
