{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaryan/penvs/pathway_dev/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "import os\n",
    "from pprint import pprint\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Google Gemini model with LangChain\n",
    "google_api_key = \"AIzaSyD939q3PbECaSJO1IAzRbmpqlREgJteLKg\"\n",
    "if not os.environ.get('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = google_api_key\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unrelatedness and reformulate query\n",
    "Reformulate the query based on the chat history (if it exists). However, if the query is completely unrelated to the chat history, return **\"Unrelated\"**.\n",
    "<br>**Note** that it is not necessary to use the attached document (if any) to reformulate the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryResolution(BaseModel):\n",
    "    \"\"\"\n",
    "    Query resolution.\n",
    "    Returns:\n",
    "        - query_type: str\n",
    "        - output: str\n",
    "    \"\"\"\n",
    "    query_type: str = Field(description=\"The type of the query\")\n",
    "    output: str = Field(description=\"The ouput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a case of \"query is sufficient\" case\n",
    "reformulation_system_prompt = SystemMessage(\n",
    "content=\"\"\"You are an intelligent AI assistant. You will be provided with a user query and the chat history between the user and the chatbot. You will have to identify the type of the query, and give your output according to the following rules:\n",
    "- If the context of query is similar to the chat history, reformulate the query based on the chat history. Ensure that the reformulated query is as detailed and contextually rich as possible. Respond with query type as 'reformulation' and output as the reformulated query.\n",
    "- If the query is general and unrelated to the chat history and doesn't require any particular information to be answered, then respond with query type as 'general' and output as a polite response to the query.\n",
    "- If the query is completely unrelated to the chat history and require additional information to be answered, respond with query type as 'unrelated' and output the gramatically corrected query.\n",
    "\n",
    "Examples are provided below.\n",
    "\n",
    "Example 1:\n",
    "Chat History:\n",
    "user: Who discovered the laws of motion?\n",
    "ai: Isaac Newton\n",
    "User Query: Tell me more about him\n",
    "Query Type: reformulation\n",
    "Output: Tell me more about Isaac Newton who discovered the laws of motion.\n",
    "\n",
    "Example 2:\n",
    "Chat History:\n",
    "\n",
    "User Query: How are you?\n",
    "Query Type: general\n",
    "Output: I am doing well, thank you for asking. How can I help you today?\n",
    "\n",
    "Example 3:\n",
    "Chat History:\n",
    "user: Who discovered the laws of motion?\n",
    "ai: Isaac Newton\n",
    "User Query: Waht is the capital of France?\n",
    "Query Type: unrelated\n",
    "Output: What is the capital of France?\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user: Who is the author of the book 'Pride and Prejudice'?\\nai: The author of the book 'Pride and Prejudice' is Jane Austen.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n",
    "chat_history.save_context(\n",
    "    {'input': \"Who is the author of the book 'Pride and Prejudice'?\"},\n",
    "    {'output': \"The author of the book 'Pride and Prejudice' is Jane Austen.\"}\n",
    ")\n",
    "\n",
    "history = chat_history.load_memory_variables({})\n",
    "history['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are an intelligent AI assistant. You will be provided with a user query and the chat history between the user and the chatbot. You will have to identify the type of the query, and give your output according to the following rules:\\n- If the context of query is similar to the chat history, reformulate the query based on the chat history. Ensure that the reformulated query is as detailed and contextually rich as possible. Respond with query type as 'reformulation' and output as the reformulated query.\\n- If the query is general and unrelated to the chat history and doesn't require any particular information to be answered, then respond with query type as 'general' and output as a polite response to the query.\\n- If the query is completely unrelated to the chat history and require additional information to be answered, respond with query type as 'unrelated' and output the gramatically corrected query.\\n\\nExamples are provided below.\\n\\nExample 1:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Tell me more about him\\nQuery Type: reformulation\\nOutput: Tell me more about Isaac Newton who discovered the laws of motion.\\n\\nExample 2:\\nChat History:\\n\\nUser Query: How are you?\\nQuery Type: general\\nOutput: I am doing well, thank you for asking. How can I help you today?\\n\\nExample 3:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Waht is the capital of France?\\nQuery Type: unrelated\\nOutput: What is the capital of France?\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content=\"Chat History:\\nuser: Who is the author of the book 'Pride and Prejudice'?\\nai: The author of the book 'Pride and Prejudice' is Jane Austen.\\nUser Query: Tell me more about her\\n\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Tell me more about her\"\n",
    "reformulation_prompt = f\"\"\"Chat History:\n",
    "{history['history']}\n",
    "User Query: {user_prompt}\n",
    "\"\"\"\n",
    "template = [\n",
    "    reformulation_system_prompt,\n",
    "    HumanMessage(content=reformulation_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(chat_history=history['history'], user_prompt=user_prompt)\n",
    "pprint(final_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResolution(query_type='reformulation', output='Tell me more about Jane Austen who wrote the book \"Pride and Prejudice.\"')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = model.with_structured_output(QueryResolution)\n",
    "structured_llm.invoke(\n",
    "    final_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are an intelligent AI assistant. You will be provided with a user query and the chat history between the user and the chatbot. You will have to identify the type of the query, and give your output according to the following rules:\\n- If the context of query is similar to the chat history, reformulate the query based on the chat history. Ensure that the reformulated query is as detailed and contextually rich as possible. Respond with query type as 'reformulation' and output as the reformulated query.\\n- If the query is general and unrelated to the chat history and doesn't require any particular information to be answered, then respond with query type as 'general' and output as a polite response to the query.\\n- If the query is completely unrelated to the chat history and require additional information to be answered, respond with query type as 'unrelated' and output the gramatically corrected query.\\n\\nExamples are provided below.\\n\\nExample 1:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Tell me more about him\\nQuery Type: reformulation\\nOutput: Tell me more about Isaac Newton who discovered the laws of motion.\\n\\nExample 2:\\nChat History:\\n\\nUser Query: How are you?\\nQuery Type: general\\nOutput: I am doing well, thank you for asking. How can I help you today?\\n\\nExample 3:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Waht is the capital of France?\\nQuery Type: unrelated\\nOutput: What is the capital of France?\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content=\"Chat History:\\nuser: Who was the 40th president of the United States?\\nai: Ronald Reagan\\nUser Query: Btw, what's the weather in Delhi?\\n\", additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QueryResolution(query_type='unrelated', output='What is the weather in Delhi?')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n",
    "chat_history.save_context(\n",
    "    {'input': \"Who was the 40th president of the United States?\"},\n",
    "    {'output': \"Ronald Reagan\"}\n",
    ")\n",
    "\n",
    "history = chat_history.load_memory_variables({})\n",
    "\n",
    "user_prompt = \"Btw, what's the weather in Delhi?\"\n",
    "reformulation_prompt = f\"\"\"Chat History:\n",
    "{history['history']}\n",
    "User Query: {user_prompt}\n",
    "\"\"\"\n",
    "template = [\n",
    "    reformulation_system_prompt,\n",
    "    HumanMessage(content=reformulation_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(chat_history=history['history'], user_prompt=user_prompt)\n",
    "pprint(final_prompt.messages)\n",
    "\n",
    "response = structured_llm.invoke(final_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResolution(query_type='general', output='Hello there! How can I assist you today?')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n",
    "# chat_history.save_context(\n",
    "#     {'input': \"Hi\"},\n",
    "#     {'output': \"Ronald Reagan\"}\n",
    "# )\n",
    "\n",
    "history = chat_history.load_memory_variables({})\n",
    "\n",
    "user_prompt = \"Hello\"\n",
    "reformulation_prompt = f\"\"\"Chat History:\n",
    "{history['history']}\n",
    "User Query: {user_prompt}\n",
    "\"\"\"\n",
    "template = [\n",
    "    reformulation_system_prompt,\n",
    "    HumanMessage(content=reformulation_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(chat_history=history['history'], user_prompt=user_prompt)\n",
    "\n",
    "response = structured_llm.invoke(final_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevancy of the retrieved documents\n",
    "Problems:\n",
    "- Should we check for \"relevancy\" or \"sufficiency\"?\n",
    "- What if the documents are not sufficient but somewhat relevant? How to retrieve more necessary information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
