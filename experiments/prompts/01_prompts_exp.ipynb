{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts.chat import ChatPromptTemplate\n",
    "# from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "# from langchain_core.prompts import MessagesPlaceholder\n",
    "import os\n",
    "# from pprint import pprint\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Google Gemini model with LangChain\n",
    "google_api_key = \"AIzaSyD939q3PbECaSJO1IAzRbmpqlREgJteLKg\"\n",
    "if not os.environ.get('GOOGLE_API_KEY'):\n",
    "    os.environ['GOOGLE_API_KEY'] = google_api_key\n",
    "# model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unrelatedness and reformulate query\n",
    "Reformulate the query based on the chat history (if it exists). However, if the query is completely unrelated to the chat history, return **\"Unrelated\"**.\n",
    "<br>**Note** that it is not necessary to use the attached document (if any) to reformulate the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryResolution(BaseModel):\n",
    "    \"\"\"\n",
    "    Query resolution.\n",
    "    Returns:\n",
    "        - query_type: str\n",
    "        - output: str\n",
    "    \"\"\"\n",
    "    query_type: str = Field(description=\"The type of the query\")\n",
    "    output: str = Field(description=\"The ouput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a case of \"query is sufficient\" case\n",
    "reformulation_system_prompt = SystemMessage(\n",
    "content=\"\"\"You are an intelligent AI assistant. You will be provided with a user query and the chat history between the user and the chatbot. You will have to identify the type of the query, and give your output strictly according to the following rules:\n",
    "- If the context of query is similar to the chat history, reformulate the query based on the chat history. Ensure that the reformulated query is as detailed and contextually rich as possible. Respond with query type as 'reformulation' and output as the reformulated query.\n",
    "- If the query is general and unrelated to the chat history and doesn't require any particular information to be answered, then respond with query type as 'general' and output as a polite response to the query.\n",
    "- If the query is completely unrelated to the chat history and require additional information to be answered, respond with query type as 'unrelated' and output the gramatically corrected query.\n",
    "\n",
    "Examples are provided below.\n",
    "\n",
    "Example 1:\n",
    "Chat History:\n",
    "user: Who discovered the laws of motion?\n",
    "ai: Isaac Newton\n",
    "User Query: Tell me more about him\n",
    "Query Type: reformulation\n",
    "Output: Tell me more about Isaac Newton who discovered the laws of motion.\n",
    "\n",
    "Example 2:\n",
    "Chat History:\n",
    "\n",
    "User Query: How are you?\n",
    "Query Type: general\n",
    "Output: I am doing well, thank you for asking. How can I help you today?\n",
    "\n",
    "Example 3:\n",
    "Chat History:\n",
    "user: Who discovered the laws of motion?\n",
    "ai: Isaac Newton\n",
    "User Query: Waht is the capital of France?\n",
    "Query Type: unrelated\n",
    "Output: What is the capital of France?\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_874608/4214346600.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"user: Who is the author of the book 'Pride and Prejudice'?\\nai: The author of the book 'Pride and Prejudice' is Jane Austen.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n",
    "chat_history.save_context(\n",
    "    {'input': \"Who is the author of the book 'Pride and Prejudice'?\"},\n",
    "    {'output': \"The author of the book 'Pride and Prejudice' is Jane Austen.\"}\n",
    ")\n",
    "\n",
    "history = chat_history.load_memory_variables({})\n",
    "history['history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are an intelligent AI assistant. You will be provided with a user query and the chat history between the user and the chatbot. You will have to identify the type of the query, and give your output strictly according to the following rules:\\n- If the context of query is similar to the chat history, reformulate the query based on the chat history. Ensure that the reformulated query is as detailed and contextually rich as possible. Respond with query type as 'reformulation' and output as the reformulated query.\\n- If the query is general and unrelated to the chat history and doesn't require any particular information to be answered, then respond with query type as 'general' and output as a polite response to the query.\\n- If the query is completely unrelated to the chat history and require additional information to be answered, respond with query type as 'unrelated' and output the gramatically corrected query.\\n\\nExamples are provided below.\\n\\nExample 1:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Tell me more about him\\nQuery Type: reformulation\\nOutput: Tell me more about Isaac Newton who discovered the laws of motion.\\n\\nExample 2:\\nChat History:\\n\\nUser Query: How are you?\\nQuery Type: general\\nOutput: I am doing well, thank you for asking. How can I help you today?\\n\\nExample 3:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Waht is the capital of France?\\nQuery Type: unrelated\\nOutput: What is the capital of France?\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content=\"Chat History:\\nuser: Who is the author of the book 'Pride and Prejudice'?\\nai: The author of the book 'Pride and Prejudice' is Jane Austen.\\nUser Query: Tell me more about her\\n\", additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Tell me more about her\"\n",
    "reformulation_prompt = f\"\"\"Chat History:\n",
    "{history['history']}\n",
    "User Query: {user_prompt}\n",
    "\"\"\"\n",
    "template = [\n",
    "    reformulation_system_prompt,\n",
    "    HumanMessage(content=reformulation_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(chat_history=history['history'], user_prompt=user_prompt)\n",
    "pprint(final_prompt.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResolution(query_type='reformulation', output='Tell me more about Jane Austen, the author of the book \\\\\\\\\"Pride and Prejudice\\\\\\\\\".')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_llm = model.with_structured_output(QueryResolution)\n",
    "structured_llm.invoke(\n",
    "    final_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content=\"You are an intelligent AI assistant. You will be provided with a user query and the chat history between the user and the chatbot. You will have to identify the type of the query, and give your output strictly according to the following rules:\\n- If the context of query is similar to the chat history, reformulate the query based on the chat history. Ensure that the reformulated query is as detailed and contextually rich as possible. Respond with query type as 'reformulation' and output as the reformulated query.\\n- If the query is general and unrelated to the chat history and doesn't require any particular information to be answered, then respond with query type as 'general' and output as a polite response to the query.\\n- If the query is completely unrelated to the chat history and require additional information to be answered, respond with query type as 'unrelated' and output the gramatically corrected query.\\n\\nExamples are provided below.\\n\\nExample 1:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Tell me more about him\\nQuery Type: reformulation\\nOutput: Tell me more about Isaac Newton who discovered the laws of motion.\\n\\nExample 2:\\nChat History:\\n\\nUser Query: How are you?\\nQuery Type: general\\nOutput: I am doing well, thank you for asking. How can I help you today?\\n\\nExample 3:\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Waht is the capital of France?\\nQuery Type: unrelated\\nOutput: What is the capital of France?\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
      " HumanMessage(content=\"Chat History:\\nuser: Who was the 40th president of the United States?\\nai: Ronald Reagan\\nUser Query: Btw, what's the weather in Delhi?\\n\", additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QueryResolution(query_type='unrelated', output='What is the current weather in Delhi?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n",
    "chat_history.save_context(\n",
    "    {'input': \"Who was the 40th president of the United States?\"},\n",
    "    {'output': \"Ronald Reagan\"}\n",
    ")\n",
    "\n",
    "history = chat_history.load_memory_variables({})\n",
    "\n",
    "user_prompt = \"Btw, what's the weather in Delhi?\"\n",
    "reformulation_prompt = f\"\"\"Chat History:\n",
    "{history['history']}\n",
    "User Query: {user_prompt}\n",
    "\"\"\"\n",
    "template = [\n",
    "    reformulation_system_prompt,\n",
    "    HumanMessage(content=reformulation_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(chat_history=history['history'], user_prompt=user_prompt)\n",
    "pprint(final_prompt.messages)\n",
    "\n",
    "response = structured_llm.invoke(final_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResolution(query_type='general', output='Hello there! How can I help you today?')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = ConversationBufferMemory(human_prefix=\"user\", ai_prefix='ai')\n",
    "# chat_history.save_context(\n",
    "#     {'input': \"Hi\"},\n",
    "#     {'output': \"Ronald Reagan\"}\n",
    "# )\n",
    "\n",
    "history = chat_history.load_memory_variables({})\n",
    "\n",
    "user_prompt = \"Hello\"\n",
    "reformulation_prompt = f\"\"\"Chat History:\n",
    "{history['history']}\n",
    "User Query: {user_prompt}\n",
    "\"\"\"\n",
    "template = [\n",
    "    reformulation_system_prompt,\n",
    "    HumanMessage(content=reformulation_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(chat_history=history['history'], user_prompt=user_prompt)\n",
    "\n",
    "response = structured_llm.invoke(final_prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query classification prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_classification_sys_prompt = SystemMessage(\n",
    "content=\"\"\"You are an intelligent AI assistant. You will be provided with a user query. Your will have to classify the query into one of the following categories:\n",
    "- **summary**: The query is either asking for a summary or it requires a large amount of information to be retrieved and summarized.\n",
    "- **search**: The query is asking for a specific information which can be answered with a single piece of information.\n",
    "- **analysis**: The query is asking for a thorough analysis of every part of some document or text, which may require reasoning and understanding of the text.\n",
    "- **comparison**: The query is asking for a comparison between two or more entities, which may require multiple sources of information.\n",
    "\n",
    "Examples are provided below.\n",
    "\n",
    "Example 1:\n",
    "User Query: In how many cricket world cups, India made it to the top 7?\n",
    "Answer: summary\n",
    "Example 2:\n",
    "User Query: Who was the captain of the Indian cricket team in the 2011 world cup?\n",
    "Answer: search\n",
    "Example 3:\n",
    "User Query: What are the grammatical and logical errors in this document?\n",
    "Answer: analysis\n",
    "Example 4:\n",
    "User Query: What are the differences between the election systems of India and the United States?\n",
    "Answer: comparison\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='analysis\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a83c7bbf-5828-45c1-b22d-e2ea10b401ad-0', usage_metadata={'input_tokens': 275, 'output_tokens': 2, 'total_tokens': 277, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What are the possible loopholes in this document?\"\n",
    "classification_prompt = f\"\"\"User Query: {user_prompt}\n",
    "Answer:\"\"\"\n",
    "template = [\n",
    "    query_classification_sys_prompt,\n",
    "    HumanMessage(content=classification_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(user_prompt=user_prompt)\n",
    "# pprint(final_prompt.messages)\n",
    "model.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='search\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-3133f4d2-04f1-48b9-aed4-fd1ffcbf936b-0', usage_metadata={'input_tokens': 271, 'output_tokens': 2, 'total_tokens': 273, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"Where did Einstein studied?\"\n",
    "classification_prompt = f\"\"\"User Query: {user_prompt}\n",
    "Answer:\"\"\"\n",
    "template = [\n",
    "    query_classification_sys_prompt,\n",
    "    HumanMessage(content=classification_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(user_prompt=user_prompt)\n",
    "# pprint(final_prompt.messages)\n",
    "model.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='comparison\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-a68b9250-9fc6-4f42-8c60-33d3dbfb234d-0', usage_metadata={'input_tokens': 289, 'output_tokens': 2, 'total_tokens': 291, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_prompt = \"In the last 50 years, compare the percentage of investement in the stock market and real estate.\"\n",
    "user_prompt = \"What are the sections in an intellectual property agreement?\"\n",
    "classification_prompt = f\"\"\"User Query: {user_prompt}\n",
    "Answer:\"\"\"\n",
    "template = [\n",
    "    query_classification_sys_prompt,\n",
    "    HumanMessage(content=classification_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(user_prompt=user_prompt)\n",
    "# pprint(final_prompt.messages)\n",
    "model.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='summary\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-bd4901c0-e130-463c-949c-f14ffa1e7568-0', usage_metadata={'input_tokens': 276, 'output_tokens': 2, 'total_tokens': 278, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What are the sections in an intellectual property agreement?\"\n",
    "classification_prompt = f\"\"\"User Query: {user_prompt}\n",
    "Answer:\"\"\"\n",
    "template = [\n",
    "    query_classification_sys_prompt,\n",
    "    HumanMessage(content=classification_prompt)\n",
    "]\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(template).format_prompt(user_prompt=user_prompt)\n",
    "# pprint(final_prompt.messages)\n",
    "model.invoke(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge response type and reformat prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prompt = \"\"\"You are an intelligent AI assistant. The current date in YYYY-MM-DD format is {current_date}. You will be provided with:\n",
    "1. Chat history between the user and the chatbot.\n",
    "2. A user-uploaded document, which might be a summary of a larger document rather than the complete document.\n",
    "3. A user query.\n",
    "\n",
    "Your task is to determine the query type and generate an output according to the following rules:\n",
    "- If the query is chit-chat or conversational in nature (e.g., greetings like 'hello', 'hi', or expressions of gratitude like 'thanks') and unrelated to the chat history or document and doesn't require any particular information to be answered, respond with query type as 'general' and output as a polite response to the query.\n",
    "- If the document or chat history is sufficiently relevant to the query (not necessarily complete), respond with query type as 'direct' and output as the reformulated query based on the chat history and/or document.\n",
    "- If the query requires additional context not sufficiently present in the chat history or document, respond with query type as 'context' and output as the reformulated query.\n",
    "\n",
    "**Note:**\n",
    "- The reformulated query should always be grammatically correct and contextually rich.\n",
    "- Only provide the Query Type and Output. Do not provide any other explanation or response.\n",
    "\n",
    "### Examples\n",
    "\n",
    "**Example 1:**\n",
    "Chat History:\n",
    "user: Who discovered the laws of motion?\n",
    "ai: Isaac Newton\n",
    "User Query: Tell me more abt him\n",
    "User Uploaded Document:\n",
    "\n",
    "Query Type: context\n",
    "Output: Tell me more about Isaac Newton who discovered the laws of motion.\n",
    "\n",
    "**Example 2:**\n",
    "Chat History:\n",
    "\n",
    "User Uploaded Document:\n",
    "\n",
    "User Query: How are you?\n",
    "Query Type: general\n",
    "Output: I am doing well, thank you for asking. How can I help you today?\n",
    "\n",
    "**Example 3:**\n",
    "Chat History:\n",
    "user: Who discovered the laws of motion?\n",
    "ai: Isaac Newton\n",
    "User Uploaded Document:\n",
    "\n",
    "User Query: Waht is the capital of France?\n",
    "Query Type: context\n",
    "Output: What is the capital of France?\n",
    "\n",
    "**Example 4:**\n",
    "Chat History:\n",
    "\n",
    "User Uploaded Document:\n",
    "The document is a financial report of Google Inc.\n",
    "User Query: What is the revenue in last quarter?\n",
    "Query Type: direct\n",
    "Output: What is the revenue of Google Inc. in the last quarter?\n",
    "\n",
    "**Example 5:**\n",
    "Chat History:\n",
    "\n",
    "User Uploaded Document:\n",
    "The Super Bowl is the annual league championship game of the National Football League (NFL) of the United States.\n",
    "User Query: When is the next super bowl happening?\n",
    "Query Type: context\n",
    "Output: When is the next Super Bowl annual league championship happening in United States?\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "User Uploaded Document:\n",
    "{document}\n",
    "User Query: {user_query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Gemini(model=\"models/gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: context\n",
      "Output: Write a Python code to print the first four letters of the alphabet.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Query Type: context\\nOutput: Write a Python code to print the first four letters of the alphabet.\\n', additional_kwargs={}, raw={'content': {'parts': [{'text': 'Query Type: context\\nOutput: Write a Python code to print the first four letters of the alphabet.\\n'}], 'role': 'model'}, 'finish_reason': 1, 'avg_logprobs': -0.004597541283477436, 'safety_ratings': [], 'token_count': 0, 'grounding_attributions': [], 'block_reason': 0, 'usage_metadata': {'prompt_token_count': 612, 'candidates_token_count': 22, 'total_token_count': 634, 'cached_content_token_count': 0}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate((merged_prompt))\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_history = \"\"\n",
    "user_query = \"Write a python code to print first 4 alphabets\"\n",
    "user_document = \"\"\n",
    "prompt = prompt.format(chat_history=chat_history, user_query=user_query, current_date=current_date, document=user_document)\n",
    "resp = llm.complete(prompt)\n",
    "print(resp)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>}, template_vars=['current_date', 'chat_history', 'document', 'user_query'], kwargs={}, output_parser=None, template_var_mappings=None, function_mappings=None, template=\"You are an intelligent AI assistant. The current date in YYYY-MM-DD format is {current_date}. You will be provided with:\\n1. Chat history between the user and the chatbot.\\n2. A user-uploaded document, which might be a summary of a larger document rather than the complete document.\\n3. A user query.\\n\\nYour task is to determine the query type and generate an output according to the following rules:\\n- If the query is chit-chat or conversational in nature (e.g., greetings like 'hello', 'hi', or expressions of gratitude like 'thanks') and unrelated to the chat history or document and doesn't require any particular information to be answered, respond with query type as 'general' and output as a polite response to the query.\\n- If the document or chat history is sufficiently relevant to the query (not necessarily complete), respond with query type as 'direct' and output as the reformulated query based on the chat history and/or document.\\n- If the query requires additional context not sufficiently present in the chat history or document, respond with query type as 'context' and output as the reformulated query.\\n\\n**Note:**\\n- The reformulated query should always be grammatically correct and contextually rich.\\n- Only provide the Query Type and Output. Do not provide any other explanation or response.\\n\\n### Examples\\n\\n**Example 1:**\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Query: Tell me more abt him\\nUser Uploaded Document:\\n\\nQuery Type: context\\nOutput: Tell me more about Isaac Newton who discovered the laws of motion.\\n\\n**Example 2:**\\nChat History:\\n\\nUser Uploaded Document:\\n\\nUser Query: How are you?\\nQuery Type: general\\nOutput: I am doing well, thank you for asking. How can I help you today?\\n\\n**Example 3:**\\nChat History:\\nuser: Who discovered the laws of motion?\\nai: Isaac Newton\\nUser Uploaded Document:\\n\\nUser Query: Waht is the capital of France?\\nQuery Type: context\\nOutput: What is the capital of France?\\n\\n**Example 4:**\\nChat History:\\n\\nUser Uploaded Document:\\nThe document is a financial report of Google Inc.\\nUser Query: What is the revenue in last quarter?\\nQuery Type: direct\\nOutput: What is the revenue of Google Inc. in the last quarter?\\n\\n**Example 5:**\\nChat History:\\n\\nUser Uploaded Document:\\nThe Super Bowl is the annual league championship game of the National Football League (NFL) of the United States.\\nUser Query: When is the next super bowl happening?\\nQuery Type: context\\nOutput: When is the next Super Bowl annual league championship happening in United States?\\n\\nChat History:\\n{chat_history}\\nUser Uploaded Document:\\n{document}\\nUser Query: {user_query}\\n\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate((merged_prompt))\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"user: Who is the author of the book 'Pride and Prejudice'?\\nai: The author of the book 'Pride and Prejudice' is Jane Austen.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_history = [\n",
    "    \"user: Who is the author of the book 'Pride and Prejudice'?\",\n",
    "    \"ai: The author of the book 'Pride and Prejudice' is Jane Austen.\"\n",
    "]\n",
    "chat_history = '\\n'.join(chat_history)\n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an intelligent AI assistant. The current date in YYYY-MM-DD format is 2024-12-04. You will be provided with:\n",
      "1. Chat history between the user and the chatbot.\n",
      "2. A user-uploaded document, which might be a summary of a larger document rather than the complete document.\n",
      "3. A user query.\n",
      "\n",
      "Your task is to determine the query type and generate an output according to the following rules:\n",
      "- If the query is chit-chat or conversational in nature (e.g., greetings like 'hello', 'hi', or expressions of gratitude like 'thanks') and unrelated to the chat history or document and doesn't require any particular information to be answered, respond with query type as 'general' and output as a polite response to the query.\n",
      "- If the document or chat history is sufficiently relevant to the query (not necessarily complete), respond with query type as 'direct' and output as the reformulated query based on the chat history and/or document.\n",
      "- If the query requires additional context not sufficiently present in the chat history or document, respond with query type as 'context' and output as the reformulated query.\n",
      "\n",
      "**Note:**\n",
      "- The reformulated query should always be grammatically correct and contextually rich.\n",
      "- Only provide the Query Type and Output. Do not provide any other explanation or response.\n",
      "\n",
      "### Examples\n",
      "\n",
      "**Example 1:**\n",
      "Chat History:\n",
      "user: Who discovered the laws of motion?\n",
      "ai: Isaac Newton\n",
      "User Query: Tell me more abt him\n",
      "User Uploaded Document:\n",
      "\n",
      "Query Type: context\n",
      "Output: Tell me more about Isaac Newton who discovered the laws of motion.\n",
      "\n",
      "**Example 2:**\n",
      "Chat History:\n",
      "\n",
      "User Uploaded Document:\n",
      "\n",
      "User Query: How are you?\n",
      "Query Type: general\n",
      "Output: I am doing well, thank you for asking. How can I help you today?\n",
      "\n",
      "**Example 3:**\n",
      "Chat History:\n",
      "user: Who discovered the laws of motion?\n",
      "ai: Isaac Newton\n",
      "User Uploaded Document:\n",
      "\n",
      "User Query: Waht is the capital of France?\n",
      "Query Type: context\n",
      "Output: What is the capital of France?\n",
      "\n",
      "**Example 4:**\n",
      "Chat History:\n",
      "\n",
      "User Uploaded Document:\n",
      "The document is a financial report of Google Inc.\n",
      "User Query: What is the revenue in last quarter?\n",
      "Query Type: direct\n",
      "Output: What is the revenue of Google Inc. in the last quarter?\n",
      "\n",
      "**Example 5:**\n",
      "Chat History:\n",
      "\n",
      "User Uploaded Document:\n",
      "The Super Bowl is the annual league championship game of the National Football League (NFL) of the United States.\n",
      "User Query: When is the next super bowl happening?\n",
      "Query Type: context\n",
      "Output: When is the next Super Bowl annual league championship happening in United States?\n",
      "\n",
      "Chat History:\n",
      "user: Who is the author of the book 'Pride and Prejudice'?\n",
      "ai: The author of the book 'Pride and Prejudice' is Jane Austen.\n",
      "User Uploaded Document:\n",
      "\n",
      "User Query: Tell me more about her\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Tell me more about her\"\n",
    "user_document = \"\"\n",
    "prompt = prompt.format(chat_history=chat_history, user_query=user_query, current_date=current_date, document=user_document)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: context\n",
      "Output: Tell me more about Jane Austen, the author of 'Pride and Prejudice'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = llm.complete(prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: context\n",
      "Output: What is the current weather in Delhi?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Query Type: context\\nOutput: What is the current weather in Delhi?\\n', additional_kwargs={}, raw={'content': {'parts': [{'text': 'Query Type: context\\nOutput: What is the current weather in Delhi?\\n'}], 'role': 'model'}, 'finish_reason': 1, 'avg_logprobs': -0.00826477725058794, 'safety_ratings': [], 'token_count': 0, 'grounding_attributions': [], 'block_reason': 0, 'usage_metadata': {'prompt_token_count': 634, 'candidates_token_count': 16, 'total_token_count': 650, 'cached_content_token_count': 0}}, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate((merged_prompt))\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_history = [\n",
    "    \"user: Who was the 40th president of the United States?\",\n",
    "    \"ai: Ronald Reagan\"\n",
    "]\n",
    "chat_history = '\\n'.join(chat_history)\n",
    "user_query = \"Btw, what's the weather in Delhi?\"\n",
    "user_document = \"\"\n",
    "prompt = prompt.format(chat_history=chat_history, user_query=user_query, current_date=current_date, document=user_document)\n",
    "resp = llm.complete(prompt)\n",
    "print(resp)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: general\n",
      "Output: Hey there!  How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate((merged_prompt))\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_history = \"\"\n",
    "user_query = \"Yo, wassup?\"\n",
    "user_document = \"\"\n",
    "prompt = prompt.format(chat_history=chat_history, user_query=user_query, current_date=current_date, document=user_document)\n",
    "resp = llm.complete(prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: direct\n",
      "Output: Based on the provided document, how do I run the \"WFS_co21btech11001.cpp\" and \"OBS_co21btech11001.cpp\" programs using the command line interface (CLI)?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate((merged_prompt))\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_history = \"\"\n",
    "user_document = \"\"\"1. There are two programs, one for Wait-Free implementation and other for Obstruction-Free implementation.\n",
    "2. To compile the program, use the following command:\n",
    "    g++ WFS_co21btech11001.cpp -o wfs\n",
    "    Similarly,\n",
    "    g++ OBS_co21btech11001.cpp -o obs\n",
    "3. To run the program, use the following command:\n",
    "    ./wfs inp-params.txt\n",
    "    Similarly,\n",
    "    ./obs inp-params.txt\n",
    "4. The program named \"WFS_co21btech11001.cpp\" will output a log file named \"LogFile_WFS.txt\".\n",
    "5. The program named \"OBS_co21btech11001.cpp\" will output a log file named \"LogFile_OBS.txt\".\n",
    "6. Both of the programs will output the input parameters (which are read from the input file),\n",
    "   the average operation time and the worst case time in stdout.\"\"\"\n",
    "user_query = \"What is CLI? How to run the program?\"\n",
    "prompt = prompt.format(chat_history=chat_history, user_query=user_query, current_date=current_date, document=user_document)\n",
    "resp = llm.complete(prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Type: context\n",
      "Output: Who won the Nobel Prize in Physics in 2024?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate((merged_prompt))\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "chat_history = \"\"\n",
    "user_document = \"\"\n",
    "user_query = \"Who won the Nobel Prize in Physics this year?\"\n",
    "prompt = prompt.format(chat_history=chat_history, user_query=user_query, current_date=current_date, document=user_document)\n",
    "resp = llm.complete(prompt)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pathway_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
