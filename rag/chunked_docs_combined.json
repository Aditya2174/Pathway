{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "inal CLIP visual encoder. Similarly, CogAgent [36] and\nMini-Gemini [48] also separate HR and LR images using\ndistinct vision encoders, subsequently merging their fea-\ntures using a cross-attention module. In contrast, our ap-\nproach offers a more simplified solution and shows advan-\ntages for varying resolutions and aspect ratio inputs. (2)\nCropped image patches [37, 46, 50, 51, 59, 99, 101]. For\nexample, Monkey [50] employs sliding windows to seg-\nment images into patches, subsequently processing them\nwith LoRA fine-tuning. TextMonkey [59] further proposes\nshifted window attention and token resampler to consider\nthe connections among different patches. These approaches\nare confined to either a few predefined high-resolution set-\ntings [36, 46, 48, 50, 51, 55, 59, 66, 97] or a limited range of\nresolutions [37, 99]. Conversely, our method devises a dy-\nnamic image partition strategy to support the scaling from\n336 pixels to 4K resolution, and the maximum resolution is\nlarger than previous approaches (*e.g*., 1.5k for Monkey [50]\nand 2k for UReader [101]).",
            {
                "chunk_size": "small"
            }
        ],
        [
            "inal CLIP visual encoder. Similarly, CogAgent [36] and\nMini-Gemini [48] also separate HR and LR images using\ndistinct vision encoders, subsequently merging their fea-\ntures using a cross-attention module. In contrast, our ap-\nproach offers a more simplified solution and shows advan-\ntages for varying resolutions and aspect ratio inputs. (2)\nCropped image patches [37, 46, 50, 51, 59, 99, 101]. For\nexample, Monkey [50] employs sliding windows to seg-\nment images into patches, subsequently processing them\nwith LoRA fine-tuning. TextMonkey [59] further proposes\nshifted window attention and token resampler to consider\nthe connections among different patches. These approaches\nare confined to either a few predefined high-resolution set-\ntings [36, 46, 48, 50, 51, 55, 59, 66, 97] or a limited range of\nresolutions [37, 99]. Conversely, our method devises a dy-\nnamic image partition strategy to support the scaling from\n336 pixels to 4K resolution, and the maximum resolution is\nlarger than previous approaches (*e.g*., 1.5k for Monkey [50]\nand 2k for UReader [101]).",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,\nJiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi\nWang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong\nXiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,\nXiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing\nYu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,\nPeng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\nWenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue\nZhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe\nZhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng\nQiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024. 1, 2<br><br>[11] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Ji-\naqi Wang. DualFocus: Integrating macro and micro per-\nspectives in multi-modal large language models. *arXiv*\n*preprint arXiv:2402.14767*, 2024. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,\nJiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi\nWang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong\nXiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,\nXiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing\nYu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,\nPeng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\nWenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue\nZhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe\nZhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng\nQiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024. 1, 2<br><br>[11] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Ji-\naqi Wang. DualFocus: Integrating macro and micro per-\nspectives in multi-modal large language models. *arXiv*\n*preprint arXiv:2402.14767*, 2024. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "90\n85\n80\n75\n70\n65\n60\n55\n50\n*\n*Val*\n*Val*\n*Val*\n*Test*\n*Test*\n*EN-Test*\nInfoVQA\nDocVQA\nTextVQA\nChartQA\nMMBench\nMME\nSeed\nAI2D\nHD-9 (1561 Tokens)\nHD-16 (2653 Tokens)\nHD-25 (4057 Tokens)\n4K HD (8737 Tokens)<br><br>Figure 5. **Influence of Training Resolution. **High-resolution training is critical for HD-OCR tasks, while its gain on other tasks is minor.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "90\n85\n80\n75\n70\n65\n60\n55\n50\n*\n*Val*\n*Val*\n*Val*\n*Test*\n*Test*\n*EN-Test*\nInfoVQA\nDocVQA\nTextVQA\nChartQA\nMMBench\nMME\nSeed\nAI2D\nHD-9 (1561 Tokens)\nHD-16 (2653 Tokens)\nHD-25 (4057 Tokens)\n4K HD (8737 Tokens)<br><br>Figure 5. **Influence of Training Resolution. **High-resolution training is critical for HD-OCR tasks, while its gain on other tasks is minor.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "here*pw*and*ph*represent the number of patches in each row\nand column, respectively. We then split the \u02c6*x*into*ph \u00d7*\n*pw*non-overlapped patches. Each patch is a small image\nwith 336*\u00d7*336 size and we treat these patches as individual\ninputs for the ViT. In the following, we use \u2018HD-*H*\u2019 to represent our high-\nresolution setting with the constraint of*H*patches. For ex-\nample, the \u2019HD-9\u2019 allows up to 9 patches, including a range\nof resolutions such as 1008*\u00d7*1008, 672*\u00d7*1344, 336*\u00d7*3024,\n*etc*. **Global-Local Format. **For each input image, we present it\nto the model with two views. The first is the global view,\nwhere the image is resized to a fixed size (in our case, 336\n\u00d7 336). This provides a macro understanding of the image. Empirically, we have found this to be crucial for the LVLM\nto correctly understand the image. The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\nGiven that an",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\nGiven that an",
            {
                "chunk_size": "small"
            }
        ],
        [
            "here*pw*and*ph*represent the number of patches in each row\nand column, respectively. We then split the \u02c6*x*into*ph \u00d7*\n*pw*non-overlapped patches. Each patch is a small image\nwith 336*\u00d7*336 size and we treat these patches as individual\ninputs for the ViT. In the following, we use \u2018HD-*H*\u2019 to represent our high-\nresolution setting with the constraint of*H*patches. For ex-\nample, the \u2019HD-9\u2019 allows up to 9 patches, including a range\nof resolutions such as 1008*\u00d7*1008, 672*\u00d7*1344, 336*\u00d7*3024,\n*etc*. **Global-Local Format. **For each input image, we present it\nto the model with two views. The first is the global view,\nwhere the image is resized to a fixed size (in our case, 336\n\u00d7 336). This provides a macro understanding of the image. Empirically, we have found this to be crucial for the LVLM\nto correctly understand the image. The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\nGiven that an",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "der a similar model scale. As shown in Table 4, our\nmodel significantly outperforms existing open-source mod-\nels, achieving competitive results across all benchmarks. Notably, the InternLM-XComposer2 series is the only\nmethod that achieves a higher than 50% score on the chal-\nlenging MMStar benchmark. **High-resolution Understanding Evaluation. **\nThen we\ncompare IXC2-4KHD with models that are specifically de-\nsigned for high-resolution understanding tasks. We report\nthe results of 5 high-resolution benchmarks in Table 5, as a\ngeneral LVLM, IXC2-4KHD shows superb performance on\nthese tasks and outperforms competitors with a large mar-\ngin. For example, IXC2-4KHD gets 68*. *6% on Infograph-\nicVQA, surpassing recent DocOwl 1.5 with +17*.*9%. For\nthe OCRBench, IXC2-4KHD gets 67*. *5%, outperforms Co-\ngAgent with +8*.*5%.<br><br>**4.2. Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*\u2217*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting. However, when we",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*\u2217*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting. However, when we",
            {
                "chunk_size": "small"
            }
        ],
        [
            "der a similar model scale. As shown in Table 4, our\nmodel significantly outperforms existing open-source mod-\nels, achieving competitive results across all benchmarks. Notably, the InternLM-XComposer2 series is the only\nmethod that achieves a higher than 50% score on the chal-\nlenging MMStar benchmark. **High-resolution Understanding Evaluation. **\nThen we\ncompare IXC2-4KHD with models that are specifically de-\nsigned for high-resolution understanding tasks. We report\nthe results of 5 high-resolution benchmarks in Table 5, as a\ngeneral LVLM, IXC2-4KHD shows superb performance on\nthese tasks and outperforms competitors with a large mar-\ngin. For example, IXC2-4KHD gets 68*. *6% on Infograph-\nicVQA, surpassing recent DocOwl 1.5 with +17*.*9%. For\nthe OCRBench, IXC2-4KHD gets 67*. *5%, outperforms Co-\ngAgent with +8*.*5%.<br><br>**4.2. Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*\u2217*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting. However, when we",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "3\nFresh, frozen, canned and dried all count\n\u2018toward your daily servings, so you have plenty<br><br>of options. \u2018\n,\n9\n:\n2\nAlways reserve half of your plate for fruits\n\u2018and vegetables. 3\nChoose whole fruits and vegetables. 4 &<br><br>4\nTr tocata variety of vegetables, instead of the\n3\n\u2018same thing all the time. 2\n. 6\n4g\nLook for fruit packed in its own fruit juice and\ns\n100% fruit juice, with no added sugar. \u00a7\nLook for low/no-sodium options for canned\nvegetables, and 100% vegetable juice. &",
            {
                "chunk_size": "small"
            }
        ],
        [
            "3\nFresh, frozen, canned and dried all count\n\u2018toward your daily servings, so you have plenty<br><br>of options. \u2018\n,\n9\n:\n2\nAlways reserve half of your plate for fruits\n\u2018and vegetables. 3\nChoose whole fruits and vegetables. 4 &<br><br>4\nTr tocata variety of vegetables, instead of the\n3\n\u2018same thing all the time. 2\n. 6\n4g\nLook for fruit packed in its own fruit juice and\ns\n100% fruit juice, with no added sugar. \u00a7\nLook for low/no-sodium options for canned\nvegetables, and 100% vegetable juice. &",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[43] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are\nyou smarter than a sixth grader? textbook question answer-\ning for multimodal machine comprehension. In*Proceed-*\n*ings of the IEEE Conference on Computer Vision and Pat-*\n*tern recognition*, pages 4999\u20135007, 2017. 6<br><br>[44] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv \u0301e\nLe Borgne, Romaric Besanc \u0327on, Jos \u0301e G Moreno, and Jes \u0301us\nLov \u0301on Melgarejo. Viquae, a dataset for knowledge-based\nvisual question answering about named entities. In*Pro-*\n*ceedings of the 45th International ACM SIGIR Conference*\n*on Research and Development in Information Retrieval*,\npages 3108\u20133120, 2022. 6<br><br>[45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[43] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are\nyou smarter than a sixth grader? textbook question answer-\ning for multimodal machine comprehension. In*Proceed-*\n*ings of the IEEE Conference on Computer Vision and Pat-*\n*tern recognition*, pages 4999\u20135007, 2017. 6<br><br>[44] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv \u0301e\nLe Borgne, Romaric Besanc \u0327on, Jos \u0301e G Moreno, and Jes \u0301us\nLov \u0301on Melgarejo. Viquae, a dataset for knowledge-based\nvisual question answering about named entities. In*Pro-*\n*ceedings of the 45th International ACM SIGIR Conference*\n*on Research and Development in Information Retrieval*,\npages 3108\u20133120, 2022. 6<br><br>[45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension, 2023. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "| 86% Public Health England<br><br>ee\nae\nMS aay\n\u201cet? OWHAT IS\nn/a\n2\nwna? <abie\u201cy wy. contact tracing \u00ab\n|\n@\n=\n\u00a9<br><br>Contact tracing is a fundamental part of outbreak control\nthat\u2019s used by public health professionals around the world\nto prevent the spread of infections<br><br>FEN\nPerson<br><br>|\nfor coronavirus\n|\n(COVID-19)\nel",
            {
                "chunk_size": "small"
            }
        ],
        [
            "| 86% Public Health England<br><br>ee\nae\nMS aay\n\u201cet? OWHAT IS\nn/a\n2\nwna? <abie\u201cy wy. contact tracing \u00ab\n|\n@\n=\n\u00a9<br><br>Contact tracing is a fundamental part of outbreak control\nthat\u2019s used by public health professionals around the world\nto prevent the spread of infections<br><br>FEN\nPerson<br><br>|\nfor coronavirus\n|\n(COVID-19)\nel",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Task\nDataset<br><br>General Semantic Alignment\nShareGPT4V-PT [14], COCO [17], Nocaps [1], TextCaps [86], LAION400M [80], SBU [75], CC 3M [83]\nWorld Knowledge Alignment\nConcept Data [110]\nVision Capability Enhancement\nWanJuan [35], Flicker[103], MMC-Inst[54], RCTW-17[84], CTW[106], LSVT[88], ReCTs[111], ArT[22]<br><br>Table 1. **Datasets used for Pre-Training**. The data are collected from diverse sources for the three objectives. The newly added data is\nhighlighted with red.<br><br>image has a 2D structure and the image ratio is dynamic,\nthe number of tokens for each row can vary across dif-\nferent images. This variation can potentially confuse the\nLVLM, making it difficult to determine which tokens be-\nlong to the same row of the image and which ones belong\nto the next row. This confusion may hinder the LVLM\u2019s\nability to understand the 2D structure of the image, which\nis crucial for comprehending structural image content such\nas documents, charts, and tables. To address this issue, we\nintroduce a learnable newline (\u2018*\\*n\u2019) token at the end of each\nrow of the image features before the flattening. Finally, we\nconcatenate the global and local views, inserting a special\n\u2018separate\u2019 token between them to distinguish the two views.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Task\nDataset<br><br>General Semantic Alignment\nShareGPT4V-PT [14], COCO [17], Nocaps [1], TextCaps [86], LAION400M [80], SBU [75], CC 3M [83]\nWorld Knowledge Alignment\nConcept Data [110]\nVision Capability Enhancement\nWanJuan [35], Flicker[103], MMC-Inst[54], RCTW-17[84], CTW[106], LSVT[88], ReCTs[111], ArT[22]<br><br>Table 1. **Datasets used for Pre-Training**. The data are collected from diverse sources for the three objectives. The newly added data is\nhighlighted with red.<br><br>image has a 2D structure and the image ratio is dynamic,\nthe number of tokens for each row can vary across dif-\nferent images. This variation can potentially confuse the\nLVLM, making it difficult to determine which tokens be-\nlong to the same row of the image and which ones belong\nto the next row. This confusion may hinder the LVLM\u2019s\nability to understand the 2D structure of the image, which\nis crucial for comprehending structural image content such\nas documents, charts, and tables. To address this issue, we\nintroduce a learnable newline (\u2018*\\*n\u2019) token at the end of each\nrow of the image features before the flattening. Finally, we\nconcatenate the global and local views, inserting a special\n\u2018separate\u2019 token between them to distinguish the two views.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Xiaoyi Dong*\u2217*1*,*2, Pan Zhang*\u2217*1, Yuhang Zang*\u2217*1, Yuhang Cao1*,*2, Bin Wang1, Linke Ouyang1,\nSongyang Zhang1, Haodong Duan1, Wenwei Zhang1, Yining Li1, Hang Yan1, Yang Gao1, Zhe Chen1<br><br>Xinyue Zhang1, Wei Li1, Jingwen Li1, Wenhai Wang1*,*2, Kai Chen1, Conghui He3, Xingcheng Zhang3,\nJifeng Dai4*,*1, Yu Qiao1, Dahua Lin1*,*2, Jiaqi Wang1*,*\ufffd<br><br>arXiv:2404.06512v1  [cs.CV]  9 Apr 2024<br><br>1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n3SenseTime Group, 4Tsinghua University<br><br>internlm@pjlab.org.cn<br><br>Figure 1. Overview of InternLM-XComposer2-4KHD perfor-\nmance on benchmarks with different resolutions. Our model based\non InternLM2-7B [91]**matches or even surpasses GPT-4V [74]**\n**and Gemini Pro [90] in 10 of the 16 benchmarks**.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Xiaoyi Dong*\u2217*1*,*2, Pan Zhang*\u2217*1, Yuhang Zang*\u2217*1, Yuhang Cao1*,*2, Bin Wang1, Linke Ouyang1,\nSongyang Zhang1, Haodong Duan1, Wenwei Zhang1, Yining Li1, Hang Yan1, Yang Gao1, Zhe Chen1<br><br>Xinyue Zhang1, Wei Li1, Jingwen Li1, Wenhai Wang1*,*2, Kai Chen1, Conghui He3, Xingcheng Zhang3,\nJifeng Dai4*,*1, Yu Qiao1, Dahua Lin1*,*2, Jiaqi Wang1*,*\ufffd<br><br>arXiv:2404.06512v1  [cs.CV]  9 Apr 2024<br><br>1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n3SenseTime Group, 4Tsinghua University<br><br>internlm@pjlab.org.cn<br><br>Figure 1. Overview of InternLM-XComposer2-4KHD perfor-\nmance on benchmarks with different resolutions. Our model based\non InternLM2-7B [91]**matches or even surpasses GPT-4V [74]**\n**and Gemini Pro [90] in 10 of the 16 benchmarks**.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**4. Experiments**<br><br>In this section, we validate the benchmark performance\nof our InternLM-XComposer2-4KHD (IXC2-4KHD in the\nfollowing for simplicity) after supervised fine-tuning.<br><br>**4.1. LVLM Benchmark results. **<br><br>In Table 3 and Table 4,\nwe compare our IXC2-\n4KHD\non\na\nlist\nof\nbenchmarks\nwith\nboth\nSOTA",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**4. Experiments**<br><br>In this section, we validate the benchmark performance\nof our InternLM-XComposer2-4KHD (IXC2-4KHD in the\nfollowing for simplicity) after supervised fine-tuning.<br><br>**4.1. LVLM Benchmark results. **<br><br>In Table 3 and Table 4,\nwe compare our IXC2-\n4KHD\non\na\nlist\nof\nbenchmarks\nwith\nboth\nSOTA",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "is assessed to see how close\nthey were to aconfirmed case,\n2\noH)\nwhich could include someone\nliving with the case, or\nspeak to the patient to identify\n_someone who has been in\n_\nanyone who has had close contact\n(Aaee ee\nwith them during the time they are infectious<br><br>A contact<br><br>Following this assessment, we can categorise\nthem into low or high risk and contact them to\nprovide advice on what they should do<br><br>ge\nchante\nIf we believe a contact is at\ni\nlow risk\n14\nhigher risk of infection\nG contact\ndays\nthey may be asked to\n\u201can doesn\u2019t require\nGW,\nself-isolate, remaining in\nHy]\nself-isolation\ntheir home for 14 days and\n4\nstaying away from work,\nschool or public places<br><br>a\nWe contact them daily\npassive follow up\nLa)\nprovide them with advice\nwhich means person being\noF\non what to do if they\nmonitored but we don\u2019t necessarily\nx4]\nbecome unwell until they\ncontact them every day\ncan be given the all-clear<br><br>contact on",
            {
                "chunk_size": "small"
            }
        ],
        [
            "is assessed to see how close\nthey were to aconfirmed case,\n2\noH)\nwhich could include someone\nliving with the case, or\nspeak to the patient to identify\n_someone who has been in\n_\nanyone who has had close contact\n(Aaee ee\nwith them during the time they are infectious<br><br>A contact<br><br>Following this assessment, we can categorise\nthem into low or high risk and contact them to\nprovide advice on what they should do<br><br>ge\nchante\nIf we believe a contact is at\ni\nlow risk\n14\nhigher risk of infection\nG contact\ndays\nthey may be asked to\n\u201can doesn\u2019t require\nGW,\nself-isolate, remaining in\nHy]\nself-isolation\ntheir home for 14 days and\n4\nstaying away from work,\nschool or public places<br><br>a\nWe contact them daily\npassive follow up\nLa)\nprovide them with advice\nwhich means person being\noF\non what to do if they\nmonitored but we don\u2019t necessarily\nx4]\nbecome unwell until they\ncontact them every day\ncan be given the all-clear<br><br>contact on",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[86] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In*Computer Vision\u2013*\n*ECCV 2020: 16th European Conference, Glasgow, UK, Au-*\n*gust 23\u201328, 2020, Proceedings, Part II 16*, pages 742\u2013758. Springer, 2020. 6<br><br>[87] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In*Proceed-*\n*ings of the IEEE/CVF conference on computer vision and*\n*pattern recognition*, pages 8317\u20138326, 2019. 1, 2, 6, 7<br><br>[88] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\nCanjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo\nLiu, Dimosthenis Karatzas, et al. Icdar 2019 competition on\nlarge-scale street view text with partial labeling-rrc-lsvt. In\n*2019 International Conference on Document Analysis and*\n*Recognition (ICDAR)*, pages 1557\u20131562. IEEE, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "IEEE, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[86] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In*Computer Vision\u2013*\n*ECCV 2020: 16th European Conference, Glasgow, UK, Au-*\n*gust 23\u201328, 2020, Proceedings, Part II 16*, pages 742\u2013758. Springer, 2020. 6<br><br>[87] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In*Proceed-*\n*ings of the IEEE/CVF conference on computer vision and*\n*pattern recognition*, pages 8317\u20138326, 2019. 1, 2, 6, 7<br><br>[88] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\nCanjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo\nLiu, Dimosthenis Karatzas, et al. Icdar 2019 competition on\nlarge-scale street view text with partial labeling-rrc-lsvt. In\n*2019 International Conference on Document Analysis and*\n*Recognition (ICDAR)*, pages 1557\u20131562. IEEE, 2019. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[102] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. *arXiv.org*, 2023. 2<br><br>[103] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. *Transactions of the Association for Computa-*\n*tional Linguistics*, 2:67\u201378, 2014. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[102] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. *arXiv.org*, 2023. 2<br><br>[103] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. *Transactions of the Association for Computa-*\n*tional Linguistics*, 2:67\u201378, 2014. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[24] OpenCompass Contributors. Opencompass:\nA univer-\nsal evaluation platform for foundation models. https:\n//github.com/open- compass/opencompass,\n2023. 7<br><br>[25] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 2<br><br>[26] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos \u0301e M.F. Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In*Proceedings of the IEEE Confer-*\n*ence on Computer Vision and Pattern Recognition (CVPR)*,\n2017. 6<br><br>[27] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\nBin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,\nHaodong Duan, Maosong Cao, Wenwei Zhang, Yining\nLi, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-\nwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu\nQiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024. 2, 5",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024. 2, 5",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[24] OpenCompass Contributors. Opencompass:\nA univer-\nsal evaluation platform for foundation models. https:\n//github.com/open- compass/opencompass,\n2023. 7<br><br>[25] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 2<br><br>[26] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos \u0301e M.F. Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In*Proceedings of the IEEE Confer-*\n*ence on Computer Vision and Pattern Recognition (CVPR)*,\n2017. 6<br><br>[27] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\nBin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,\nHaodong Duan, Maosong Cao, Wenwei Zhang, Yining\nLi, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-\nwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu\nQiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024. 2, 5",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Model\n\u2018*\\*n\u2019\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>HD9\n*\u00d7*\n79.5\n50.3\n74.0\n78.2\n79.1\n2206\n75.9\nHD9\n\u2713\n79.4\n50.5\n73.8\n78.2\n79.5\n2201\n**76.6**<br><br>4KHD\n*\u00d7*\n88.1\n67.4\n75.9\n80.4\n79.9\n**2232**\n76.4\n4KHD\n\u2713\n**89.0**\n**69.3**\n**77.2**\n**81.0**\n**80.2**\n2205\n76.2<br><br>Table 8. **Influence of Indicator \u2018***\\***n\u2019 in the Image Features. **\u2018*\\*n\u2019\nhelps LVLM understand structural images when the input resolu-\ntion is dynamic and large.<br><br>Strategy\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>Re-Sampler\n86.2\n67.1\n75.3\n78.8\n79.6\n2124\n74.2\nC-Abstractor\n88.6\n69.5\n77.1\n80.6\n80.4\n2236\n76.7\nConcat\n89.0\n69.3\n77.2\n81.0\n80.2\n2205\n76.2<br><br>Table 9. **Ablation on Token Merging Strategy. **Both the simple\nconcatenation operation and the C-Abstractor works well.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Model\n\u2018*\\*n\u2019\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>HD9\n*\u00d7*\n79.5\n50.3\n74.0\n78.2\n79.1\n2206\n75.9\nHD9\n\u2713\n79.4\n50.5\n73.8\n78.2\n79.5\n2201\n**76.6**<br><br>4KHD\n*\u00d7*\n88.1\n67.4\n75.9\n80.4\n79.9\n**2232**\n76.4\n4KHD\n\u2713\n**89.0**\n**69.3**\n**77.2**\n**81.0**\n**80.2**\n2205\n76.2<br><br>Table 8. **Influence of Indicator \u2018***\\***n\u2019 in the Image Features. **\u2018*\\*n\u2019\nhelps LVLM understand structural images when the input resolu-\ntion is dynamic and large.<br><br>Strategy\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>Re-Sampler\n86.2\n67.1\n75.3\n78.8\n79.6\n2124\n74.2\nC-Abstractor\n88.6\n69.5\n77.1\n80.6\n80.4\n2236\n76.7\nConcat\n89.0\n69.3\n77.2\n81.0\n80.2\n2205\n76.2<br><br>Table 9. **Ablation on Token Merging Strategy. **Both the simple\nconcatenation operation and the C-Abstractor works well.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Task\nDataset<br><br>Caption\nShareGPT4V [14], COCO [17],Nocaps [1]<br><br>General QA\nVQAv2 [3], GQA [38], OK-VQA [67]\nVD [26], RD[13], VSR[53],<br><br>Science QA\nAI2D [42], SQA [63], TQA[43], IconQA[65]<br><br>Chart QA\nDVQA [40], ChartQA, ChartQA-AUG [68]<br><br>Math QA\nMathQA [104], Geometry3K[62], TabMWP[64],\nCLEVR-MATH[52]/Super[49]<br><br>World Knowledge QA A-OKVQA [81],KVQA [82], ViQuAE[44]<br><br>OCR QA\nTextVQA[87], OCR-VQA[72], ST-VQA[8]<br><br>HD-OCR QA\nInfoVQA[69], DocVQA[70]<br><br>Conversation\nLLaVA-150k [56], LVIS-Instruct4V [94]\nShareGPT-en&zh [21], InternLM-Chat[91]<br><br>Table 2. **Datasets used for Supervised Fine-Tuning**. We collect\ndata from diverse sources to empower the model with different\ncapabilities. The newly added data is highlighted with red.<br><br>**3.3. Pre-Training**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Task\nDataset<br><br>Caption\nShareGPT4V [14], COCO [17],Nocaps [1]<br><br>General QA\nVQAv2 [3], GQA [38], OK-VQA [67]\nVD [26], RD[13], VSR[53],<br><br>Science QA\nAI2D [42], SQA [63], TQA[43], IconQA[65]<br><br>Chart QA\nDVQA [40], ChartQA, ChartQA-AUG [68]<br><br>Math QA\nMathQA [104], Geometry3K[62], TabMWP[64],\nCLEVR-MATH[52]/Super[49]<br><br>World Knowledge QA A-OKVQA [81],KVQA [82], ViQuAE[44]<br><br>OCR QA\nTextVQA[87], OCR-VQA[72], ST-VQA[8]<br><br>HD-OCR QA\nInfoVQA[69], DocVQA[70]<br><br>Conversation\nLLaVA-150k [56], LVIS-Instruct4V [94]\nShareGPT-en&zh [21], InternLM-Chat[91]<br><br>Table 2. **Datasets used for Supervised Fine-Tuning**. We collect\ndata from diverse sources to empower the model with different\ncapabilities. The newly added data is highlighted with red.<br><br>**3.3. Pre-Training**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[83] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In*Pro-*\n*ceedings of the 56th Annual Meeting of the Association for*\n*Computational Linguistics (Volume 1: Long Papers)*, pages\n2556\u20132565, 2018. 6<br><br>[84] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\nPei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\nBai. Icdar2017 competition on reading chinese text in the\nwild (rctw-17). In*2017 14th iapr international conference*\n*on document analysis and recognition (ICDAR)*, volume 1,\npages 1429\u20131434. IEEE, 2017. 6<br><br>[85] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu,\nand Diyi Yang. Design2code: How far are we from au-\ntomating front-end engineering?, 2024. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[83] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In*Pro-*\n*ceedings of the 56th Annual Meeting of the Association for*\n*Computational Linguistics (Volume 1: Long Papers)*, pages\n2556\u20132565, 2018. 6<br><br>[84] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\nPei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\nBai. Icdar2017 competition on reading chinese text in the\nwild (rctw-17). In*2017 14th iapr international conference*\n*on document analysis and recognition (ICDAR)*, volume 1,\npages 1429\u20131434. IEEE, 2017. 6<br><br>[85] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu,\nand Diyi Yang. Design2code: How far are we from au-\ntomating front-end engineering?, 2024. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "a>\nAnalyze this figure in detail<br><br>7)<br><br>The image is a scientific paper titled \"ShareGPT4V: Improving \nLarge Multi-Modal Models with Better Captions\" from the \nUniversity of Science and Technology of China and Shanghai \nAI Laboratory. The paper discusses the comparison between the \ncaptions in their proposed ShareGPT4V dataset and those \nutilized by recent large multi-modal models (LMMs). The \ncaptions are compared in terms of their quality and \nperformance. The paper also highlights the remarkable \nperformance of the proposed LMMs based on a simple \narchitecture that has remarkable performance across a majority \nof the multi-modal benchmarks.<br><br>0.6<br><br>64,\n68.8\n6.7,Lm\nMMBench\n623<br><br>MMBench-CN<br><br>@ ShareGPT4V-7B<br><br>a>\nWhich model has the best performance on the \nMMBench and what is its exact number? Qwen-VL-Chat-7B<br><br>7)\nAccording to the paper, ShareGPT4V-7B has the best \nperformance on the MMBench with a score of 68.8.<br><br>a>\nFigure 1. (a) We showease a comparison between the caption in our proposed ShareGPT4V dataset and those utilized by recent large\nmulti-modal models (LMMs). Unlike COCO-Caption [7] involves brief human-made captions on the main subject. LLaVA-Instruct\n(31) combines human-made captions, bounding boxes, and GPT4 [39] to \u2018imagine\u2019 the image details, which leads to inevitable er-\nror/hallucination description (marked in red), Our approach involves feeding carefully designed prompts along with images directly into\nthe advanced GPT4-Vision [40] and the descriptions are more detailed and accurate (marked in blue). (b) We highlight the remarkable\nperformance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset.<br><br>Where can I get this project?<br><br>7) You can get this project from the following GitHub link: \nhttps://ShareGPT4V.github.io\nThe HD image (2550 x 3300)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nAnalyze this figure in detail<br><br>7)<br><br>The image is a scientific paper titled \"ShareGPT4V: Improving \nLarge Multi-Modal Models with Better Captions\" from the \nUniversity of Science and Technology of China and Shanghai \nAI Laboratory. The paper discusses the comparison between the \ncaptions in their proposed ShareGPT4V dataset and those \nutilized by recent large multi-modal models (LMMs). The \ncaptions are compared in terms of their quality and \nperformance. The paper also highlights the remarkable \nperformance of the proposed LMMs based on a simple \narchitecture that has remarkable performance across a majority \nof the multi-modal benchmarks.<br><br>0.6<br><br>64,\n68.8\n6.7,Lm\nMMBench\n623<br><br>MMBench-CN<br><br>@ ShareGPT4V-7B<br><br>a>\nWhich model has the best performance on the \nMMBench and what is its exact number? Qwen-VL-Chat-7B<br><br>7)\nAccording to the paper, ShareGPT4V-7B has the best \nperformance on the MMBench with a score of 68.8.<br><br>a>\nFigure 1. (a) We showease a comparison between the caption in our proposed ShareGPT4V dataset and those utilized by recent large\nmulti-modal models (LMMs). Unlike COCO-Caption [7] involves brief human-made captions on the main subject. LLaVA-Instruct\n(31) combines human-made captions, bounding boxes, and GPT4 [39] to \u2018imagine\u2019 the image details, which leads to inevitable er-\nror/hallucination description (marked in red), Our approach involves feeding carefully designed prompts along with images directly into\nthe advanced GPT4-Vision [40] and the descriptions are more detailed and accurate (marked in blue). (b) We highlight the remarkable\nperformance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset.<br><br>Where can I get this project?<br><br>7) You can get this project from the following GitHub link: \nhttps://ShareGPT4V.github.io\nThe HD image (2550 x 3300)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**5. Conclusion**<br><br>In this paper, we propose the InternLM-Xcomposer2-\n4KHD that exceeds the performance of previous open-\nsource models on OCR-related tasks and also achieves\ncompetitive results on general-purpose LVLM benchmarks. Thanks to our dynamic resolution and automatic patch con-\nfiguration, our model supports a maximum training resolu-\ntion of up to 4K HD. We also integrate a global view patch\nto support the macro understanding and a learnable newline\ntoken to handle the various input image resolutions. Our\nmodel\u2019s performance continues to improve as the training\nresolution increases for HD-OCR tasks. Notably, we do\nnot observe any performance saturation even for the 4KHD\nsetting, and we have not explored the upper bound due to\nthe computational burden increasing with higher-resolution\ninputs. In future work, we plan to explore efficient solu-\ntions for accurate LVLM training and inference, enabling\nour model to handle even higher resolutions while main-\ntaining computational efficiency.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**5. Conclusion**<br><br>In this paper, we propose the InternLM-Xcomposer2-\n4KHD that exceeds the performance of previous open-\nsource models on OCR-related tasks and also achieves\ncompetitive results on general-purpose LVLM benchmarks. Thanks to our dynamic resolution and automatic patch con-\nfiguration, our model supports a maximum training resolu-\ntion of up to 4K HD. We also integrate a global view patch\nto support the macro understanding and a learnable newline\ntoken to handle the various input image resolutions. Our\nmodel\u2019s performance continues to improve as the training\nresolution increases for HD-OCR tasks. Notably, we do\nnot observe any performance saturation even for the 4KHD\nsetting, and we have not explored the upper bound due to\nthe computational burden increasing with higher-resolution\ninputs. In future work, we plan to explore efficient solu-\ntions for accurate LVLM training and inference, enabling\nour model to handle even higher resolutions while main-\ntaining computational efficiency.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[32] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. *arXiv preprint*\n*arXiv:2312.12436*, 2023. 2<br><br>[33] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a seed of vision in large language model. *arXiv preprint arXiv:2307.08041*, 2023. 1, 5<br><br>[34] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou. Hallusionbench: An advanced diagnostic suite for\nentangled language hallucination & visual illusion in large\nvision-language models, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[32] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. *arXiv preprint*\n*arXiv:2312.12436*, 2023. 2<br><br>[33] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a seed of vision in large language model. *arXiv preprint arXiv:2307.08041*, 2023. 1, 5<br><br>[34] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou. Hallusionbench: An advanced diagnostic suite for\nentangled language hallucination & visual illusion in large\nvision-language models, 2023. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "During the pre-training phase, the LLM is frozen while\nboth the vision encoder and Partial LoRA are fine-tuned to\nalign the visual tokens with the LLM. The pre-training data\nmainly follow the design in XComposer2 which is curated\nwith**three objectives**in mind: 1) general semantic align-\nment, 2) world knowledge alignment, 3) vision capability\nenhancement. In this paper, we focus on high-resolution\nand structural image understanding. Therefore, we have\ncollected more related data to enhance this specific capa-\nbility. As shown in Table.1, we have utilized a diverse OCR\ndataset for this purpose.<br><br>**3.4. 4KHD Supervised Fine-tuning**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "During the pre-training phase, the LLM is frozen while\nboth the vision encoder and Partial LoRA are fine-tuned to\nalign the visual tokens with the LLM. The pre-training data\nmainly follow the design in XComposer2 which is curated\nwith**three objectives**in mind: 1) general semantic align-\nment, 2) world knowledge alignment, 3) vision capability\nenhancement. In this paper, we focus on high-resolution\nand structural image understanding. Therefore, we have\ncollected more related data to enhance this specific capa-\nbility. As shown in Table.1, we have utilized a diverse OCR\ndataset for this purpose.<br><br>**3.4. 4KHD Supervised Fine-tuning**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "and Pete Florence. Palm-e: An embodied multimodal lan-\nguage model. In*arXiv preprint arXiv:2303.03378*, 2023. 2<br><br>[29] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. In\n*Proceedings of the 60th Annual Meeting of the Association*\n*for Computational Linguistics (Volume 1: Long Papers)*,\npages 320\u2013335, 2022. 1<br><br>[30] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang\nLi, and Can Huang. DocPedia: Unleashing the power\nof large multimodal model in the frequency domain\nfor versatile document understanding. *arXiv preprint*\n*arXiv:2311.11810*, 2023. 5<br><br>[31] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "and Pete Florence. Palm-e: An embodied multimodal lan-\nguage model. In*arXiv preprint arXiv:2303.03378*, 2023. 2<br><br>[29] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. In\n*Proceedings of the 60th Annual Meeting of the Association*\n*for Computational Linguistics (Volume 1: Long Papers)*,\npages 320\u2013335, 2022. 1<br><br>[30] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang\nLi, and Can Huang. DocPedia: Unleashing the power\nof large multimodal model in the frequency domain\nfor versatile document understanding. *arXiv preprint*\n*arXiv:2311.11810*, 2023. 5<br><br>[31] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023. 1, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "In many communities, fruits and vegetables are limited/expensive and only\navailable in corner stores, convenience stores, bodegas and gas stations.<br><br>Lack of Access Can Profoundly Impact Diet\nThere are ways that you can actively support improved access to\nhigher quality foods for neighbourhoods. Here's how:<br><br>Contact your city and state\nFind ways to spread the word\nCreate a petition for more\nleaders to let them know\nabout nutrition assistance\nvariety, improve affordability\nwhat food access is like in\nprograms, such as SNAP, WIC and\nor advocate for better\nyour community\nschool meals. signage/placement.<br><br>\u00a2\nS<br><br>Meet with an after-school or\nOrganize a letter-writing\n\u2018Sign up for \u201cYou're the Cure\u201d\ndaycare program representative\n\u2018campaign and set up a meeting\nand then send a note to your\nto discuss serving more fruits and\n_\u2014_with state leaders. Congressperson advocating for\nveggies for snacks. For example, ask for funding to\nhealthier meals at school\nhost a farmers\u2019 market in an\nunderserved community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In many communities, fruits and vegetables are limited/expensive and only\navailable in corner stores, convenience stores, bodegas and gas stations.<br><br>Lack of Access Can Profoundly Impact Diet\nThere are ways that you can actively support improved access to\nhigher quality foods for neighbourhoods. Here's how:<br><br>Contact your city and state\nFind ways to spread the word\nCreate a petition for more\nleaders to let them know\nabout nutrition assistance\nvariety, improve affordability\nwhat food access is like in\nprograms, such as SNAP, WIC and\nor advocate for better\nyour community\nschool meals. signage/placement.<br><br>\u00a2\nS<br><br>Meet with an after-school or\nOrganize a letter-writing\n\u2018Sign up for \u201cYou're the Cure\u201d\ndaycare program representative\n\u2018campaign and set up a meeting\nand then send a note to your\nto discuss serving more fruits and\n_\u2014_with state leaders. Congressperson advocating for\nveggies for snacks. For example, ask for funding to\nhealthier meals at school\nhost a farmers\u2019 market in an\nunderserved community.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "After the pre-training, we empower the model to understand\nhigh-resolution images and solve diverse challenges. Dif-\nferent from previous perception tasks (*e.g*., VQAv2, GQA)\nwhich typically answer questions based on the noticeable\nobject in the image. OCR-related tasks depend on a de-\ntailed understanding of text within a high-resolution image. For instance, in InfoVQA, the length of the longer side of\n50% of the images exceeds 2000 pixels. Low-resolution\ninputs can distort the dense text information, causing the\nmodel to fail in its understanding. However, we have ob-\nserved a resolution saturation problem with the aforemen-\ntioned perception tasks, where the influence of resolution\nbecomes negligible. To address this, we introduce a mixed-resolution train-\ning strategy for more efficient training. For tasks requir-\ning high resolution, we employ the \u2018HD-55\u2019 setting during\ntraining. This allows for the input of 4K (3840*\u00d7*1600) im-\nages without necessitating additional image compression. These tasks are referred to as the HD-OCR QA tasks in Ta-\nble 2. For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the \u2018HD25\u2019 set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence. For instance, we have observed that using the \u2018HD30\u2019",
            {
                "chunk_size": "small"
            }
        ],
        [
            "For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the \u2018HD25\u2019 set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence. For instance, we have observed that using the \u2018HD30\u2019",
            {
                "chunk_size": "small"
            }
        ],
        [
            "After the pre-training, we empower the model to understand\nhigh-resolution images and solve diverse challenges. Dif-\nferent from previous perception tasks (*e.g*., VQAv2, GQA)\nwhich typically answer questions based on the noticeable\nobject in the image. OCR-related tasks depend on a de-\ntailed understanding of text within a high-resolution image. For instance, in InfoVQA, the length of the longer side of\n50% of the images exceeds 2000 pixels. Low-resolution\ninputs can distort the dense text information, causing the\nmodel to fail in its understanding. However, we have ob-\nserved a resolution saturation problem with the aforemen-\ntioned perception tasks, where the influence of resolution\nbecomes negligible. To address this, we introduce a mixed-resolution train-\ning strategy for more efficient training. For tasks requir-\ning high resolution, we employ the \u2018HD-55\u2019 setting during\ntraining. This allows for the input of 4K (3840*\u00d7*1600) im-\nages without necessitating additional image compression. These tasks are referred to as the HD-OCR QA tasks in Ta-\nble 2. For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the \u2018HD25\u2019 set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence. For instance, we have observed that using the \u2018HD30\u2019",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "*arXiv:2209.14610*, 2022. 6<br><br>[65] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram under-\nstanding and visual language reasoning. *arXiv preprint*\n*arXiv:2110.13214*, 2021. 6<br><br>[66] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\ning Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,\nLi Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha\nZhang, and Furu Wei. Kosmos-2.5: A multimodal literate\nmodel, 2023. 2, 5<br><br>[67] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In*Proceedings*\n*of the IEEE/cvf conference on computer vision and pattern*\n*recognition*, pages 3195\u20133204, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*arXiv:2209.14610*, 2022. 6<br><br>[65] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram under-\nstanding and visual language reasoning. *arXiv preprint*\n*arXiv:2110.13214*, 2021. 6<br><br>[66] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\ning Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,\nLi Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha\nZhang, and Furu Wei. Kosmos-2.5: A multimodal literate\nmodel, 2023. 2, 5<br><br>[67] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In*Proceedings*\n*of the IEEE/cvf conference on computer vision and pattern*\n*recognition*, pages 3195\u20133204, 2019. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**LVLMs for Document Understanding. **\nDocument un-\nderstanding involves analyzing and comprehending various\ndigital documents, such as figures, tables, and academic pa-\npers. Many document understanding tasks require models\nto handle high-resolution inputs, complex layouts, various\naspect ratios, and diverse document formats. To enhance the\ncapabilities of LVLMs for document understanding, sev-\neral works have collected and constructed high-quality doc-\nument instruction tuning data, including LLaVAR [112],\nmPLUG-DocOwl [100] and TGDoc [96]. DocPediaDoc-\nPedia [30] processes document inputs in the frequency do-\nmain. Some previous works have improved document un-\nderstanding ability by designing special modules for high-\nresolution inputs, such as HR and LR encoders [36, 97]\nor cropped image patches [59, 99, 101]. Our InternLM-\nXComposer2-4KHD first scales to 4K resolution inputs\nand demonstrates strong document understanding ability on\nOCR-related benchmarks. Also, our approach also achieves\ncomparable results on other general LVLM benchmarks like\nperception and reasoning [15, 33, 57, 61].",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**LVLMs for Document Understanding. **\nDocument un-\nderstanding involves analyzing and comprehending various\ndigital documents, such as figures, tables, and academic pa-\npers. Many document understanding tasks require models\nto handle high-resolution inputs, complex layouts, various\naspect ratios, and diverse document formats. To enhance the\ncapabilities of LVLMs for document understanding, sev-\neral works have collected and constructed high-quality doc-\nument instruction tuning data, including LLaVAR [112],\nmPLUG-DocOwl [100] and TGDoc [96]. DocPediaDoc-\nPedia [30] processes document inputs in the frequency do-\nmain. Some previous works have improved document un-\nderstanding ability by designing special modules for high-\nresolution inputs, such as HR and LR encoders [36, 97]\nor cropped image patches [59, 99, 101]. Our InternLM-\nXComposer2-4KHD first scales to 4K resolution inputs\nand demonstrates strong document understanding ability on\nOCR-related benchmarks. Also, our approach also achieves\ncomparable results on other general LVLM benchmarks like\nperception and reasoning [15, 33, 57, 61].",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Which department has the highest cost in most cases?",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Which department has the highest cost in most cases?",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[60] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-\naoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. RAR: Retrieving and ranking augmented mllms for visual\nrecognition. *arXiv preprint arXiv:2403.13805*, 2024. 2<br><br>[61] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. In*International Conference on Learning Represen-*\n*tations (ICLR)*, 2024. 5, 7<br><br>[62] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\nterpretable geometry problem solving with formal language\nand symbolic reasoning. In*The 59th Annual Meeting of the*\n*Association for Computational Linguistics (ACL)*, 2021. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[60] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-\naoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. RAR: Retrieving and ranking augmented mllms for visual\nrecognition. *arXiv preprint arXiv:2403.13805*, 2024. 2<br><br>[61] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. In*International Conference on Learning Represen-*\n*tations (ICLR)*, 2024. 5, 7<br><br>[62] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\nterpretable geometry problem solving with formal language\nand symbolic reasoning. In*The 59th Annual Meeting of the*\n*Association for Computational Linguistics (ACL)*, 2021. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**The Role of Global-View. **We first examine the impact\nof the global view in our Global-Local Format. As indi-\ncated in Table 7, we find that the global view is essential\nfor the LVLM to accurately comprehend the input image. When it is removed, the model performs worse across all\nbenchmarks. For instance, the model experiences a*\u2212*4*. *4%\ndrop in performance on the MMBench EN-Test without the\nglobal view. We contend that the global view offers a gen-\neral macro understanding of the image, which the model\nstruggled to derive from the large number of tokens in the\nlocal view. **The Role of the Newline Token. **We incorporate a special\nnewline token at the end of each row of the image features\nbefore the flattening operation. This token serves as an in-\ndicator of the image\u2019s 2D structure. We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM\u2019s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension. We have",
            {
                "chunk_size": "small"
            }
        ],
        [
            "We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM\u2019s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension. We have",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**The Role of Global-View. **We first examine the impact\nof the global view in our Global-Local Format. As indi-\ncated in Table 7, we find that the global view is essential\nfor the LVLM to accurately comprehend the input image. When it is removed, the model performs worse across all\nbenchmarks. For instance, the model experiences a*\u2212*4*. *4%\ndrop in performance on the MMBench EN-Test without the\nglobal view. We contend that the global view offers a gen-\neral macro understanding of the image, which the model\nstruggled to derive from the large number of tokens in the\nlocal view. **The Role of the Newline Token. **We incorporate a special\nnewline token at the end of each row of the image features\nbefore the flattening operation. This token serves as an in-\ndicator of the image\u2019s 2D structure. We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM\u2019s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension. We have",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "tical applicability in real-world scenarios. Recent advancements have aimed at enhancing the\nresolution of Large Vision-Language Models (LVLMs). Some approaches [36, 48, 66, 97] involve adapting high-\nresolution vision encoders directly. However, the Vi-\nsion Transformer (ViT) architecture falls short when deal-\ning with images of varying resolutions and aspect ratios,\nthereby restricting its ability to handle diverse inputs ef-\nfectively. Alternatively, some methods [37, 46, 50, 51, 55,\n59, 99] maintain the vision encoder\u2019s resolution, segment-\ning high-resolution images into multiple low-resolution\npatches. Yet, these methods are constrained by an inad-\nequate resolution, typically around 1500*\u00d7*1500, which\ndoes not satisfy the demands of daily content,*e.g*., website\nscreenshots [85], document pages [70], and blueprints [69]. Furthermore, they are confined to either a few predefined\nhigh-resolution settings [36, 46, 48, 50, 51, 55, 59, 66, 97]\nor a limited range of resolutions [37, 99], thereby restricting\ntheir utility across a variety of applications. In this work, we introduce InternLM-XComposer2-\n4KHD, a pioneering model that for the first time expands\nthe resolution capabilities of Large Vision-Language Mod-\nels (LVLMs) to 4K HD and even higher, thereby setting\na new standard in high-resolution vision-language under-\nstanding. Designed to handle a broad range of resolutions,\nInternLM-XComposer2-4KHD supports images with any\naspect ratio from 336 pixels up to 4K HD, facilitating its\ndeployment in real-world contexts. InternLM-XComposer2-4KHD\nfollows\npatch\ndivi-\nsion [46, 50] paradigm and enhances it by incorporating an\ninnovative extension: dynamic resolution with automatic\npatch configuration. To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*\u00d7*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2. Related Works**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*\u00d7*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2. Related Works**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "tical applicability in real-world scenarios. Recent advancements have aimed at enhancing the\nresolution of Large Vision-Language Models (LVLMs). Some approaches [36, 48, 66, 97] involve adapting high-\nresolution vision encoders directly. However, the Vi-\nsion Transformer (ViT) architecture falls short when deal-\ning with images of varying resolutions and aspect ratios,\nthereby restricting its ability to handle diverse inputs ef-\nfectively. Alternatively, some methods [37, 46, 50, 51, 55,\n59, 99] maintain the vision encoder\u2019s resolution, segment-\ning high-resolution images into multiple low-resolution\npatches. Yet, these methods are constrained by an inad-\nequate resolution, typically around 1500*\u00d7*1500, which\ndoes not satisfy the demands of daily content,*e.g*., website\nscreenshots [85], document pages [70], and blueprints [69]. Furthermore, they are confined to either a few predefined\nhigh-resolution settings [36, 46, 48, 50, 51, 55, 59, 66, 97]\nor a limited range of resolutions [37, 99], thereby restricting\ntheir utility across a variety of applications. In this work, we introduce InternLM-XComposer2-\n4KHD, a pioneering model that for the first time expands\nthe resolution capabilities of Large Vision-Language Mod-\nels (LVLMs) to 4K HD and even higher, thereby setting\na new standard in high-resolution vision-language under-\nstanding. Designed to handle a broad range of resolutions,\nInternLM-XComposer2-4KHD supports images with any\naspect ratio from 336 pixels up to 4K HD, facilitating its\ndeployment in real-world contexts. InternLM-XComposer2-4KHD\nfollows\npatch\ndivi-\nsion [46, 50] paradigm and enhances it by incorporating an\ninnovative extension: dynamic resolution with automatic\npatch configuration. To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*\u00d7*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2. Related Works**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[89] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\nShu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. *arXiv preprint arXiv:2312.03818*, 2023. 2<br><br>[90] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2<br><br>[91] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities. https://",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[89] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\nShu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. *arXiv preprint arXiv:2312.03818*, 2023. 2<br><br>[90] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2<br><br>[91] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities. https://",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Muted Color Palettes\nMuted colors are colors that have alow\nsaturation (as opposed to vivid colors). Muted colors feel safe and secure, even\nnostalgic. They can also feel natural and\norganic. That\u2019s why many health and\nwellness brands have been using muted\ncolor palettes this year.<br><br>Simple Data\n|\nVisualizations\nThe goal of any data visualization should<br><br>be to make the complex\ndata easy to\nunderstand. We are living in a time where<br><br>a lot of data is constantly being circulated. Simple data visualizations\ncanmake\ncommunication more effective.<br><br>Geometric Shapes\nEverywhere<br><br>Last year, we saw designers usinga lot\nofflowing and abstract shapes in their\ndesigns. This year, they have been\nreplaced with rigid, hard-edged geo-\nmetric shapes and patterns. The hard\nedges of a geometric shape create a\ngreat contrast against muted colors.<br><br>Flat Icons and\nIllustrations<br><br>Many brands are using flat icons and\nillustrationsin their social media graphics,\nwebsite design, and more. Icons can be a<br><br>powerful tool for visual communication. With\na simple icon, you can communicate meaning\nin less space than words. Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Muted Color Palettes\nMuted colors are colors that have alow\nsaturation (as opposed to vivid colors). Muted colors feel safe and secure, even\nnostalgic. They can also feel natural and\norganic. That\u2019s why many health and\nwellness brands have been using muted\ncolor palettes this year.<br><br>Simple Data\n|\nVisualizations\nThe goal of any data visualization should<br><br>be to make the complex\ndata easy to\nunderstand. We are living in a time where<br><br>a lot of data is constantly being circulated. Simple data visualizations\ncanmake\ncommunication more effective.<br><br>Geometric Shapes\nEverywhere<br><br>Last year, we saw designers usinga lot\nofflowing and abstract shapes in their\ndesigns. This year, they have been\nreplaced with rigid, hard-edged geo-\nmetric shapes and patterns. The hard\nedges of a geometric shape create a\ngreat contrast against muted colors.<br><br>Flat Icons and\nIllustrations<br><br>Many brands are using flat icons and\nillustrationsin their social media graphics,\nwebsite design, and more. Icons can be a<br><br>powerful tool for visual communication. With\na simple icon, you can communicate meaning\nin less space than words. Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[38] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. *Conference on Computer Vision and*\n*Pattern Recognition (CVPR)*, 2019. 6<br><br>[39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, L \u0301elio Renard Lavaud, Marie-Anne",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[38] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. *Conference on Computer Vision and*\n*Pattern Recognition (CVPR)*, 2019. 6<br><br>[39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, L \u0301elio Renard Lavaud, Marie-Anne",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**Large Vision-Language Models (LVLMs). **Large Lan-\nguage Models (LLMs) [6, 9, 10, 23, 39, 41, 73, 76, 78, 91\u2013\n93, 108] have gained significant attention due to their\nimpressive performance in various language-related tasks\nsuch as text generation and question answering. Follow-\ning this enthusiasm, recent Large Vision-Language Mod-\nels (LVLMs) have emerged[4, 7, 16, 18, 19, 25, 28, 32,\n47, 74, 77, 102, 110, 113], combining LLMs with vi-\nsion encoders [79, 89, 109] to leverage the complemen-\ntary strengths of language and vision modalities. By fusing\ntextual and visual representations, LVLMs can ground lan-\nguage in visual contexts, enabling a more comprehensive\nunderstanding and generation of multimodal content [5, 11,\n14, 20, 27, 51, 60, 95]. **LVLMs for High-Resolution Understanding. **\nLarge\nVision-Language Models (LVLMs) often employ CLIP-\nViT as the visual encoder for vision-dependent tasks. How-\never, the visual encoder\u2019s reliance on low resolutions,\nsuch as 224*\u00d7*224 or 336*\u00d7*336 pixels, limits its ef-\nfectiveness for high-resolution tasks like OCR and docu-\nment/chart perception. To enhance high-resolution under-\nstanding, recent works have primarily employed the fol-\nlowing strategies: (1) High-resolution (HR) visual encoders\nor dual encoders catering to HR and low-resolution (LR)\ninputs [36, 48, 66, 97]. For instance, Vary [97] intro-\nduces a new image encoder supporting HR inputs, which\nare then concatenated with LR embeddings from the orig-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**Large Vision-Language Models (LVLMs). **Large Lan-\nguage Models (LLMs) [6, 9, 10, 23, 39, 41, 73, 76, 78, 91\u2013\n93, 108] have gained significant attention due to their\nimpressive performance in various language-related tasks\nsuch as text generation and question answering. Follow-\ning this enthusiasm, recent Large Vision-Language Mod-\nels (LVLMs) have emerged[4, 7, 16, 18, 19, 25, 28, 32,\n47, 74, 77, 102, 110, 113], combining LLMs with vi-\nsion encoders [79, 89, 109] to leverage the complemen-\ntary strengths of language and vision modalities. By fusing\ntextual and visual representations, LVLMs can ground lan-\nguage in visual contexts, enabling a more comprehensive\nunderstanding and generation of multimodal content [5, 11,\n14, 20, 27, 51, 60, 95]. **LVLMs for High-Resolution Understanding. **\nLarge\nVision-Language Models (LVLMs) often employ CLIP-\nViT as the visual encoder for vision-dependent tasks. How-\never, the visual encoder\u2019s reliance on low resolutions,\nsuch as 224*\u00d7*224 or 336*\u00d7*336 pixels, limits its ef-\nfectiveness for high-resolution tasks like OCR and docu-\nment/chart perception. To enhance high-resolution under-\nstanding, recent works have primarily employed the fol-\nlowing strategies: (1) High-resolution (HR) visual encoders\nor dual encoders catering to HR and low-resolution (LR)\ninputs [36, 48, 66, 97]. For instance, Vary [97] intro-\nduces a new image encoder supporting HR inputs, which\nare then concatenated with LR embeddings from the orig-",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Figure 6. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Figure 6. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "open-source LVLMs and closed-source APIs. Here\nwe\nreport\nresults\nin\nDocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87],\nOCRBench[58],\nMMStar[15], MathVista[61], MMMU[107], AI2D[42],\nMME [31], MMBench (MMB) [57], MMBench-Chinese\n(MMB*CN*) [57], SEED-Bench Image Part (SEED*I*)[45],\nQBench-Testset (QBench*T*)[98],\nMM-Vet [105],\nHal-\nlusionBench (HallB)[34]. The evaluation is mainly\nconducted on the OpenCompass VLMEvalKit[24] for the\nunified reproduction of the results.<br><br>**Comparison with Closed-Source APIs. **As demonstrated\nin Table 3, IXC2-4KHD exhibits competitive performance\nacross a variety of benchmarks, rivaling that of Closed-\nSource APIs. Owing to its high-resolution input, IXC2-\n4KHD achieves a score of 90*. *0% on DocVQA and 81*. *0%\non ChartQA, thereby surpassing GPT-4V and Gemini-Pro\nwith a non-trivial margin. In the challenging Infograph-\nicVQA task, our model is the first open-source model that is\nclose to the performance of Closed-Source APIs, exceeding\nthe performance of previous open-source models by nearly\n20%. In addition to OCR-related tasks, IXC2-4KHD is a\ngeneral-purpose Large Vision-Language Modal that excels\nin semantic-level tasks, demonstrating competitive results.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "open-source LVLMs and closed-source APIs. Here\nwe\nreport\nresults\nin\nDocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87],\nOCRBench[58],\nMMStar[15], MathVista[61], MMMU[107], AI2D[42],\nMME [31], MMBench (MMB) [57], MMBench-Chinese\n(MMB*CN*) [57], SEED-Bench Image Part (SEED*I*)[45],\nQBench-Testset (QBench*T*)[98],\nMM-Vet [105],\nHal-\nlusionBench (HallB)[34]. The evaluation is mainly\nconducted on the OpenCompass VLMEvalKit[24] for the\nunified reproduction of the results.<br><br>**Comparison with Closed-Source APIs. **As demonstrated\nin Table 3, IXC2-4KHD exhibits competitive performance\nacross a variety of benchmarks, rivaling that of Closed-\nSource APIs. Owing to its high-resolution input, IXC2-\n4KHD achieves a score of 90*. *0% on DocVQA and 81*. *0%\non ChartQA, thereby surpassing GPT-4V and Gemini-Pro\nwith a non-trivial margin. In the challenging Infograph-\nicVQA task, our model is the first open-source model that is\nclose to the performance of Closed-Source APIs, exceeding\nthe performance of previous open-source models by nearly\n20%. In addition to OCR-related tasks, IXC2-4KHD is a\ngeneral-purpose Large Vision-Language Modal that excels\nin semantic-level tasks, demonstrating competitive results.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "switch to the HD-16 setting, we observe a performance gain\nof +10*.*2%. The performance continues to improve as the\nresolution increases, with saturation not observed even for\nthe 4KHD setting. Due to computational constraints, we\ndefer the exploration of the upper bound of improvement\nto future work. In terms of other OCR-related tasks, the\nperformance gain attributable to increased resolution is rel-\natively minor. For the perception-related benchmarks, per-\nformance is saturated on the resolution that only has negli-\ngible difference between the four settings. **Higher Inference Resolution Leads to better results on**\n**Text-related Tasks. **An intriguing observation from our ex-\nperiments is that our model, when inferring with a slightly\nhigher resolution, tends to yield improved results on text-\nrelated tasks. We present the results of HD-9, HD-16,\nand HD-25 in Table 6. For instance, IXC2-HD9 achieves\na 50*. *5% score on InfographicVQA. When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25. We posit that the dy-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25. We posit that the dy-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "switch to the HD-16 setting, we observe a performance gain\nof +10*.*2%. The performance continues to improve as the\nresolution increases, with saturation not observed even for\nthe 4KHD setting. Due to computational constraints, we\ndefer the exploration of the upper bound of improvement\nto future work. In terms of other OCR-related tasks, the\nperformance gain attributable to increased resolution is rel-\natively minor. For the perception-related benchmarks, per-\nformance is saturated on the resolution that only has negli-\ngible difference between the four settings. **Higher Inference Resolution Leads to better results on**\n**Text-related Tasks. **An intriguing observation from our ex-\nperiments is that our model, when inferring with a slightly\nhigher resolution, tends to yield improved results on text-\nrelated tasks. We present the results of HD-9, HD-16,\nand HD-25 in Table 6. For instance, IXC2-HD9 achieves\na 50*. *5% score on InfographicVQA. When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25. We posit that the dy-",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In*Proceedings of the International Conference on*\n*Machine learning (ICML)*, pages 8748\u20138763. PMLR, 2021. 2<br><br>[80] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. *arXiv preprint arXiv:2111.02114*, 2021. 6<br><br>[81] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In*European Conference on Computer Vision*,\npages 146\u2013162. Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In*Proceedings of the International Conference on*\n*Machine learning (ICML)*, pages 8748\u20138763. PMLR, 2021. 2<br><br>[80] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. *arXiv preprint arXiv:2111.02114*, 2021. 6<br><br>[81] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In*European Conference on Computer Vision*,\npages 146\u2013162. Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[35] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\njuan: A comprehensive multimodal dataset for advancing\nenglish and chinese large models. *ArXiv*, abs/2308.10755,\n2023. 6<br><br>[36] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\nWenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, et al. Cogagent: A visual language\nmodel for gui agents. *arXiv preprint arXiv:2312.08914*,\n2023. 2, 5, 7, 8<br><br>[37] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\nZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei\nHuang, et al. mplug-docowl 1.5: Unified structure learn-\ning for ocr-free document understanding. *arXiv preprint*\n*arXiv:2403.12895*, 2024. 2, 5, 7, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[35] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\njuan: A comprehensive multimodal dataset for advancing\nenglish and chinese large models. *ArXiv*, abs/2308.10755,\n2023. 6<br><br>[36] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\nWenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, et al. Cogagent: A visual language\nmodel for gui agents. *arXiv preprint arXiv:2312.08914*,\n2023. 2, 5, 7, 8<br><br>[37] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\nZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei\nHuang, et al. mplug-docowl 1.5: Unified structure learn-\ning for ocr-free document understanding. *arXiv preprint*\n*arXiv:2403.12895*, 2024. 2, 5, 7, 8",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[12] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and\nByungseok Roh. Honeybee: Locality-enhanced projec-\ntor for multimodal llm. *arXiv preprint arXiv:2312.06742*,\n2023. 9<br><br>[13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. *arXiv.org*, 2023. 6<br><br>[14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. *arXiv preprint arXiv:2311.12793*, 2023. 1, 2, 6<br><br>[15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, and Feng Zhao. Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024. 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024. 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[12] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and\nByungseok Roh. Honeybee: Locality-enhanced projec-\ntor for multimodal llm. *arXiv preprint arXiv:2312.06742*,\n2023. 9<br><br>[13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. *arXiv.org*, 2023. 6<br><br>[14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. *arXiv preprint arXiv:2311.12793*, 2023. 1, 2, 6<br><br>[15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, and Feng Zhao. Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024. 5, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "In practice, we employ the OpenAI CLIP ViT-L-14-336\nas the vision encoder. Different from XComposer2, We\nkeep the ViT resolution as 336*\u00d7*336 and increase the input\nresolution with more patches. For the Dynamic Image Par-\ntition strategy, we use \u2018HD-25\u2019 for the pertaining. For each\nimage or patch, the image token number is decreased to 1*/*4\nwith a simple**merge operation**. We concatenate the nearby\n4 tokens into a new token through the channel dimension,\nthen align it with the LLM by an MLP. The \u2018separate\u2019 and\n\u2018*\\*n\u2019 token are randomly initialized. For the Partial LoRA,\nwe set a rank of 256 for all the linear layers in the LLM de-\ncoder block. Our training process involves a batch size of\n4096 and spans across 2 epochs. The learning rate linearly\nincreases to 2*\u00d7*10*\u2212*4 within the first 1% of the training\nsteps. Following this, it decreases to 0 according to a co-\nsine decay strategy. To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In practice, we employ the OpenAI CLIP ViT-L-14-336\nas the vision encoder. Different from XComposer2, We\nkeep the ViT resolution as 336*\u00d7*336 and increase the input\nresolution with more patches. For the Dynamic Image Par-\ntition strategy, we use \u2018HD-25\u2019 for the pertaining. For each\nimage or patch, the image token number is decreased to 1*/*4\nwith a simple**merge operation**. We concatenate the nearby\n4 tokens into a new token through the channel dimension,\nthen align it with the LLM by an MLP. The \u2018separate\u2019 and\n\u2018*\\*n\u2019 token are randomly initialized. For the Partial LoRA,\nwe set a rank of 256 for all the linear layers in the LLM de-\ncoder block. Our training process involves a batch size of\n4096 and spans across 2 epochs. The learning rate linearly\nincreases to 2*\u00d7*10*\u2212*4 within the first 1% of the training\nsteps. Following this, it decreases to 0 according to a co-\nsine decay strategy. To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[28] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[28] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "0.18 # 1 @ Payal # Support \u00a9 Engnering \u00a9 Faces \u00a9 Finance\n\u00a9 Sales",
            {
                "chunk_size": "small"
            }
        ],
        [
            "0.18 # 1 @ Payal # Support \u00a9 Engnering \u00a9 Faces \u00a9 Finance\n\u00a9 Sales",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[50] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important\nthings for large multi-modal models. *arXiv preprint*\n*arXiv:2311.06607*, 2023. 2, 5, 7<br><br>[51] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\nQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[50] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important\nthings for large multi-modal models. *arXiv preprint*\n*arXiv:2311.06607*, 2023. 2, 5, 7<br><br>[51] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\nQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[104] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengy-\ning Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. Metamath: Bootstrap your own\nmathematical questions for large language models. *arXiv*\n*preprint arXiv:2309.12284*, 2023. 6<br><br>[105] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. *arXiv preprint arXiv:2308.02490*, 2023. 7<br><br>[106] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in\nthe wild. *Journal of Computer Science and Technology*,\n34(3):509\u2013521, 2019. 6<br><br>[107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\nYang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\nWenhu Chen. Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[104] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengy-\ning Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. Metamath: Bootstrap your own\nmathematical questions for large language models. *arXiv*\n*preprint arXiv:2309.12284*, 2023. 6<br><br>[105] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. *arXiv preprint arXiv:2308.02490*, 2023. 7<br><br>[106] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in\nthe wild. *Journal of Computer Science and Technology*,\n34(3):509\u2013521, 2019. 6<br><br>[107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\nYang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\nWenhu Chen. Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023. 1, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Model\nDoc Info Text Chart MMB MME SEED*\u2217*<br><br>HD9\n79.4 50.5 73.8\n78.2\n79.5\n2201\n76.6\n+ w/o global-view 78.1 47.9 71.2\n77.9\n75.1\n2019\n76.2<br><br>Table 7. **Influence of Global-View in the Input. **Global-view is\ncritical for most benchmarks.<br><br>namic image token length used in training enhances the ro-\nbustness of the LVLM, leading to better results when the\ntext in the image is more \u2018clear\u2019 in the higher resolution\ninput. Conversely, the results on ChartQA consistently de-\ngrade under this setting. This could be due to the model be-\ncoming confused about the chart structure when the resolu-\ntion is altered. Additionally, similar to the observation from\nFigure 5, the impact of resolution on perception-related\nbenchmarks appears to be quite minor. **Visualization Results. **We provide the visualization results\non ultra-high HD images in Figure 2 and Figure 3. Please\nrefer to the appendix for more results.<br><br>found this approach to be effective in reducing the num-\nber of image tokens efficiently. Here we study the influence\nof different token-merging strategies under the 4KHD set-\nting. In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3. High-Resolution Strategy Ablation**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3. High-Resolution Strategy Ablation**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Model\nDoc Info Text Chart MMB MME SEED*\u2217*<br><br>HD9\n79.4 50.5 73.8\n78.2\n79.5\n2201\n76.6\n+ w/o global-view 78.1 47.9 71.2\n77.9\n75.1\n2019\n76.2<br><br>Table 7. **Influence of Global-View in the Input. **Global-view is\ncritical for most benchmarks.<br><br>namic image token length used in training enhances the ro-\nbustness of the LVLM, leading to better results when the\ntext in the image is more \u2018clear\u2019 in the higher resolution\ninput. Conversely, the results on ChartQA consistently de-\ngrade under this setting. This could be due to the model be-\ncoming confused about the chart structure when the resolu-\ntion is altered. Additionally, similar to the observation from\nFigure 5, the impact of resolution on perception-related\nbenchmarks appears to be quite minor. **Visualization Results. **We provide the visualization results\non ultra-high HD images in Figure 2 and Figure 3. Please\nrefer to the appendix for more results.<br><br>found this approach to be effective in reducing the num-\nber of image tokens efficiently. Here we study the influence\nof different token-merging strategies under the 4KHD set-\nting. In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3. High-Resolution Strategy Ablation**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>IXC2-VL</th><th>InernLM2-7B</th><th>55.4</th><th>57.6</th><th>81.2</th><th>1,712.0</th><th>530.7</th><th>80.7</th><th>79.4</th><th>74.9</th><th>72.5</th><th>46.7</th></tr>\n<tr><td>IXC2-VL</td><td>InernLM2-7B</td><td>55.4</td><td>57.6</td><td>81.2</td><td>1,712.0</td><td>530.7</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td></tr>\n<tr><td>IXC2-4KHD</td><td>InernLM2-7B</td><td>54.1</td><td>57.8</td><td>80.9</td><td>1,655.9</td><td>548.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>IXC2-VL</th><th>InernLM2-7B</th><th>55.4</th><th>57.6</th><th>81.2</th><th>1,712.0</th><th>530.7</th><th>80.7</th><th>79.4</th><th>74.9</th><th>72.5</th><th>46.7</th></tr>\n<tr><td>IXC2-VL</td><td>InernLM2-7B</td><td>55.4</td><td>57.6</td><td>81.2</td><td>1,712.0</td><td>530.7</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td></tr>\n<tr><td>IXC2-4KHD</td><td>InernLM2-7B</td><td>54.1</td><td>57.8</td><td>80.9</td><td>1,655.9</td><td>548.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>HD25\nHD25\nHD30</th><th>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</th></tr>\n<tr><td>HD25\nHD25\nHD30</td><td>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</td></tr>\n</table><br><br>Table 6. **Influence of Inference Resolution. **The model achieves\nbetter performance on text-related tasks when the inference reso-\nlution is higher than its training resolution.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>HD25\nHD25\nHD30</th><th>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</th></tr>\n<tr><td>HD25\nHD25\nHD30</td><td>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</td></tr>\n</table><br><br>Table 6. **Influence of Inference Resolution. **The model achieves\nbetter performance on text-related tasks when the inference reso-\nlution is higher than its training resolution.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "\u201caencare |<br><br>Figure 4. **The illustration of processing high-resolution input.**<br><br>**3.2. High-Resolution Input. **<br><br>**Dynamic Image Partition. **\nUtilizing a static input im-\nage size for processing high-resolution images, particularly\nthose with varying aspect ratios, is neither efficient nor ef-\nfective. To overcome this limitation, we introduce a dy-\nnamic image partitioning approach, as shown in Figure 4. Our method strategically segments the image into smaller\npatches, while maintaining the integrity of the original im-\nage\u2019s aspect ratio. Given a maximum partition number*H*, the image*x*with\nsize [*h, w*] is resized and padded to the new image \u02c6*x*with\nsize [*ph \u00d7*336*, pw \u00d7*336]. This process is subject to the\nfollowing constraints:<br><br>*p_w  \\*t*im*e*s p_h \\leq \\mathcal {H}; \\; p_h = \\lceil p_w \\times h / w \\rceil*\n(1)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u201caencare |<br><br>Figure 4. **The illustration of processing high-resolution input.**<br><br>**3.2. High-Resolution Input. **<br><br>**Dynamic Image Partition. **\nUtilizing a static input im-\nage size for processing high-resolution images, particularly\nthose with varying aspect ratios, is neither efficient nor ef-\nfective. To overcome this limitation, we introduce a dy-\nnamic image partitioning approach, as shown in Figure 4. Our method strategically segments the image into smaller\npatches, while maintaining the integrity of the original im-\nage\u2019s aspect ratio. Given a maximum partition number*H*, the image*x*with\nsize [*h, w*] is resized and padded to the new image \u02c6*x*with\nsize [*ph \u00d7*336*, pw \u00d7*336]. This process is subject to the\nfollowing constraints:<br><br>*p_w  \\*t*im*e*s p_h \\leq \\mathcal {H}; \\; p_h = \\lceil p_w \\times h / w \\rceil*\n(1)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "a>\n@ HR @ IT @ Payroll @ Support @ Engineering @ Facilities @ Finance @ Sales\nConduct an in-depth analysis of the chart.<br><br>7)<br><br>The line chart presents the operations cost by department in US \ndollars for various categories from May 2018 to Mar 2019. The \ncategories include Human Resources (HR), Information \nTechnology (IT), Payroll, Support, Engineering, Facilities, \nFinance, and Sales. Each category is represented by a different \ncolor and line on the graph. The y-axis represents the cost in \nthousands of US dollars, while the x-axis represents the months \nfrom May 2018 to Mar 2019. The graph shows fluctuations in \nthe cost for each category throughout the months, with some \ncategories experiencing peaks and troughs at different times. 80xee\nSKS\n~\nZg\n=\n=\nee\n\u2014~\na,\n\u2014=\n\u201c=\nSS==a\nway 2018\nsut20t8\nsep 2018\nNow 2018\nJon 2019\nWar 20\nMonths<br><br>a><br><br>a\n0\nMay 2018\nMonths\nJan 2019\nMar 29\n7) Payroll has the highest cost in most cases, as indicated by \nthe purple line on the graph.<br><br>The HD image (1475 x 675)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\n@ HR @ IT @ Payroll @ Support @ Engineering @ Facilities @ Finance @ Sales\nConduct an in-depth analysis of the chart.<br><br>7)<br><br>The line chart presents the operations cost by department in US \ndollars for various categories from May 2018 to Mar 2019. The \ncategories include Human Resources (HR), Information \nTechnology (IT), Payroll, Support, Engineering, Facilities, \nFinance, and Sales. Each category is represented by a different \ncolor and line on the graph. The y-axis represents the cost in \nthousands of US dollars, while the x-axis represents the months \nfrom May 2018 to Mar 2019. The graph shows fluctuations in \nthe cost for each category throughout the months, with some \ncategories experiencing peaks and troughs at different times. 80xee\nSKS\n~\nZg\n=\n=\nee\n\u2014~\na,\n\u2014=\n\u201c=\nSS==a\nway 2018\nsut20t8\nsep 2018\nNow 2018\nJon 2019\nWar 20\nMonths<br><br>a><br><br>a\n0\nMay 2018\nMonths\nJan 2019\nMar 29\n7) Payroll has the highest cost in most cases, as indicated by \nthe purple line on the graph.<br><br>The HD image (1475 x 675)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "wid\u00a9a4\u201c<\nie\nete|\nlate<br><br>Social Media\nSlide Decks\nAlot of people have been sharing slide decks<br><br>on Instagram and Linkedin. They're a visual\nway of communicating longer messages than\na single image post. That\u2019s why we've seen<br><br>so many social justice, political and wellness\naccounts sharing slide decks. On top of that,\nthe algorithms\non Instagram and Linkedin. seem to promote these slide decks a lot\nmore thana single image.<br><br>Text Heavy Videos\nPeople will likely continue to work\nremotely well into 2021. That means\nshooting new video content is going to\nbe difficult. Videos that make use of text\non-screen to communicate messages\nare a way of getting around that.<br><br>Instead of needing a whole production\nteam to create a video, brands can\ncreate a simple text heavy video in\na fraction ofthe time.<br><br>Read the full guide: venngage.com/blog/graphic-design-trends",
            {
                "chunk_size": "small"
            }
        ],
        [
            "wid\u00a9a4\u201c<\nie\nete|\nlate<br><br>Social Media\nSlide Decks\nAlot of people have been sharing slide decks<br><br>on Instagram and Linkedin. They're a visual\nway of communicating longer messages than\na single image post. That\u2019s why we've seen<br><br>so many social justice, political and wellness\naccounts sharing slide decks. On top of that,\nthe algorithms\non Instagram and Linkedin. seem to promote these slide decks a lot\nmore thana single image.<br><br>Text Heavy Videos\nPeople will likely continue to work\nremotely well into 2021. That means\nshooting new video content is going to\nbe difficult. Videos that make use of text\non-screen to communicate messages\nare a way of getting around that.<br><br>Instead of needing a whole production\nteam to create a video, brands can\ncreate a simple text heavy video in\na fraction ofthe time.<br><br>Read the full guide: venngage.com/blog/graphic-design-trends",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Amputee\npaneer\n7<br><br>initialize\namoot\n= feat\neumemtoy O04)<br><br>goo (ins ian si44)<br><br>\u00e9\nere\nainda aren\na quuatontion\nhoe\n'\n2<br><br>3\nmort= cummed\n7\nab\nith, Yndew\n:\nund of gor WP<br><br>Se<br><br>The HD image (1888 x 3172)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Amputee\npaneer\n7<br><br>initialize\namoot\n= feat\neumemtoy O04)<br><br>goo (ins ian si44)<br><br>\u00e9\nere\nainda aren\na quuatontion\nhoe\n'\n2<br><br>3\nmort= cummed\n7\nab\nith, Yndew\n:\nund of gor WP<br><br>Se<br><br>The HD image (1888 x 3172)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br><table border=\"1\">\n<tr><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>) creme, e -\u00a7 SHES\" paw\nEncode Resize :\nSEEETON | cercemesaaes | Seale, A TREE\niliNg\nDon\u2019t send\n& Merge EDUCATE r ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW curs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc",
            {
                "chunk_size": "small"
            }
        ],
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br><table border=\"1\">\n<tr><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>) creme, e -\u00a7 SHES\" paw\nEncode Resize :\nSEEETON | cercemesaaes | Seale, A TREE\niliNg\nDon\u2019t send\n& Merge EDUCATE r ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW curs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "a>\nDetect all the word in the given image<br><br>7)\nAmerican Heart Association. VennGAGE. <br><br>How to Develop Healthy Eating Habits. And Reduce the Risk of Heart Disease. <br><br>Your best defense is a healthy lifestyle filled with physical activity and \nnutritious meals, while cutting down or avoiding drinking and smoking. <br><br>The Current Trend<br><br>75% of the US population is low in its fruit and vegetables consumption. 1/10 \nadults meets the recommendation for fruit and vegetable consumption. Men and \nyoung adults in general eat the fewest fruits and vegetables compared to any \nother group. <br><br>3 Steps to Eating Healthy 1. Add produce of various colors to your diet to \ndiversify your produce consumption. 2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nDetect all the word in the given image<br><br>7)\nAmerican Heart Association. VennGAGE. <br><br>How to Develop Healthy Eating Habits. And Reduce the Risk of Heart Disease. <br><br>Your best defense is a healthy lifestyle filled with physical activity and \nnutritious meals, while cutting down or avoiding drinking and smoking. <br><br>The Current Trend<br><br>75% of the US population is low in its fruit and vegetables consumption. 1/10 \nadults meets the recommendation for fruit and vegetable consumption. Men and \nyoung adults in general eat the fewest fruits and vegetables compared to any \nother group. <br><br>3 Steps to Eating Healthy 1. Add produce of various colors to your diet to \ndiversify your produce consumption. 2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</th><th>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</th><th>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</th></tr>\n<tr><td>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</td><td>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</td><td>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</th><th>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</th><th>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</th></tr>\n<tr><td>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</td><td>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</td><td>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>GPT-4V\nGemini-Pro</th><th>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</th></tr>\n<tr><td>GPT-4V\nGemini-Pro</td><td>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>GPT-4V\nGemini-Pro</th><th>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</th></tr>\n<tr><td>GPT-4V\nGemini-Pro</td><td>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "75%\n1/10\n(*)\nMen and young adults\nof the US population is\nadults meets the\nin general eat the fewest\nlow in its fruit and\nrecommendation for fruit and\nfruits and vegetables,\nvegetables consumption. vegetable consumption. compared to any other group.<br><br>3 Steps to Eating Healthy\n1\n2\n3\n\u2018Add produce of varying colors\nUse hacks to incorporate more\nTake steps to improve\nproduce consumption.to your diet to diversify your\nveggies and fruits to your meals,\naccessto fruits and\nand throughout the day. vegetables forall",
            {
                "chunk_size": "small"
            }
        ],
        [
            "75%\n1/10\n(*)\nMen and young adults\nof the US population is\nadults meets the\nin general eat the fewest\nlow in its fruit and\nrecommendation for fruit and\nfruits and vegetables,\nvegetables consumption. vegetable consumption. compared to any other group.<br><br>3 Steps to Eating Healthy\n1\n2\n3\n\u2018Add produce of varying colors\nUse hacks to incorporate more\nTake steps to improve\nproduce consumption.to your diet to diversify your\nveggies and fruits to your meals,\naccessto fruits and\nand throughout the day. vegetables forall",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Source: https://ww.empoweredtoserve.rg/-/media/ETS-files/Community Resources/Health-Lessons\nAmgen Branded/Fruits-and-Veggies/DS18477_ETS_Amgen_Presentation_FruitVeg_OTkk_\nNOTES. pdf<br><br>The HD image (816 x 3813)<br><br>Figure 7. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.<br><br>16",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Source: https://ww.empoweredtoserve.rg/-/media/ETS-files/Community Resources/Health-Lessons\nAmgen Branded/Fruits-and-Veggies/DS18477_ETS_Amgen_Presentation_FruitVeg_OTkk_\nNOTES. pdf<br><br>The HD image (816 x 3813)<br><br>Figure 7. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.<br><br>16",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In*ICDAR*, 2019. 6<br><br>[73] OpenAI. Chatgpt. https://openai.com/blog/\nchatgpt, 2022. 1, 2<br><br>[74] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[75] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text:\nDescribing images using 1 million captioned\nphotographs. In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730\u201327744, 2022. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730\u201327744, 2022. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In*ICDAR*, 2019. 6<br><br>[73] OpenAI. Chatgpt. https://openai.com/blog/\nchatgpt, 2022. 1, 2<br><br>[74] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[75] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text:\nDescribing images using 1 million captioned\nphotographs. In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730\u201327744, 2022. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Overall, the infographic provides a \ncomprehensive overview of the 7 graphic design \ntrends that are expected to shape the design \nlandscape in 2021.<br><br>=\nWhat is the detailed explanation of the \nthird part?<br><br>*<br><br>The third part of the image is titled \"Geometric \nShapes Everywhere\". It explains that last year, \ndesigners used a lot of flowing and abstract \nshapes in their designs. However, this year, they \nhave been replaced with rigid, hard-edged \ngeometric shapes and patterns. The hard edges of \na geometric shape create a great contrast against \nmuted colors.<br><br>\u00ae\n(@)<br><br>=<br><br>*\nYou can find the full guide on \nvennage.com/blog/graphic-design-trends.<br><br>The HD image (816 x 5133)<br><br>Figure 3. **Chat with InternLM-XComposer2-4KHD on ultra-high HD image with the 816***\u00d7***5133 resolution**. Some regions of the\ninput HD images are zoomed in for better visualization.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Overall, the infographic provides a \ncomprehensive overview of the 7 graphic design \ntrends that are expected to shape the design \nlandscape in 2021.<br><br>=\nWhat is the detailed explanation of the \nthird part?<br><br>*<br><br>The third part of the image is titled \"Geometric \nShapes Everywhere\". It explains that last year, \ndesigners used a lot of flowing and abstract \nshapes in their designs. However, this year, they \nhave been replaced with rigid, hard-edged \ngeometric shapes and patterns. The hard edges of \na geometric shape create a great contrast against \nmuted colors.<br><br>\u00ae\n(@)<br><br>=<br><br>*\nYou can find the full guide on \nvennage.com/blog/graphic-design-trends.<br><br>The HD image (816 x 5133)<br><br>Figure 3. **Chat with InternLM-XComposer2-4KHD on ultra-high HD image with the 816***\u00d7***5133 resolution**. Some regions of the\ninput HD images are zoomed in for better visualization.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[63] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal rea-\nsoning via thought chains for science question answer-\ning. *Advances in Neural Information Processing Systems*,\n35:2507\u20132521, 2022. 6<br><br>[64] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning. *arXiv preprint*",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[63] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal rea-\nsoning via thought chains for science question answer-\ning. *Advances in Neural Information Processing Systems*,\n35:2507\u20132521, 2022. 6<br><br>[64] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning. *arXiv preprint*",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "github.com/InternLM/InternLM, 2023. 1, 2, 6<br><br>[92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth \u0301ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. *arXiv.org*, 2023.<br><br>[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models,\n2023. 1, 2<br><br>[94] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. *arXiv preprint*\n*arXiv:2311.07574*, 2023. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "github.com/InternLM/InternLM, 2023. 1, 2, 6<br><br>[92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth \u0301ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. *arXiv.org*, 2023.<br><br>[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models,\n2023. 1, 2<br><br>[94] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. *arXiv preprint*\n*arXiv:2311.07574*, 2023. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[111] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\nLi, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\nMingkun Yang, et al. Icdar 2019 robust reading challenge\non reading chinese text on signboard. In*2019 international*\n*conference on document analysis and recognition (ICDAR)*,\npages 1577\u20131581. IEEE, 2019. 6<br><br>[112] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: En-\nhanced visual instruction tuning for text-rich image under-\nstanding. *arXiv preprint arXiv:2306.17107*, 2023. 5<br><br>[113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. Minigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language\nmodels. *arXiv.org*, 2023. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[111] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\nLi, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\nMingkun Yang, et al. Icdar 2019 robust reading challenge\non reading chinese text on signboard. In*2019 international*\n*conference on document analysis and recognition (ICDAR)*,\npages 1577\u20131581. IEEE, 2019. 6<br><br>[112] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: En-\nhanced visual instruction tuning for text-rich image under-\nstanding. *arXiv preprint arXiv:2306.17107*, 2023. 5<br><br>[113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. Minigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language\nmodels. *arXiv.org*, 2023. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In*International*\n*Conference on Computer Vision (ICCV)*, 2015. 6<br><br>[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt. Openflamingo:\nAn open-\nsource framework for training large autoregressive vision-\nlanguage models. *arXiv.org*, 2023. 2<br><br>[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. *arXiv.org*, 2023. 2, 9",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In*International*\n*Conference on Computer Vision (ICCV)*, 2015. 6<br><br>[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt. Openflamingo:\nAn open-\nsource framework for training large autoregressive vision-\nlanguage models. *arXiv.org*, 2023. 2<br><br>[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. *arXiv.org*, 2023. 2, 9",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Find ways to spread the word about nutrition assistance programs, such as \nSNAP, WIC and school meals. <br><br>Create a petition for more variety, improve affordability and advocate for better \nsignage/placement. <br><br>Meet with an after-school or daycare program representative to discuss serving \nmore fruits and vegetables for snacks. Organize a letter-writing campaign and \nset up a meeting with state leaders. <br><br>For example, ask for funding to host a farmers market in an underserved \ncommunity. Sign up for \"You're the Cure\" and send a note to your \nCongressperson advocating for healthier meals at school. <br><br>It's American Heart Month! Share this infographic with your family, friends and \nnetwork today. You can carry out all of these actions or just a few. But \nwhichever actions you choose, they pave the way for greater access to nutritious \nfood for your community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Find ways to spread the word about nutrition assistance programs, such as \nSNAP, WIC and school meals. <br><br>Create a petition for more variety, improve affordability and advocate for better \nsignage/placement. <br><br>Meet with an after-school or daycare program representative to discuss serving \nmore fruits and vegetables for snacks. Organize a letter-writing campaign and \nset up a meeting with state leaders. <br><br>For example, ask for funding to host a farmers market in an underserved \ncommunity. Sign up for \"You're the Cure\" and send a note to your \nCongressperson advocating for healthier meals at school. <br><br>It's American Heart Month! Share this infographic with your family, friends and \nnetwork today. You can carry out all of these actions or just a few. But \nwhichever actions you choose, they pave the way for greater access to nutritious \nfood for your community.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "\u2018Adding a variety of produce to your diet has a number of health benefits. It'll help you:<br><br>Manage your weight\nControl your blood pressure\nSupport healthy digestion<br><br>Reduce the risk of some\nReduce the risk of chronic health\ncancers, such as colon cancer\nproblems, such as diabetes",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u2018Adding a variety of produce to your diet has a number of health benefits. It'll help you:<br><br>Manage your weight\nControl your blood pressure\nSupport healthy digestion<br><br>Reduce the risk of some\nReduce the risk of chronic health\ncancers, such as colon cancer\nproblems, such as diabetes",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**1. Introduction**<br><br>In recent years, the progress in Large Language Models\n(LLMs) [10, 21, 29, 39, 73, 78, 91\u201393] has provoked the\ndevelopment of Large Vision-Language Models (LVLMs). These models have demonstrated proficiency in tasks such\nas image captioning [14, 17] and visual-question-answering\n(VQA) [31, 33, 57, 107]. Nevertheless, due to their limited\nresolution, they struggle with processing images containing\nfine details, such as charts [68], tables [87], documents [70],\nand infographics [69]. This limitation constrains their prac-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. Introduction**<br><br>In recent years, the progress in Large Language Models\n(LLMs) [10, 21, 29, 39, 73, 78, 91\u201393] has provoked the\ndevelopment of Large Vision-Language Models (LVLMs). These models have demonstrated proficiency in tasks such\nas image captioning [14, 17] and visual-question-answering\n(VQA) [31, 33, 57, 107]. Nevertheless, due to their limited\nresolution, they struggle with processing images containing\nfine details, such as charts [68], tables [87], documents [70],\nand infographics [69]. This limitation constrains their prac-",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth \u0301ee Lacroix, and William El Sayed. Mistral 7b, 2023. 1, 2<br><br>[40] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. Dvqa: Understanding data visualizations via ques-\ntion answering. In*Proceedings of the IEEE conference on*\n*computer vision and pattern recognition*, pages 5648\u20135656,\n2018. 6<br><br>[41] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. *arXiv preprint arXiv:2001.08361*,\n2020. 2<br><br>[42] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In*Computer Vision\u2013ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11\u201314, 2016, Proceedings, Part IV 14*, pages 235\u2013\n251. Springer, 2016. 6, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In*Computer Vision\u2013ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11\u201314, 2016, Proceedings, Part IV 14*, pages 235\u2013\n251. Springer, 2016. 6, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth \u0301ee Lacroix, and William El Sayed. Mistral 7b, 2023. 1, 2<br><br>[40] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. Dvqa: Understanding data visualizations via ques-\ntion answering. In*Proceedings of the IEEE conference on*\n*computer vision and pattern recognition*, pages 5648\u20135656,\n2018. 6<br><br>[41] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. *arXiv preprint arXiv:2001.08361*,\n2020. 2<br><br>[42] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In*Computer Vision\u2013ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11\u201314, 2016, Proceedings, Part IV 14*, pages 235\u2013\n251. Springer, 2016. 6, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Figure 2. **Chat with InternLM-XComposer2-4KHD**. Some regions of the input HD images are zoomed in for better visualization.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Figure 2. **Chat with InternLM-XComposer2-4KHD**. Some regions of the input HD images are zoomed in for better visualization.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "\u00a7} ShareGPT4\u00a5: Improving Large Multi-Modal Models with Better Captions<br><br>Lin Chen**! ?, Jinsong Li*\u00ae?, Xiaoyi Dong?, Pan Zhang\u201d, Conghui He?, Jiaqi Wang?,\nFeng Zhao! ', Dahua Lin\u2019? 1University of Science and Technology of China\nShanghai AI Laboratory",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u00a7} ShareGPT4\u00a5: Improving Large Multi-Modal Models with Better Captions<br><br>Lin Chen**! ?, Jinsong Li*\u00ae?, Xiaoyi Dong?, Pan Zhang\u201d, Conghui He?, Jiaqi Wang?,\nFeng Zhao! ', Dahua Lin\u2019? 1University of Science and Technology of China\nShanghai AI Laboratory",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**3. Method**<br><br>**3.1. Model Architecture. **<br><br>The model architecture of InternLM-XComposer2-4KHD\nmainly follows the design of InternLM-XComposer2[27]\n(XComposer2 in the following for simplicity. ), including a\nlight-weight Vision Encoder OpenAI ViT-Large/14, Large\nLanguage Model InternLM2-7B, and Partial LoRA for effi-\ncient alignment. We recommend the readers to the XCom-\nposer2 paper for more details.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3. Method**<br><br>**3.1. Model Architecture. **<br><br>The model architecture of InternLM-XComposer2-4KHD\nmainly follows the design of InternLM-XComposer2[27]\n(XComposer2 in the following for simplicity. ), including a\nlight-weight Vision Encoder OpenAI ViT-Large/14, Large\nLanguage Model InternLM2-7B, and Partial LoRA for effi-\ncient alignment. We recommend the readers to the XCom-\nposer2 paper for more details.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "And Reduce the Risk of Heart Disease\na\n. *\n%\nw\nRs\n=\n~~\nY<br><br>Your best defense is a healthy lifestyle filled with physical activity and\nnutritious meals, while cutting down or avoiding drinking and smoking.<br><br>The Current Trend",
            {
                "chunk_size": "small"
            }
        ],
        [
            "And Reduce the Risk of Heart Disease\na\n. *\n%\nw\nRs\n=\n~~\nY<br><br>Your best defense is a healthy lifestyle filled with physical activity and\nnutritious meals, while cutting down or avoiding drinking and smoking.<br><br>The Current Trend",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "a>\nWhat is the algorithm of this flow chart?<br><br>7)\nThe flow chart in the image is an algorithm for finding the maximum element in an array. Here is a \ndetailed explanation of the algorithm:<br><br>1. Declare an integer array having size m.\n2. Input the m numbers into the array. 3. Initialize max as the first element of the array. 4. For each element in the array, starting from the second element:\nIf the current element is greater than max, set max to the current element. 5. At the end of the loop, max will hold the maximum element in the array. 6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \nelement is stored in the variable max, and it is then outputted",
            {
                "chunk_size": "small"
            }
        ],
        [
            "6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \nelement is stored in the variable max, and it is then outputted",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nWhat is the algorithm of this flow chart?<br><br>7)\nThe flow chart in the image is an algorithm for finding the maximum element in an array. Here is a \ndetailed explanation of the algorithm:<br><br>1. Declare an integer array having size m.\n2. Input the m numbers into the array. 3. Initialize max as the first element of the array. 4. For each element in the array, starting from the second element:\nIf the current element is greater than max, set max to the current element. 5. At the end of the loop, max will hold the maximum element in the array. 6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \nelement is stored in the variable max, and it is then outputted",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Adding a variety of produce to your diet has a number of health benefits. It \nhelps you: Manage your weight, Control your blood pressure, Support healthy \ndigestion, Reduce the risk of some cancers, such as colon cancer, Reduce the \nrisk of chronic health problems, such as diabetes. <br><br>2. Use Healthy Eating Hacks <br><br>1. Fresh, frozen, canned and dried all count toward your daily servings, so you \nhave plenty of options. 2. Always reserve half of your plate for fruits and \nvegetables. 3. Choose whole fruits and vegetables. 4. Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Adding a variety of produce to your diet has a number of health benefits. It \nhelps you: Manage your weight, Control your blood pressure, Support healthy \ndigestion, Reduce the risk of some cancers, such as colon cancer, Reduce the \nrisk of chronic health problems, such as diabetes. <br><br>2. Use Healthy Eating Hacks <br><br>1. Fresh, frozen, canned and dried all count toward your daily servings, so you \nhave plenty of options. 2. Always reserve half of your plate for fruits and \nvegetables. 3. Choose whole fruits and vegetables. 4. Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[18] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut. Pali-3 vision language models: Smaller, faster,\nstronger, 2023. 2<br><br>[19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[18] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut. Pali-3 vision language models: Smaller, faster,\nstronger, 2023. 2<br><br>[19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>)\ncreme, e -\u00a7\nSHES\" paw\n:\nSEEETON | cercemesaaes | Seale, A TREE\nili Ng\nDon\u2019t send\nEDUCATE\nr ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW\ncurs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc<br><br>Dynamic Image \nPartition\nR PLANET STARTS PROTECTING OU<br><br><table border=\"1\">\n<tr><th></th><th></th><th></th><th></th><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>Dalve vess\n\u00a2 Freocte<br><br>Encode\nWA\n= ane\n& Merge<br><br>ae<br><br>ae\n\u00ab(1\nEDUCATE.<br><br>es \u00e9nour\nee Y\nVolunteer! | 2\nin",
            {
                "chunk_size": "small"
            }
        ],
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>)\ncreme, e -\u00a7\nSHES\" paw\n:\nSEEETON | cercemesaaes | Seale, A TREE\nili Ng\nDon\u2019t send\nEDUCATE\nr ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW\ncurs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc<br><br>Dynamic Image \nPartition\nR PLANET STARTS PROTECTING OU<br><br><table border=\"1\">\n<tr><th></th><th></th><th></th><th></th><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>Dalve vess\n\u00a2 Freocte<br><br>Encode\nWA\n= ane\n& Merge<br><br>ae<br><br>ae\n\u00ab(1\nEDUCATE.<br><br>es \u00e9nour\nee Y\nVolunteer! | 2\nin",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Chen, et al. Sphinx: The joint mixing of weights, tasks, and\nvisual embeddings for multi-modal large language models. *arXiv preprint arXiv:2311.07575*, 2023. 2, 5<br><br>[52] Adam Dahlgren Lindstr \u0308om and Savitha Sam Abra-\nham. Clevr-math:\nA dataset for compositional lan-\nguage, visual and mathematical reasoning. *arXiv preprint*\n*arXiv:2208.05358*, 2022. 6<br><br>[53] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. *Transactions of the Association*\n*for Computational Linguistics*, 2023. 6<br><br>[54] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu. Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024. 2, 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024. 2, 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Chen, et al. Sphinx: The joint mixing of weights, tasks, and\nvisual embeddings for multi-modal large language models. *arXiv preprint arXiv:2311.07575*, 2023. 2, 5<br><br>[52] Adam Dahlgren Lindstr \u0308om and Savitha Sam Abra-\nham. Clevr-math:\nA dataset for compositional lan-\nguage, visual and mathematical reasoning. *arXiv preprint*\n*arXiv:2208.05358*, 2022. 6<br><br>[53] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. *Transactions of the Association*\n*for Computational Linguistics*, 2023. 6<br><br>[54] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu. Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024. 2, 5, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2023. 2<br><br>[20] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and\nJifeng Dai. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. *arXiv*\n*preprint arXiv:2312.14238*, 2023. 2, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2023. 2<br><br>[20] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and\nJifeng Dai. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. *arXiv*\n*preprint arXiv:2312.14238*, 2023. 2, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**References**<br><br>[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. Nocaps: Novel object\ncaptioning at scale. In*Proceedings of the IEEE/CVF inter-*\n*national conference on computer vision*, pages 8948\u20138957,\n2019. 6<br><br>[2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,\nGe Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jian-\nqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu,\nShawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen\nXie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yux-\nuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi:\nOpen foundation models by 01.ai, 2024. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**References**<br><br>[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. Nocaps: Novel object\ncaptioning at scale. In*Proceedings of the IEEE/CVF inter-*\n*national conference on computer vision*, pages 8948\u20138957,\n2019. 6<br><br>[2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,\nGe Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jian-\nqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu,\nShawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen\nXie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yux-\nuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi:\nOpen foundation models by 01.ai, 2024. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Table 3. **Comparison with closed-source APIs and previous open-source SOTAs. **Our InternLM-XComposer2-4KHD gets SOTA\nresults in 6 of the 16 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs. The best results\nare**bold**and the second-best results are underlined.<br><br><table border=\"1\">\n<tr><th>Method</th><th>LLM</th><th>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</th></tr>\n<tr><td>Method</td><td>LLM</td><td>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Table 3. **Comparison with closed-source APIs and previous open-source SOTAs. **Our InternLM-XComposer2-4KHD gets SOTA\nresults in 6 of the 16 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs. The best results\nare**bold**and the second-best results are underlined.<br><br><table border=\"1\">\n<tr><th>Method</th><th>LLM</th><th>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</th></tr>\n<tr><td>Method</td><td>LLM</td><td>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[99] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\nNi, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\nSun, and Gao Huang. Llava-uhd: an lmm perceiving any\naspect ratio and high-resolution images. *arXiv preprint*\n*arXiv:2403.11703*, 2024. 2, 5, 8<br><br>[100] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Jun-\nfeng Tian, et al. mPLUG-DocOwl: Modularized multi-\nmodal large language model for document understanding. *arXiv preprint arXiv:2307.02499*, 2023. 5<br><br>[101] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming\nYan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji\nZhang, et al. Ureader: Universal ocr-free visually-situated\nlanguage understanding with multimodal large language\nmodel. *arXiv preprint arXiv:2310.05126*, 2023. 5, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[99] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\nNi, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\nSun, and Gao Huang. Llava-uhd: an lmm perceiving any\naspect ratio and high-resolution images. *arXiv preprint*\n*arXiv:2403.11703*, 2024. 2, 5, 8<br><br>[100] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Jun-\nfeng Tian, et al. mPLUG-DocOwl: Modularized multi-\nmodal large language model for document understanding. *arXiv preprint arXiv:2307.02499*, 2023. 5<br><br>[101] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming\nYan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji\nZhang, et al. Ureader: Universal ocr-free visually-situated\nlanguage understanding with multimodal large language\nmodel. *arXiv preprint arXiv:2310.05126*, 2023. 5, 8",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[77] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world. *arXiv.org*,\n2023. 2<br><br>[78] Qwen. Introducing qwen-7b: Open foundation and human-\naligned models (of the state-of-the-arts), 2023. 1, 2<br><br>[79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[77] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world. *arXiv.org*,\n2023. 2<br><br>[78] Qwen. Introducing qwen-7b: Open foundation and human-\naligned models (of the state-of-the-arts), 2023. 1, 2<br><br>[79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,\nYang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia\nGuo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang,\nTao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiax-\ning Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\ning Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-\nwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun\nLv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang\nNing, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,\nYunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,\nYang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia\nGuo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang,\nTao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiax-\ning Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\ning Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-\nwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun\nLv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang\nNing, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,\nYunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%*chatgpt quality, March 2023. 1, 6<br><br>[22] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\nNg, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\nZhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\ning challenge on arbitrary-shaped text-rrc-art. In*2019 In-*\n*ternational Conference on Document Analysis and Recog-*\n*nition (ICDAR)*, pages 1571\u20131576. IEEE, 2019. 6<br><br>[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. *arXiv.org*, 2022. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%*chatgpt quality, March 2023. 1, 6<br><br>[22] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\nNg, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\nZhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\ning challenge on arbitrary-shaped text-rrc-art. In*2019 In-*\n*ternational Conference on Document Analysis and Recog-*\n*nition (ICDAR)*, pages 1571\u20131576. IEEE, 2019. 6<br><br>[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. *arXiv.org*, 2022. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Train Eval</th><th>Doc Info Text Chart MMB MME SEED\u2217</th></tr>\n<tr><td>Train Eval</td><td>Doc Info Text Chart MMB MME SEED\u2217</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD9\nHD9\nHD16</th><th>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</th></tr>\n<tr><td>HD9\nHD9\nHD16</td><td>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD16\nHD16\nHD25</th><th>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</th></tr>\n<tr><td>HD16\nHD16\nHD25</td><td>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Train Eval</th><th>Doc Info Text Chart MMB MME SEED\u2217</th></tr>\n<tr><td>Train Eval</td><td>Doc Info Text Chart MMB MME SEED\u2217</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD9\nHD9\nHD16</th><th>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</th></tr>\n<tr><td>HD9\nHD9\nHD16</td><td>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD16\nHD16\nHD25</th><th>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</th></tr>\n<tr><td>HD16\nHD16\nHD25</td><td>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Gemini-Pro</th><th>88.1</th><th>74.1</th><th>75.2</th><th>74.6</th><th>68.0</th><th>42.6</th><th>45.8</th><th>70.2</th><th>47.9</th><th>1,933.3</th><th>73.6</th><th>74.3</th><th>70.7</th><th>70.6</th><th>59.2</th><th>45.2</th></tr>\n<tr><td>IXC2-VL</td><td>57.7</td><td>72.6</td><td>34.4</td><td>70.1</td><td>53.2</td><td>55.4</td><td>57.6</td><td>81.2</td><td>41.4</td><td>2,220.4</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td><td>41.0</td></tr>\n<tr><td>IXC2-4KHD</td><td>90.0</td><td>81.0</td><td>68.6</td><td>77.2</td><td>67.5</td><td>54.1</td><td>57.8</td><td>80.9</td><td>39.7</td><td>2,204.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td><td>40.9</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Gemini-Pro</th><th>88.1</th><th>74.1</th><th>75.2</th><th>74.6</th><th>68.0</th><th>42.6</th><th>45.8</th><th>70.2</th><th>47.9</th><th>1,933.3</th><th>73.6</th><th>74.3</th><th>70.7</th><th>70.6</th><th>59.2</th><th>45.2</th></tr>\n<tr><td>IXC2-VL</td><td>57.7</td><td>72.6</td><td>34.4</td><td>70.1</td><td>53.2</td><td>55.4</td><td>57.6</td><td>81.2</td><td>41.4</td><td>2,220.4</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td><td>41.0</td></tr>\n<tr><td>IXC2-4KHD</td><td>90.0</td><td>81.0</td><td>68.6</td><td>77.2</td><td>67.5</td><td>54.1</td><td>57.8</td><td>80.9</td><td>39.7</td><td>2,204.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td><td>40.9</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[16] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali-x: On scaling up a multilingual vision\nand language model, 2023. 2<br><br>[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server, 2015. 1, 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[16] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali-x: On scaling up a multilingual vision\nand language model, 2023. 2<br><br>[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server, 2015. 1, 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "multi-modal benchmarks. This. project\nis available at\nhttps: //ShareGPT4V. github. io to serve asa",
            {
                "chunk_size": "small"
            }
        ],
        [
            "multi-modal benchmarks. This. project\nis available at\nhttps: //ShareGPT4V. github. io to serve asa",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "& shareGPTWV: Improving Large Mul-Modal Me with BeterCaptions\nsca onme ee\nnye\nnc gg\naa aay<br><br>Zz\ni\n\u2014\u2014\u2014 ee\nS\nES\nSSeeeass) 4oes<br><br>5 spmamilcamees ene\nvoemooee",
            {
                "chunk_size": "small"
            }
        ],
        [
            "& shareGPTWV: Improving Large Mul-Modal Me with BeterCaptions\nsca onme ee\nnye\nnc gg\naa aay<br><br>Zz\ni\n\u2014\u2014\u2014 ee\nS\nES\nSSeeeass) 4oes<br><br>5 spmamilcamees ene\nvoemooee",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "up to 4K resolution, the model can achieve additional\nperformance improvements during inference by processing\nimages at higher resolutions. Furthermore, scaling the training resolution up to 4K\nstandard results in a consistent improvement in perfor-\nmance, highlighting the potential for training even beyond\n4K resolution. This underscores the capacity for further en-\nhancing model capabilities and suggests a promising tra-\njectory for advancing the frontiers of high-resolution image\nprocessing within the domain of large vision-language mod-\nels. We evaluate our InternLM-XComposer2-4KHD on\n16 diverse benchmarks spanning various domains, in-\ncluding 5 challenging HD-OCR datasets (DocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87] and\nOCRBench[58]). Compared to previous open-source\nLVLM models and closed-source APIs, our approach\nachieves SOTA results in 6 of 16 benchmarks, demon-\nstrating competitive performance despite only 7B parame-\nters. As shown in Figure 1, InternLM-XComposer2-4KHD\neven surpasses the performance of GPT4V [74] and Gemini\nPro [90] across ten benchmarks. Notably, our method ex-\nhibits excellent performance on 5 HD-OCR datasets, over\nexisting open-source LVLMs by a substantial margin.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "up to 4K resolution, the model can achieve additional\nperformance improvements during inference by processing\nimages at higher resolutions. Furthermore, scaling the training resolution up to 4K\nstandard results in a consistent improvement in perfor-\nmance, highlighting the potential for training even beyond\n4K resolution. This underscores the capacity for further en-\nhancing model capabilities and suggests a promising tra-\njectory for advancing the frontiers of high-resolution image\nprocessing within the domain of large vision-language mod-\nels. We evaluate our InternLM-XComposer2-4KHD on\n16 diverse benchmarks spanning various domains, in-\ncluding 5 challenging HD-OCR datasets (DocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87] and\nOCRBench[58]). Compared to previous open-source\nLVLM models and closed-source APIs, our approach\nachieves SOTA results in 6 of 16 benchmarks, demon-\nstrating competitive performance despite only 7B parame-\nters. As shown in Figure 1, InternLM-XComposer2-4KHD\neven surpasses the performance of GPT4V [74] and Gemini\nPro [90] across ten benchmarks. Notably, our method ex-\nhibits excellent performance on 5 HD-OCR datasets, over\nexisting open-source LVLMs by a substantial margin.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "\u2019 Recovery\n&\ntime<br><br>Finally, the infographic emphasizes the \nimportance of recovery time, as contact displays \nno symptoms of coronavirus (COVID-19) the \npatient makes a full recovery, they can go back \nto daily lives as normal and pose no risk to \nothers. The HD image (986 x 2560)<br><br>@\nContact displays no symptoms\nof coronavirus (COVID-19)\nthe patient makesa full recovery,\nthey can go back to daily lives as\nnormal and pose no risk to others",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u2019 Recovery\n&\ntime<br><br>Finally, the infographic emphasizes the \nimportance of recovery time, as contact displays \nno symptoms of coronavirus (COVID-19) the \npatient makes a full recovery, they can go back \nto daily lives as normal and pose no risk to \nothers. The HD image (986 x 2560)<br><br>@\nContact displays no symptoms\nof coronavirus (COVID-19)\nthe patient makesa full recovery,\nthey can go back to daily lives as\nnormal and pose no risk to others",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "DocVQA\n**Abstract** OCRBench ChartQA<br><br>81.0\n90.0 67.5\nMathVista TextVQA<br><br>57.8\n77.2<br><br>AI2D InfoVQA\n80.9\n68.6<br><br>2204.9 74.7\nMME\nSEEDI<br><br>71.8\n77.7<br><br>QBenchT MMBenchCN\n54.9 39.9<br><br>40.9 80.2<br><br>MM-Vet MMBench\n54.1<br><br>HallB MMMU\nMMStar<br><br>GPT-4V\nGemini-Pro\nIXC2-4KHD\nPrevious open-source SOTA",
            {
                "chunk_size": "small"
            }
        ],
        [
            "DocVQA\n**Abstract** OCRBench ChartQA<br><br>81.0\n90.0 67.5\nMathVista TextVQA<br><br>57.8\n77.2<br><br>AI2D InfoVQA\n80.9\n68.6<br><br>2204.9 74.7\nMME\nSEEDI<br><br>71.8\n77.7<br><br>QBenchT MMBenchCN\n54.9 39.9<br><br>40.9 80.2<br><br>MM-Vet MMBench\n54.1<br><br>HallB MMMU\nMMStar<br><br>GPT-4V\nGemini-Pro\nIXC2-4KHD\nPrevious open-source SOTA",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "a>\nGive an in-depth description of the visual.<br><br>7)<br><br>The image is an infographic from Public Health \nEngland that explains the concept of contact \ntracing in the context of preventing the spread of \ncoronavirus (COVID-19). Contact tracing is a \nfundamental part of outbreak control used by \npublic health professionals around the world to \nprevent the spread of infections.<br><br>The infographic is divided into several sections, \neach providing a different aspect of the contact \ntracing process. The first section explains that a \ncontact is assessed to see how close they were to \na confirmed case, which could include someone \nliving with the case, or someone who has been \nin direct contact with them or their bodily fluids. Following this assessment, the contacts are \ncategorised into low or high risk and provided \nwith advice on what they should do.<br><br>The second section explains that if a contact is at \nhigh risk of infection, they may be asked to self-\nisolate, remaining in their home for 14 days and \nstaying away from work, school or public \nplaces. If a contact is at low risk, they don't \nrequire self-isolation.<br><br>The third section explains that if the person \ndevelops symptoms, they would be tested and \nprovided with specialist care if they have the \nnovel coronavirus (COVID-19).",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nGive an in-depth description of the visual.<br><br>7)<br><br>The image is an infographic from Public Health \nEngland that explains the concept of contact \ntracing in the context of preventing the spread of \ncoronavirus (COVID-19). Contact tracing is a \nfundamental part of outbreak control used by \npublic health professionals around the world to \nprevent the spread of infections.<br><br>The infographic is divided into several sections, \neach providing a different aspect of the contact \ntracing process. The first section explains that a \ncontact is assessed to see how close they were to \na confirmed case, which could include someone \nliving with the case, or someone who has been \nin direct contact with them or their bodily fluids. Following this assessment, the contacts are \ncategorised into low or high risk and provided \nwith advice on what they should do.<br><br>The second section explains that if a contact is at \nhigh risk of infection, they may be asked to self-\nisolate, remaining in their home for 14 days and \nstaying away from work, school or public \nplaces. If a contact is at low risk, they don't \nrequire self-isolation.<br><br>The third section explains that if the person \ndevelops symptoms, they would be tested and \nprovided with specialist care if they have the \nnovel coronavirus (COVID-19).",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Table 5. **High-resolution Evaluation. **IntenrLM-XComposer2-4KHD has the largest input resolution and outperforms open-source\nLVLMs which are specifically tuned for document understanding.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Table 5. **High-resolution Evaluation. **IntenrLM-XComposer2-4KHD has the largest input resolution and outperforms open-source\nLVLMs which are specifically tuned for document understanding.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "=\nIllustrate the fine details present in the image.<br><br>*<br><br>The image is a vibrant and colorful infographic \nthat provides an overview of 7 graphic design \ntrends that are expected to dominate in 2021. The \ninfographic is divided into 7 sections, each \nrepresenting a different trend.<br><br>At the top, there's a section titled \"Muted Color \nPalettes\", which introduces the concept of muted \ncolors as a trend for 2021. <br><br>This is followed by a section on \"Simple Data \nVisualizations\", highlighting the goal of making \ncomplex data easy to understand.<br><br>The third section focuses on \"Geometric Shapes \nEverywhere\", showcasing how designers are \nusing geometric shapes in their designs. <br><br>The fourth section delves into \"Flat Icons and \nIllustrations\", explaining how brands are using \nflat icons and illustrations in their social media \ngraphics, website design, and more.<br><br>The fifth section introduces \"Classic Serif Fonts\", \ndiscussing how these fonts are seen as classic, \nelegant, and trustworthy. <br><br>The sixth section explores \"Social Media Slide \nDecks\", illustrating how slide decks have become \na visual way of communicating longer messages \non Instagram and LinkedIn.<br><br>Finally, the seventh section discusses \"Text \nHeavy Videos\", predicting that people will likely \ncontinue to work remotely well into 2021, leading \nto an increase in video content that uses text on-\nscreen to communicate messages.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "=\nIllustrate the fine details present in the image.<br><br>*<br><br>The image is a vibrant and colorful infographic \nthat provides an overview of 7 graphic design \ntrends that are expected to dominate in 2021. The \ninfographic is divided into 7 sections, each \nrepresenting a different trend.<br><br>At the top, there's a section titled \"Muted Color \nPalettes\", which introduces the concept of muted \ncolors as a trend for 2021. <br><br>This is followed by a section on \"Simple Data \nVisualizations\", highlighting the goal of making \ncomplex data easy to understand.<br><br>The third section focuses on \"Geometric Shapes \nEverywhere\", showcasing how designers are \nusing geometric shapes in their designs. <br><br>The fourth section delves into \"Flat Icons and \nIllustrations\", explaining how brands are using \nflat icons and illustrations in their social media \ngraphics, website design, and more.<br><br>The fifth section introduces \"Classic Serif Fonts\", \ndiscussing how these fonts are seen as classic, \nelegant, and trustworthy. <br><br>The sixth section explores \"Social Media Slide \nDecks\", illustrating how slide decks have become \na visual way of communicating longer messages \non Instagram and LinkedIn.<br><br>Finally, the seventh section discusses \"Text \nHeavy Videos\", predicting that people will likely \ncontinue to work remotely well into 2021, leading \nto an increase in video content that uses text on-\nscreen to communicate messages.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "**InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model**\n**Handling Resolutions from 336 Pixels to 4K HD**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model**\n**Handling Resolutions from 336 Pixels to 4K HD**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[46] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang,\nFanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-\nmodality model, 2023. 2, 5<br><br>[47] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. *arXiv.org*, 2023. 2<br><br>[48] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-Gemini: Mining the potential of multi-modality\nvision language models. *arXiv preprint arXiv:2403.18814*,\n2024. 2, 5<br><br>[49] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam\nKortylewski, Wufei Ma, Benjamin Van Durme, and Alan L\nYuille. Super-clevr: A virtual benchmark to diagnose do-\nmain robustness in visual reasoning. In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963\u201314973, 2023. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963\u201314973, 2023. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[46] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang,\nFanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-\nmodality model, 2023. 2, 5<br><br>[47] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. *arXiv.org*, 2023. 2<br><br>[48] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-Gemini: Mining the potential of multi-modality\nvision language models. *arXiv preprint arXiv:2403.18814*,\n2024. 2, 5<br><br>[49] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam\nKortylewski, Wufei Ma, Benjamin Van Durme, and Alan L\nYuille. Super-clevr: A virtual benchmark to diagnose do-\nmain robustness in visual reasoning. In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963\u201314973, 2023. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[68] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. *arXiv preprint arXiv:2203.10244*, 2022. 1, 2, 6, 7<br><br>[69] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In*Proceedings of the IEEE/CVF Winter Conference on Ap-*\n*plications of Computer Vision*, pages 1697\u20131706, 2022. 1,\n2, 6, 7<br><br>[70] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In*Pro-*\n*ceedings of the IEEE/CVF winter conference on applica-*\n*tions of computer vision*, pages 2200\u20132209, 2021. 1, 2, 6,\n7<br><br>[71] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024. 9",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024. 9",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[68] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. *arXiv preprint arXiv:2203.10244*, 2022. 1, 2, 6, 7<br><br>[69] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In*Proceedings of the IEEE/CVF Winter Conference on Ap-*\n*plications of Computer Vision*, pages 1697\u20131706, 2022. 1,\n2, 6, 7<br><br>[70] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In*Pro-*\n*ceedings of the IEEE/CVF winter conference on applica-*\n*tions of computer vision*, pages 2200\u20132209, 2021. 1, 2, 6,\n7<br><br>[71] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024. 9",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "=<br><br>If the person does\n|(@)a\nwe would test\nanaes\n\u2014\nthem and provide them with\n=== Gas\n|\nspecialist care if they have\nwe O|e=\n|\nthe novel coronavirus\n\u2014\u2014\u2014_\n(Cow\noch ea",
            {
                "chunk_size": "small"
            }
        ],
        [
            "=<br><br>If the person does\n|(@)a\nwe would test\nanaes\n\u2014\nthem and provide them with\n=== Gas\n|\nspecialist care if they have\nwe O|e=\n|\nthe novel coronavirus\n\u2014\u2014\u2014_\n(Cow\noch ea",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "*The Large Vision-Language Model (LVLM) field has*\n*seen significant advancements, yet its progression has been*\n*hindered by challenges in comprehending fine-grained vi-*\n*sual content due to limited resolution. Recent efforts have*\n*aimed to enhance the high-resolution understanding ca-*\n*pabilities of LVLMs, yet they remain capped at approxi-*\n*mately 1500 \u00d7 1500 pixels and constrained to a relatively*\n*narrow resolution range. This paper represents InternLM-*\n*XComposer2-4KHD, a groundbreaking exploration into el-*\n*evating LVLM resolution capabilities up to 4K HD (3840*\n*\u00d7 1600) and beyond. Concurrently, considering the ultra-*\n*high resolution may not be necessary in all scenarios, it*\n*supports a wide range of diverse resolutions from 336 pix-*\n*els to 4K standard, significantly broadening its scope of ap-*\n*plicability. Specifically, this research advances the patch*\n*division paradigm by introducing a novel extension: dy-*\n*namic resolution with automatic patch configuration. *\n*It*\n*maintains the training image aspect ratios while automati-*\n*cally varying patch counts and configuring layouts based on*\n*a pre-trained Vision Transformer (ViT) (336 \u00d7 336), lead-*\n*ing to dynamic training resolution from 336 pixels to 4K*\n*standard. *\n*Our research demonstrates that scaling train-*\n*ing resolution up to 4K HD leads to consistent perfor-*\n*mance enhancements without hitting the ceiling of poten-*\n*tial improvements. *\n*InternLM-XComposer2-4KHD shows*\n*superb capability that matches or even surpasses GPT-*\n*4V and Gemini Pro in 10 of the 16 benchmarks. *\n*The*\n*InternLM-XComposer2-4KHD model series with 7B pa-*\n*rameters are publicly available at https://github. *\n*com/InternLM/InternLM-XComposer. *<br><br>*indicates equal contribution.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*<br><br>*indicates equal contribution.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*The Large Vision-Language Model (LVLM) field has*\n*seen significant advancements, yet its progression has been*\n*hindered by challenges in comprehending fine-grained vi-*\n*sual content due to limited resolution. Recent efforts have*\n*aimed to enhance the high-resolution understanding ca-*\n*pabilities of LVLMs, yet they remain capped at approxi-*\n*mately 1500 \u00d7 1500 pixels and constrained to a relatively*\n*narrow resolution range. This paper represents InternLM-*\n*XComposer2-4KHD, a groundbreaking exploration into el-*\n*evating LVLM resolution capabilities up to 4K HD (3840*\n*\u00d7 1600) and beyond. Concurrently, considering the ultra-*\n*high resolution may not be necessary in all scenarios, it*\n*supports a wide range of diverse resolutions from 336 pix-*\n*els to 4K standard, significantly broadening its scope of ap-*\n*plicability. Specifically, this research advances the patch*\n*division paradigm by introducing a novel extension: dy-*\n*namic resolution with automatic patch configuration. *\n*It*\n*maintains the training image aspect ratios while automati-*\n*cally varying patch counts and configuring layouts based on*\n*a pre-trained Vision Transformer (ViT) (336 \u00d7 336), lead-*\n*ing to dynamic training resolution from 336 pixels to 4K*\n*standard. *\n*Our research demonstrates that scaling train-*\n*ing resolution up to 4K HD leads to consistent perfor-*\n*mance enhancements without hitting the ceiling of poten-*\n*tial improvements. *\n*InternLM-XComposer2-4KHD shows*\n*superb capability that matches or even surpasses GPT-*\n*4V and Gemini Pro in 10 of the 16 benchmarks. *\n*The*\n*InternLM-XComposer2-4KHD model series with 7B pa-*\n*rameters are publicly available at https://github. *\n*com/InternLM/InternLM-XComposer. *<br><br>*indicates equal contribution.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[95] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models, 2023. 2, 7<br><br>[96] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and\nHouqiang Li. Towards improving document understanding:\nAn exploration on text-grounding via mllms. *arXiv preprint*\n*arXiv:2311.13194*, 2023. 5<br><br>[97] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang. Vary:\nScaling up the vision vocab-\nulary for large vision-language models. *arXiv preprint*\n*arXiv:2312.06109*, 2023. 2, 5<br><br>[98] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, et al. Q-bench: A benchmark for\ngeneral-purpose foundation models on low-level vision. *arXiv preprint arXiv:2309.14181*, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*arXiv preprint arXiv:2309.14181*, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[95] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models, 2023. 2, 7<br><br>[96] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and\nHouqiang Li. Towards improving document understanding:\nAn exploration on text-grounding via mllms. *arXiv preprint*\n*arXiv:2311.13194*, 2023. 5<br><br>[97] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang. Vary:\nScaling up the vision vocab-\nulary for large vision-language models. *arXiv preprint*\n*arXiv:2312.06109*, 2023. 2, 5<br><br>[98] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, et al. Q-bench: A benchmark for\ngeneral-purpose foundation models on low-level vision. *arXiv preprint arXiv:2309.14181*, 2023. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Open-Source\nPrevious SOTA</th><th>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</th></tr>\n<tr><td>Open-Source\nPrevious SOTA</td><td>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Open-Source\nPrevious SOTA</th><th>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</th></tr>\n<tr><td>Open-Source\nPrevious SOTA</td><td>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Table 4. **Comparison with open-source SOTA methods. **IXC2-4KHD outperforms competitors in most benchmarks. The best results\nare**bold**and the second-best results are underlined.<br><br>setting yields better results on most OCR-related tasks when\nthe LVLM is trained under the \u2018HD25\u2019 setting. In practice, we jointly train all the components with a\nbatch size of 2048 over 3500 steps. Data from multiple\nsources are sampled in a weighted manner, with the weights\nbased on the number of data from each source. As the \u2018HD-\n55\u2019 setting has double image tokens than the \u2018HD-25\u2019, we\nadjust the data loader to enable different batch sizes for\nthem and adjust their weight accordingly. The maximum\nlearning rate is set to 5*\u00d7*10*\u2212*5, and each component has its\nown unique learning strategy. For the vision encoder, we set\nthe LLDR to 0*. *9, which aligns with the pretraining strategy. For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Table 4. **Comparison with open-source SOTA methods. **IXC2-4KHD outperforms competitors in most benchmarks. The best results\nare**bold**and the second-best results are underlined.<br><br>setting yields better results on most OCR-related tasks when\nthe LVLM is trained under the \u2018HD25\u2019 setting. In practice, we jointly train all the components with a\nbatch size of 2048 over 3500 steps. Data from multiple\nsources are sampled in a weighted manner, with the weights\nbased on the number of data from each source. As the \u2018HD-\n55\u2019 setting has double image tokens than the \u2018HD-25\u2019, we\nadjust the data loader to enable different batch sizes for\nthem and adjust their weight accordingly. The maximum\nlearning rate is set to 5*\u00d7*10*\u2212*5, and each component has its\nown unique learning strategy. For the vision encoder, we set\nthe LLDR to 0*. *9, which aligns with the pretraining strategy. For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables each may\nreduce your risk for cardiovascular disease, including heart attack and stroke. Examples of a single serving of fruit:\nExamples of a single serving of vegetables:\n\u00a9\n1 medium apple\n\u00a9\n5-8 broccoli florets\n\u00a9\n1small banana\n+\nYa\nlarge sweet potato\n*% medium avocado\n\u00a9\n6 baby carrots\n\u00ab\u00a9 large strawberries\n\u00a9\n1 cup of raw spinach",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables each may\nreduce your risk for cardiovascular disease, including heart attack and stroke. Examples of a single serving of fruit:\nExamples of a single serving of vegetables:\n\u00a9\n1 medium apple\n\u00a9\n5-8 broccoli florets\n\u00a9\n1small banana\n+\nYa\nlarge sweet potato\n*% medium avocado\n\u00a9\n6 baby carrots\n\u00ab\u00a9 large strawberries\n\u00a9\n1 cup of raw spinach",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Method</th><th>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</th></tr>\n<tr><td>Method</td><td>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Method</th><th>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</th></tr>\n<tr><td>Method</td><td>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "RA a\nos\n@)<br><br>5= 9)<br><br>a\n:\nfe >",
            {
                "chunk_size": "small"
            }
        ],
        [
            "RA a\nos\n@)<br><br>5= 9)<br><br>a\n:\nfe >",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Operations Cost by Department by Month<br><br>160K<br><br>120K",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Operations Cost by Department by Month<br><br>160K<br><br>120K",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "Model\nModel Size\nMax Resolution\nDocVQA*T est*\nChartQA*T est*\nInfoVQA*T est*\nTextVQA*V al*\nOCRBench<br><br>TextMonkey[59]\n9B\n896x896\n73.0\n66.9\n28.6\n65.6\n55.8\nLLaVA-UHD [99]\n13B\n1008x672\n\u2014\n\u2014\n\u2014\n67.7\n\u2014\nCogAgent [36]\n17B\n1024x1024\n81.6\n68.4\n44.5\n76.1\n59.0\nUReader [101]\n7B\n2240x2240\n65.4\n59.3\n42.2\n57.6\n\u2014\nDocOwl 1.5 [37]\n8B\n1344x1344\n82.2\n70.2\n50.7\n68.6\n\u2014<br><br><table border=\"1\">\n<tr><th>IXC2-4KHD</th><th>8B</th><th>3840x1600</th><th>90.0 (+7.8)</th><th>81.0 (+10.8)</th><th>68.6 (+17.9)</th><th>77.2 (+1.2)</th><th>67.5 (+8.5)</th></tr>\n<tr><td>IXC2-4KHD</td><td>8B</td><td>3840x1600</td><td>90.0 (+7.8)</td><td>81.0 (+10.8)</td><td>68.6 (+17.9)</td><td>77.2 (+1.2)</td><td>67.5 (+8.5)</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Model\nModel Size\nMax Resolution\nDocVQA*T est*\nChartQA*T est*\nInfoVQA*T est*\nTextVQA*V al*\nOCRBench<br><br>TextMonkey[59]\n9B\n896x896\n73.0\n66.9\n28.6\n65.6\n55.8\nLLaVA-UHD [99]\n13B\n1008x672\n\u2014\n\u2014\n\u2014\n67.7\n\u2014\nCogAgent [36]\n17B\n1024x1024\n81.6\n68.4\n44.5\n76.1\n59.0\nUReader [101]\n7B\n2240x2240\n65.4\n59.3\n42.2\n57.6\n\u2014\nDocOwl 1.5 [37]\n8B\n1344x1344\n82.2\n70.2\n50.7\n68.6\n\u2014<br><br><table border=\"1\">\n<tr><th>IXC2-4KHD</th><th>8B</th><th>3840x1600</th><th>90.0 (+7.8)</th><th>81.0 (+10.8)</th><th>68.6 (+17.9)</th><th>77.2 (+1.2)</th><th>67.5 (+8.5)</th></tr>\n<tr><td>IXC2-4KHD</td><td>8B</td><td>3840x1600</td><td>90.0 (+7.8)</td><td>81.0 (+10.8)</td><td>68.6 (+17.9)</td><td>77.2 (+1.2)</td><td>67.5 (+8.5)</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[6] Baichuan. Baichuan 2: Open large-scale language models. *arXiv.org*, 2023. 2<br><br>[7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa \u0306gnak Tas \u0327\u0131rlar. Introducing our multimodal models, 2023. 2<br><br>[8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc \u0327al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\n*Proceedings of the IEEE/CVF international conference on*\n*computer vision*, pages 4291\u20134301, 2019. 6<br><br>[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877\u2013\n1901, 2020. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877\u2013\n1901, 2020. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[6] Baichuan. Baichuan 2: Open large-scale language models. *arXiv.org*, 2023. 2<br><br>[7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa \u0306gnak Tas \u0327\u0131rlar. Introducing our multimodal models, 2023. 2<br><br>[8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc \u0327al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\n*Proceedings of the IEEE/CVF international conference on*\n*computer vision*, pages 4291\u20134301, 2019. 6<br><br>[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877\u2013\n1901, 2020. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. *arXiv.org*, 2023. 6<br><br>[57] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player? *arXiv:2307.06281*, 2023. 1, 5, 7<br><br>[58] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\nYin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the\nhidden mystery of ocr in large multimodal models, 2024. 2,\n7<br><br>[59] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,\nShuo Zhang, and Xiang Bai. Textmonkey: An ocr-free\nlarge multimodal model for understanding document. *arXiv*\n*preprint arXiv:2403.04473*, 2024. 2, 5, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*arXiv*\n*preprint arXiv:2403.04473*, 2024. 2, 5, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. *arXiv.org*, 2023. 6<br><br>[57] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player? *arXiv:2307.06281*, 2023. 1, 5, 7<br><br>[58] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\nYin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the\nhidden mystery of ocr in large multimodal models, 2024. 2,\n7<br><br>[59] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,\nShuo Zhang, and Xiang Bai. Textmonkey: An ocr-free\nlarge multimodal model for understanding document. *arXiv*\n*preprint arXiv:2403.04473*, 2024. 2, 5, 8",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1732288813
    },
    "rets": [
        [
            "[108] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\nZhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130b: An open bilingual pre-\ntrained model. In*The Eleventh International Conference*\n*on Learning Representations (ICLR)*, 2023. 2<br><br>[109] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang,\nand Jiaqi Wang. Long-CLIP: Unlocking the long-text capa-\nbility of clip. *arXiv preprint arXiv:2403.15378*, 2024. 2<br><br>[110] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al. Internlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. *arXiv preprint*\n*arXiv:2309.15112*, 2023. 2, 6, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[108] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\nZhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130b: An open bilingual pre-\ntrained model. In*The Eleventh International Conference*\n*on Learning Representations (ICLR)*, 2023. 2<br><br>[109] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang,\nand Jiaqi Wang. Long-CLIP: Unlocking the long-text capa-\nbility of clip. *arXiv preprint arXiv:2403.15378*, 2024. 2<br><br>[110] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al. Internlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. *arXiv preprint*\n*arXiv:2309.15112*, 2023. 2, 6, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "inal CLIP visual encoder. Similarly, CogAgent [36] and\nMini-Gemini [48] also separate HR and LR images using\ndistinct vision encoders, subsequently merging their fea-\ntures using a cross-attention module. In contrast, our ap-\nproach offers a more simplified solution and shows advan-\ntages for varying resolutions and aspect ratio inputs. (2)\nCropped image patches [37, 46, 50, 51, 59, 99, 101]. For\nexample, Monkey [50] employs sliding windows to seg-\nment images into patches, subsequently processing them\nwith LoRA fine-tuning. TextMonkey [59] further proposes\nshifted window attention and token resampler to consider\nthe connections among different patches. These approaches\nare confined to either a few predefined high-resolution set-\ntings [36, 46, 48, 50, 51, 55, 59, 66, 97] or a limited range of\nresolutions [37, 99]. Conversely, our method devises a dy-\nnamic image partition strategy to support the scaling from\n336 pixels to 4K resolution, and the maximum resolution is\nlarger than previous approaches (*e.g*., 1.5k for Monkey [50]\nand 2k for UReader [101]).",
            {
                "chunk_size": "small"
            }
        ],
        [
            "inal CLIP visual encoder. Similarly, CogAgent [36] and\nMini-Gemini [48] also separate HR and LR images using\ndistinct vision encoders, subsequently merging their fea-\ntures using a cross-attention module. In contrast, our ap-\nproach offers a more simplified solution and shows advan-\ntages for varying resolutions and aspect ratio inputs. (2)\nCropped image patches [37, 46, 50, 51, 59, 99, 101]. For\nexample, Monkey [50] employs sliding windows to seg-\nment images into patches, subsequently processing them\nwith LoRA fine-tuning. TextMonkey [59] further proposes\nshifted window attention and token resampler to consider\nthe connections among different patches. These approaches\nare confined to either a few predefined high-resolution set-\ntings [36, 46, 48, 50, 51, 55, 59, 66, 97] or a limited range of\nresolutions [37, 99]. Conversely, our method devises a dy-\nnamic image partition strategy to support the scaling from\n336 pixels to 4K resolution, and the maximum resolution is\nlarger than previous approaches (*e.g*., 1.5k for Monkey [50]\nand 2k for UReader [101]).",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,\nJiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi\nWang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong\nXiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,\nXiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing\nYu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,\nPeng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\nWenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue\nZhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe\nZhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng\nQiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024. 1, 2<br><br>[11] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Ji-\naqi Wang. DualFocus: Integrating macro and micro per-\nspectives in multi-modal large language models. *arXiv*\n*preprint arXiv:2402.14767*, 2024. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Sun, Yu Sun, Huanze Tang, Bin Wang, Guoteng Wang,\nJiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi\nWang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong\nXiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan,\nXiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing\nYu, Yuhang Zang, Chuyu Zhang, Li Zhang, Pan Zhang,\nPeng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang,\nWenjian Zhang, Wenwei Zhang, Xingcheng Zhang, Xinyue\nZhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe\nZhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng\nQiu, Yu Qiao, and Dahua Lin. Internlm2 technical report. *arXiv preprint arXiv:2403.17297*, 2024. 1, 2<br><br>[11] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Ji-\naqi Wang. DualFocus: Integrating macro and micro per-\nspectives in multi-modal large language models. *arXiv*\n*preprint arXiv:2402.14767*, 2024. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "90\n85\n80\n75\n70\n65\n60\n55\n50\n*\n*Val*\n*Val*\n*Val*\n*Test*\n*Test*\n*EN-Test*\nInfoVQA\nDocVQA\nTextVQA\nChartQA\nMMBench\nMME\nSeed\nAI2D\nHD-9 (1561 Tokens)\nHD-16 (2653 Tokens)\nHD-25 (4057 Tokens)\n4K HD (8737 Tokens)<br><br>Figure 5. **Influence of Training Resolution. **High-resolution training is critical for HD-OCR tasks, while its gain on other tasks is minor.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "90\n85\n80\n75\n70\n65\n60\n55\n50\n*\n*Val*\n*Val*\n*Val*\n*Test*\n*Test*\n*EN-Test*\nInfoVQA\nDocVQA\nTextVQA\nChartQA\nMMBench\nMME\nSeed\nAI2D\nHD-9 (1561 Tokens)\nHD-16 (2653 Tokens)\nHD-25 (4057 Tokens)\n4K HD (8737 Tokens)<br><br>Figure 5. **Influence of Training Resolution. **High-resolution training is critical for HD-OCR tasks, while its gain on other tasks is minor.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "here*pw*and*ph*represent the number of patches in each row\nand column, respectively. We then split the \u02c6*x*into*ph \u00d7*\n*pw*non-overlapped patches. Each patch is a small image\nwith 336*\u00d7*336 size and we treat these patches as individual\ninputs for the ViT. In the following, we use \u2018HD-*H*\u2019 to represent our high-\nresolution setting with the constraint of*H*patches. For ex-\nample, the \u2019HD-9\u2019 allows up to 9 patches, including a range\nof resolutions such as 1008*\u00d7*1008, 672*\u00d7*1344, 336*\u00d7*3024,\n*etc*. **Global-Local Format. **For each input image, we present it\nto the model with two views. The first is the global view,\nwhere the image is resized to a fixed size (in our case, 336\n\u00d7 336). This provides a macro understanding of the image. Empirically, we have found this to be crucial for the LVLM\nto correctly understand the image. The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\nGiven that an",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\nGiven that an",
            {
                "chunk_size": "small"
            }
        ],
        [
            "here*pw*and*ph*represent the number of patches in each row\nand column, respectively. We then split the \u02c6*x*into*ph \u00d7*\n*pw*non-overlapped patches. Each patch is a small image\nwith 336*\u00d7*336 size and we treat these patches as individual\ninputs for the ViT. In the following, we use \u2018HD-*H*\u2019 to represent our high-\nresolution setting with the constraint of*H*patches. For ex-\nample, the \u2019HD-9\u2019 allows up to 9 patches, including a range\nof resolutions such as 1008*\u00d7*1008, 672*\u00d7*1344, 336*\u00d7*3024,\n*etc*. **Global-Local Format. **For each input image, we present it\nto the model with two views. The first is the global view,\nwhere the image is resized to a fixed size (in our case, 336\n\u00d7 336). This provides a macro understanding of the image. Empirically, we have found this to be crucial for the LVLM\nto correctly understand the image. The second view is the\nlocal view. We divide the image into patches using the pre-\nviously mentioned Dynamic Image Partition strategy and\nextract features from each patch. Following feature extrac-\ntion, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features\nafter a straightforward token merging process. **Image 2D Structure Newline Indicator. **\nGiven that an",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "der a similar model scale. As shown in Table 4, our\nmodel significantly outperforms existing open-source mod-\nels, achieving competitive results across all benchmarks. Notably, the InternLM-XComposer2 series is the only\nmethod that achieves a higher than 50% score on the chal-\nlenging MMStar benchmark. **High-resolution Understanding Evaluation. **\nThen we\ncompare IXC2-4KHD with models that are specifically de-\nsigned for high-resolution understanding tasks. We report\nthe results of 5 high-resolution benchmarks in Table 5, as a\ngeneral LVLM, IXC2-4KHD shows superb performance on\nthese tasks and outperforms competitors with a large mar-\ngin. For example, IXC2-4KHD gets 68*. *6% on Infograph-\nicVQA, surpassing recent DocOwl 1.5 with +17*.*9%. For\nthe OCRBench, IXC2-4KHD gets 67*. *5%, outperforms Co-\ngAgent with +8*.*5%.<br><br>**4.2. Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*\u2217*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting. However, when we",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*\u2217*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting. However, when we",
            {
                "chunk_size": "small"
            }
        ],
        [
            "der a similar model scale. As shown in Table 4, our\nmodel significantly outperforms existing open-source mod-\nels, achieving competitive results across all benchmarks. Notably, the InternLM-XComposer2 series is the only\nmethod that achieves a higher than 50% score on the chal-\nlenging MMStar benchmark. **High-resolution Understanding Evaluation. **\nThen we\ncompare IXC2-4KHD with models that are specifically de-\nsigned for high-resolution understanding tasks. We report\nthe results of 5 high-resolution benchmarks in Table 5, as a\ngeneral LVLM, IXC2-4KHD shows superb performance on\nthese tasks and outperforms competitors with a large mar-\ngin. For example, IXC2-4KHD gets 68*. *6% on Infograph-\nicVQA, surpassing recent DocOwl 1.5 with +17*.*9%. For\nthe OCRBench, IXC2-4KHD gets 67*. *5%, outperforms Co-\ngAgent with +8*.*5%.<br><br>**4.2. Dive into Resolution**<br><br>**High-Resolution Training is Critical for HD-OCR tasks. **\nWe study four resolution settings: HD-9 (1561 image to-\nkens at most, we simply the statement if the following), HD-\n16 (2653 tokens), HD-25 (4057 tokens), and 4KHD (8737\ntokens). Here we report the validation set of InfoVQA,\nDocVQA, and TextVQA, test set of ChartQA and AI2D,\nMMBench EN-Test, and a 2k subset of SEEDBench (we\ndenote it as SEED*\u2217*). In the following experiments, we re-\nport results on the above benchmarks by default. As illustrated in Fig.5, we note a significant improve-\nment in the HD-OCR tasks as the resolution increases. For\ninstance, the model achieves only a 50*. *5% score on the In-\nfographicVQA with the HD-9 setting. However, when we",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "3\nFresh, frozen, canned and dried all count\n\u2018toward your daily servings, so you have plenty<br><br>of options. \u2018\n,\n9\n:\n2\nAlways reserve half of your plate for fruits\n\u2018and vegetables. 3\nChoose whole fruits and vegetables. 4 &<br><br>4\nTr tocata variety of vegetables, instead of the\n3\n\u2018same thing all the time. 2\n. 6\n4g\nLook for fruit packed in its own fruit juice and\ns\n100% fruit juice, with no added sugar. \u00a7\nLook for low/no-sodium options for canned\nvegetables, and 100% vegetable juice. &",
            {
                "chunk_size": "small"
            }
        ],
        [
            "3\nFresh, frozen, canned and dried all count\n\u2018toward your daily servings, so you have plenty<br><br>of options. \u2018\n,\n9\n:\n2\nAlways reserve half of your plate for fruits\n\u2018and vegetables. 3\nChoose whole fruits and vegetables. 4 &<br><br>4\nTr tocata variety of vegetables, instead of the\n3\n\u2018same thing all the time. 2\n. 6\n4g\nLook for fruit packed in its own fruit juice and\ns\n100% fruit juice, with no added sugar. \u00a7\nLook for low/no-sodium options for canned\nvegetables, and 100% vegetable juice. &",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[43] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are\nyou smarter than a sixth grader? textbook question answer-\ning for multimodal machine comprehension. In*Proceed-*\n*ings of the IEEE Conference on Computer Vision and Pat-*\n*tern recognition*, pages 4999\u20135007, 2017. 6<br><br>[44] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv \u0301e\nLe Borgne, Romaric Besanc \u0327on, Jos \u0301e G Moreno, and Jes \u0301us\nLov \u0301on Melgarejo. Viquae, a dataset for knowledge-based\nvisual question answering about named entities. In*Pro-*\n*ceedings of the 45th International ACM SIGIR Conference*\n*on Research and Development in Information Retrieval*,\npages 3108\u20133120, 2022. 6<br><br>[45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[43] Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk,\nJonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. Are\nyou smarter than a sixth grader? textbook question answer-\ning for multimodal machine comprehension. In*Proceed-*\n*ings of the IEEE Conference on Computer Vision and Pat-*\n*tern recognition*, pages 4999\u20135007, 2017. 6<br><br>[44] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv \u0301e\nLe Borgne, Romaric Besanc \u0327on, Jos \u0301e G Moreno, and Jes \u0301us\nLov \u0301on Melgarejo. Viquae, a dataset for knowledge-based\nvisual question answering about named entities. In*Pro-*\n*ceedings of the 45th International ACM SIGIR Conference*\n*on Research and Development in Information Retrieval*,\npages 3108\u20133120, 2022. 6<br><br>[45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension, 2023. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "| 86% Public Health England<br><br>ee\nae\nMS aay\n\u201cet? OWHAT IS\nn/a\n2\nwna? <abie\u201cy wy. contact tracing \u00ab\n|\n@\n=\n\u00a9<br><br>Contact tracing is a fundamental part of outbreak control\nthat\u2019s used by public health professionals around the world\nto prevent the spread of infections<br><br>FEN\nPerson<br><br>|\nfor coronavirus\n|\n(COVID-19)\nel",
            {
                "chunk_size": "small"
            }
        ],
        [
            "| 86% Public Health England<br><br>ee\nae\nMS aay\n\u201cet? OWHAT IS\nn/a\n2\nwna? <abie\u201cy wy. contact tracing \u00ab\n|\n@\n=\n\u00a9<br><br>Contact tracing is a fundamental part of outbreak control\nthat\u2019s used by public health professionals around the world\nto prevent the spread of infections<br><br>FEN\nPerson<br><br>|\nfor coronavirus\n|\n(COVID-19)\nel",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Task\nDataset<br><br>General Semantic Alignment\nShareGPT4V-PT [14], COCO [17], Nocaps [1], TextCaps [86], LAION400M [80], SBU [75], CC 3M [83]\nWorld Knowledge Alignment\nConcept Data [110]\nVision Capability Enhancement\nWanJuan [35], Flicker[103], MMC-Inst[54], RCTW-17[84], CTW[106], LSVT[88], ReCTs[111], ArT[22]<br><br>Table 1. **Datasets used for Pre-Training**. The data are collected from diverse sources for the three objectives. The newly added data is\nhighlighted with red.<br><br>image has a 2D structure and the image ratio is dynamic,\nthe number of tokens for each row can vary across dif-\nferent images. This variation can potentially confuse the\nLVLM, making it difficult to determine which tokens be-\nlong to the same row of the image and which ones belong\nto the next row. This confusion may hinder the LVLM\u2019s\nability to understand the 2D structure of the image, which\nis crucial for comprehending structural image content such\nas documents, charts, and tables. To address this issue, we\nintroduce a learnable newline (\u2018*\\*n\u2019) token at the end of each\nrow of the image features before the flattening. Finally, we\nconcatenate the global and local views, inserting a special\n\u2018separate\u2019 token between them to distinguish the two views.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Task\nDataset<br><br>General Semantic Alignment\nShareGPT4V-PT [14], COCO [17], Nocaps [1], TextCaps [86], LAION400M [80], SBU [75], CC 3M [83]\nWorld Knowledge Alignment\nConcept Data [110]\nVision Capability Enhancement\nWanJuan [35], Flicker[103], MMC-Inst[54], RCTW-17[84], CTW[106], LSVT[88], ReCTs[111], ArT[22]<br><br>Table 1. **Datasets used for Pre-Training**. The data are collected from diverse sources for the three objectives. The newly added data is\nhighlighted with red.<br><br>image has a 2D structure and the image ratio is dynamic,\nthe number of tokens for each row can vary across dif-\nferent images. This variation can potentially confuse the\nLVLM, making it difficult to determine which tokens be-\nlong to the same row of the image and which ones belong\nto the next row. This confusion may hinder the LVLM\u2019s\nability to understand the 2D structure of the image, which\nis crucial for comprehending structural image content such\nas documents, charts, and tables. To address this issue, we\nintroduce a learnable newline (\u2018*\\*n\u2019) token at the end of each\nrow of the image features before the flattening. Finally, we\nconcatenate the global and local views, inserting a special\n\u2018separate\u2019 token between them to distinguish the two views.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Xiaoyi Dong*\u2217*1*,*2, Pan Zhang*\u2217*1, Yuhang Zang*\u2217*1, Yuhang Cao1*,*2, Bin Wang1, Linke Ouyang1,\nSongyang Zhang1, Haodong Duan1, Wenwei Zhang1, Yining Li1, Hang Yan1, Yang Gao1, Zhe Chen1<br><br>Xinyue Zhang1, Wei Li1, Jingwen Li1, Wenhai Wang1*,*2, Kai Chen1, Conghui He3, Xingcheng Zhang3,\nJifeng Dai4*,*1, Yu Qiao1, Dahua Lin1*,*2, Jiaqi Wang1*,*\ufffd<br><br>arXiv:2404.06512v1  [cs.CV]  9 Apr 2024<br><br>1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n3SenseTime Group, 4Tsinghua University<br><br>internlm@pjlab.org.cn<br><br>Figure 1. Overview of InternLM-XComposer2-4KHD perfor-\nmance on benchmarks with different resolutions. Our model based\non InternLM2-7B [91]**matches or even surpasses GPT-4V [74]**\n**and Gemini Pro [90] in 10 of the 16 benchmarks**.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Xiaoyi Dong*\u2217*1*,*2, Pan Zhang*\u2217*1, Yuhang Zang*\u2217*1, Yuhang Cao1*,*2, Bin Wang1, Linke Ouyang1,\nSongyang Zhang1, Haodong Duan1, Wenwei Zhang1, Yining Li1, Hang Yan1, Yang Gao1, Zhe Chen1<br><br>Xinyue Zhang1, Wei Li1, Jingwen Li1, Wenhai Wang1*,*2, Kai Chen1, Conghui He3, Xingcheng Zhang3,\nJifeng Dai4*,*1, Yu Qiao1, Dahua Lin1*,*2, Jiaqi Wang1*,*\ufffd<br><br>arXiv:2404.06512v1  [cs.CV]  9 Apr 2024<br><br>1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n3SenseTime Group, 4Tsinghua University<br><br>internlm@pjlab.org.cn<br><br>Figure 1. Overview of InternLM-XComposer2-4KHD perfor-\nmance on benchmarks with different resolutions. Our model based\non InternLM2-7B [91]**matches or even surpasses GPT-4V [74]**\n**and Gemini Pro [90] in 10 of the 16 benchmarks**.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**4. Experiments**<br><br>In this section, we validate the benchmark performance\nof our InternLM-XComposer2-4KHD (IXC2-4KHD in the\nfollowing for simplicity) after supervised fine-tuning.<br><br>**4.1. LVLM Benchmark results. **<br><br>In Table 3 and Table 4,\nwe compare our IXC2-\n4KHD\non\na\nlist\nof\nbenchmarks\nwith\nboth\nSOTA",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**4. Experiments**<br><br>In this section, we validate the benchmark performance\nof our InternLM-XComposer2-4KHD (IXC2-4KHD in the\nfollowing for simplicity) after supervised fine-tuning.<br><br>**4.1. LVLM Benchmark results. **<br><br>In Table 3 and Table 4,\nwe compare our IXC2-\n4KHD\non\na\nlist\nof\nbenchmarks\nwith\nboth\nSOTA",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "is assessed to see how close\nthey were to aconfirmed case,\n2\noH)\nwhich could include someone\nliving with the case, or\nspeak to the patient to identify\n_someone who has been in\n_\nanyone who has had close contact\n(Aaee ee\nwith them during the time they are infectious<br><br>A contact<br><br>Following this assessment, we can categorise\nthem into low or high risk and contact them to\nprovide advice on what they should do<br><br>ge\nchante\nIf we believe a contact is at\ni\nlow risk\n14\nhigher risk of infection\nG contact\ndays\nthey may be asked to\n\u201can doesn\u2019t require\nGW,\nself-isolate, remaining in\nHy]\nself-isolation\ntheir home for 14 days and\n4\nstaying away from work,\nschool or public places<br><br>a\nWe contact them daily\npassive follow up\nLa)\nprovide them with advice\nwhich means person being\noF\non what to do if they\nmonitored but we don\u2019t necessarily\nx4]\nbecome unwell until they\ncontact them every day\ncan be given the all-clear<br><br>contact on",
            {
                "chunk_size": "small"
            }
        ],
        [
            "is assessed to see how close\nthey were to aconfirmed case,\n2\noH)\nwhich could include someone\nliving with the case, or\nspeak to the patient to identify\n_someone who has been in\n_\nanyone who has had close contact\n(Aaee ee\nwith them during the time they are infectious<br><br>A contact<br><br>Following this assessment, we can categorise\nthem into low or high risk and contact them to\nprovide advice on what they should do<br><br>ge\nchante\nIf we believe a contact is at\ni\nlow risk\n14\nhigher risk of infection\nG contact\ndays\nthey may be asked to\n\u201can doesn\u2019t require\nGW,\nself-isolate, remaining in\nHy]\nself-isolation\ntheir home for 14 days and\n4\nstaying away from work,\nschool or public places<br><br>a\nWe contact them daily\npassive follow up\nLa)\nprovide them with advice\nwhich means person being\noF\non what to do if they\nmonitored but we don\u2019t necessarily\nx4]\nbecome unwell until they\ncontact them every day\ncan be given the all-clear<br><br>contact on",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[86] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In*Computer Vision\u2013*\n*ECCV 2020: 16th European Conference, Glasgow, UK, Au-*\n*gust 23\u201328, 2020, Proceedings, Part II 16*, pages 742\u2013758. Springer, 2020. 6<br><br>[87] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In*Proceed-*\n*ings of the IEEE/CVF conference on computer vision and*\n*pattern recognition*, pages 8317\u20138326, 2019. 1, 2, 6, 7<br><br>[88] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\nCanjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo\nLiu, Dimosthenis Karatzas, et al. Icdar 2019 competition on\nlarge-scale street view text with partial labeling-rrc-lsvt. In\n*2019 International Conference on Document Analysis and*\n*Recognition (ICDAR)*, pages 1557\u20131562. IEEE, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "IEEE, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[86] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension. In*Computer Vision\u2013*\n*ECCV 2020: 16th European Conference, Glasgow, UK, Au-*\n*gust 23\u201328, 2020, Proceedings, Part II 16*, pages 742\u2013758. Springer, 2020. 6<br><br>[87] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In*Proceed-*\n*ings of the IEEE/CVF conference on computer vision and*\n*pattern recognition*, pages 8317\u20138326, 2019. 1, 2, 6, 7<br><br>[88] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu,\nCanjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo\nLiu, Dimosthenis Karatzas, et al. Icdar 2019 competition on\nlarge-scale street view text with partial labeling-rrc-lsvt. In\n*2019 International Conference on Document Analysis and*\n*Recognition (ICDAR)*, pages 1557\u20131562. IEEE, 2019. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[102] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. *arXiv.org*, 2023. 2<br><br>[103] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. *Transactions of the Association for Computa-*\n*tional Linguistics*, 2:67\u201378, 2014. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[102] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers\nlarge language models with multimodality. *arXiv.org*, 2023. 2<br><br>[103] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. *Transactions of the Association for Computa-*\n*tional Linguistics*, 2:67\u201378, 2014. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[24] OpenCompass Contributors. Opencompass:\nA univer-\nsal evaluation platform for foundation models. https:\n//github.com/open- compass/opencompass,\n2023. 7<br><br>[25] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 2<br><br>[26] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos \u0301e M.F. Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In*Proceedings of the IEEE Confer-*\n*ence on Computer Vision and Pattern Recognition (CVPR)*,\n2017. 6<br><br>[27] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\nBin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,\nHaodong Duan, Maosong Cao, Wenwei Zhang, Yining\nLi, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-\nwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu\nQiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024. 2, 5",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024. 2, 5",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[24] OpenCompass Contributors. Opencompass:\nA univer-\nsal evaluation platform for foundation models. https:\n//github.com/open- compass/opencompass,\n2023. 7<br><br>[25] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 2<br><br>[26] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh,\nDeshraj Yadav, Jos \u0301e M.F. Moura, Devi Parikh, and Dhruv\nBatra. Visual Dialog. In*Proceedings of the IEEE Confer-*\n*ence on Computer Vision and Pattern Recognition (CVPR)*,\n2017. 6<br><br>[27] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao,\nBin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang,\nHaodong Duan, Maosong Cao, Wenwei Zhang, Yining\nLi, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jing-\nwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu\nQiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2:\nMastering free-form text-image composition and compre-\nhension in vision-language large model. *arXiv preprint*\n*arXiv:2401.16420*, 2024. 2, 5",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Model\n\u2018*\\*n\u2019\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>HD9\n*\u00d7*\n79.5\n50.3\n74.0\n78.2\n79.1\n2206\n75.9\nHD9\n\u2713\n79.4\n50.5\n73.8\n78.2\n79.5\n2201\n**76.6**<br><br>4KHD\n*\u00d7*\n88.1\n67.4\n75.9\n80.4\n79.9\n**2232**\n76.4\n4KHD\n\u2713\n**89.0**\n**69.3**\n**77.2**\n**81.0**\n**80.2**\n2205\n76.2<br><br>Table 8. **Influence of Indicator \u2018***\\***n\u2019 in the Image Features. **\u2018*\\*n\u2019\nhelps LVLM understand structural images when the input resolu-\ntion is dynamic and large.<br><br>Strategy\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>Re-Sampler\n86.2\n67.1\n75.3\n78.8\n79.6\n2124\n74.2\nC-Abstractor\n88.6\n69.5\n77.1\n80.6\n80.4\n2236\n76.7\nConcat\n89.0\n69.3\n77.2\n81.0\n80.2\n2205\n76.2<br><br>Table 9. **Ablation on Token Merging Strategy. **Both the simple\nconcatenation operation and the C-Abstractor works well.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Model\n\u2018*\\*n\u2019\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>HD9\n*\u00d7*\n79.5\n50.3\n74.0\n78.2\n79.1\n2206\n75.9\nHD9\n\u2713\n79.4\n50.5\n73.8\n78.2\n79.5\n2201\n**76.6**<br><br>4KHD\n*\u00d7*\n88.1\n67.4\n75.9\n80.4\n79.9\n**2232**\n76.4\n4KHD\n\u2713\n**89.0**\n**69.3**\n**77.2**\n**81.0**\n**80.2**\n2205\n76.2<br><br>Table 8. **Influence of Indicator \u2018***\\***n\u2019 in the Image Features. **\u2018*\\*n\u2019\nhelps LVLM understand structural images when the input resolu-\ntion is dynamic and large.<br><br>Strategy\nDoc\nInfo\nText\nChart\nMMB\nMME\nSEED*\u2217*<br><br>Re-Sampler\n86.2\n67.1\n75.3\n78.8\n79.6\n2124\n74.2\nC-Abstractor\n88.6\n69.5\n77.1\n80.6\n80.4\n2236\n76.7\nConcat\n89.0\n69.3\n77.2\n81.0\n80.2\n2205\n76.2<br><br>Table 9. **Ablation on Token Merging Strategy. **Both the simple\nconcatenation operation and the C-Abstractor works well.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Task\nDataset<br><br>Caption\nShareGPT4V [14], COCO [17],Nocaps [1]<br><br>General QA\nVQAv2 [3], GQA [38], OK-VQA [67]\nVD [26], RD[13], VSR[53],<br><br>Science QA\nAI2D [42], SQA [63], TQA[43], IconQA[65]<br><br>Chart QA\nDVQA [40], ChartQA, ChartQA-AUG [68]<br><br>Math QA\nMathQA [104], Geometry3K[62], TabMWP[64],\nCLEVR-MATH[52]/Super[49]<br><br>World Knowledge QA A-OKVQA [81],KVQA [82], ViQuAE[44]<br><br>OCR QA\nTextVQA[87], OCR-VQA[72], ST-VQA[8]<br><br>HD-OCR QA\nInfoVQA[69], DocVQA[70]<br><br>Conversation\nLLaVA-150k [56], LVIS-Instruct4V [94]\nShareGPT-en&zh [21], InternLM-Chat[91]<br><br>Table 2. **Datasets used for Supervised Fine-Tuning**. We collect\ndata from diverse sources to empower the model with different\ncapabilities. The newly added data is highlighted with red.<br><br>**3.3. Pre-Training**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Task\nDataset<br><br>Caption\nShareGPT4V [14], COCO [17],Nocaps [1]<br><br>General QA\nVQAv2 [3], GQA [38], OK-VQA [67]\nVD [26], RD[13], VSR[53],<br><br>Science QA\nAI2D [42], SQA [63], TQA[43], IconQA[65]<br><br>Chart QA\nDVQA [40], ChartQA, ChartQA-AUG [68]<br><br>Math QA\nMathQA [104], Geometry3K[62], TabMWP[64],\nCLEVR-MATH[52]/Super[49]<br><br>World Knowledge QA A-OKVQA [81],KVQA [82], ViQuAE[44]<br><br>OCR QA\nTextVQA[87], OCR-VQA[72], ST-VQA[8]<br><br>HD-OCR QA\nInfoVQA[69], DocVQA[70]<br><br>Conversation\nLLaVA-150k [56], LVIS-Instruct4V [94]\nShareGPT-en&zh [21], InternLM-Chat[91]<br><br>Table 2. **Datasets used for Supervised Fine-Tuning**. We collect\ndata from diverse sources to empower the model with different\ncapabilities. The newly added data is highlighted with red.<br><br>**3.3. Pre-Training**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[83] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In*Pro-*\n*ceedings of the 56th Annual Meeting of the Association for*\n*Computational Linguistics (Volume 1: Long Papers)*, pages\n2556\u20132565, 2018. 6<br><br>[84] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\nPei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\nBai. Icdar2017 competition on reading chinese text in the\nwild (rctw-17). In*2017 14th iapr international conference*\n*on document analysis and recognition (ICDAR)*, volume 1,\npages 1429\u20131434. IEEE, 2017. 6<br><br>[85] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu,\nand Diyi Yang. Design2code: How far are we from au-\ntomating front-end engineering?, 2024. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[83] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In*Pro-*\n*ceedings of the 56th Annual Meeting of the Association for*\n*Computational Linguistics (Volume 1: Long Papers)*, pages\n2556\u20132565, 2018. 6<br><br>[84] Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang,\nPei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang\nBai. Icdar2017 competition on reading chinese text in the\nwild (rctw-17). In*2017 14th iapr international conference*\n*on document analysis and recognition (ICDAR)*, volume 1,\npages 1429\u20131434. IEEE, 2017. 6<br><br>[85] Chenglei Si, Yanzhe Zhang, Zhengyuan Yang, Ruibo Liu,\nand Diyi Yang. Design2code: How far are we from au-\ntomating front-end engineering?, 2024. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "a>\nAnalyze this figure in detail<br><br>7)<br><br>The image is a scientific paper titled \"ShareGPT4V: Improving \nLarge Multi-Modal Models with Better Captions\" from the \nUniversity of Science and Technology of China and Shanghai \nAI Laboratory. The paper discusses the comparison between the \ncaptions in their proposed ShareGPT4V dataset and those \nutilized by recent large multi-modal models (LMMs). The \ncaptions are compared in terms of their quality and \nperformance. The paper also highlights the remarkable \nperformance of the proposed LMMs based on a simple \narchitecture that has remarkable performance across a majority \nof the multi-modal benchmarks.<br><br>0.6<br><br>64,\n68.8\n6.7,Lm\nMMBench\n623<br><br>MMBench-CN<br><br>@ ShareGPT4V-7B<br><br>a>\nWhich model has the best performance on the \nMMBench and what is its exact number? Qwen-VL-Chat-7B<br><br>7)\nAccording to the paper, ShareGPT4V-7B has the best \nperformance on the MMBench with a score of 68.8.<br><br>a>\nFigure 1. (a) We showease a comparison between the caption in our proposed ShareGPT4V dataset and those utilized by recent large\nmulti-modal models (LMMs). Unlike COCO-Caption [7] involves brief human-made captions on the main subject. LLaVA-Instruct\n(31) combines human-made captions, bounding boxes, and GPT4 [39] to \u2018imagine\u2019 the image details, which leads to inevitable er-\nror/hallucination description (marked in red), Our approach involves feeding carefully designed prompts along with images directly into\nthe advanced GPT4-Vision [40] and the descriptions are more detailed and accurate (marked in blue). (b) We highlight the remarkable\nperformance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset.<br><br>Where can I get this project?<br><br>7) You can get this project from the following GitHub link: \nhttps://ShareGPT4V.github.io\nThe HD image (2550 x 3300)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nAnalyze this figure in detail<br><br>7)<br><br>The image is a scientific paper titled \"ShareGPT4V: Improving \nLarge Multi-Modal Models with Better Captions\" from the \nUniversity of Science and Technology of China and Shanghai \nAI Laboratory. The paper discusses the comparison between the \ncaptions in their proposed ShareGPT4V dataset and those \nutilized by recent large multi-modal models (LMMs). The \ncaptions are compared in terms of their quality and \nperformance. The paper also highlights the remarkable \nperformance of the proposed LMMs based on a simple \narchitecture that has remarkable performance across a majority \nof the multi-modal benchmarks.<br><br>0.6<br><br>64,\n68.8\n6.7,Lm\nMMBench\n623<br><br>MMBench-CN<br><br>@ ShareGPT4V-7B<br><br>a>\nWhich model has the best performance on the \nMMBench and what is its exact number? Qwen-VL-Chat-7B<br><br>7)\nAccording to the paper, ShareGPT4V-7B has the best \nperformance on the MMBench with a score of 68.8.<br><br>a>\nFigure 1. (a) We showease a comparison between the caption in our proposed ShareGPT4V dataset and those utilized by recent large\nmulti-modal models (LMMs). Unlike COCO-Caption [7] involves brief human-made captions on the main subject. LLaVA-Instruct\n(31) combines human-made captions, bounding boxes, and GPT4 [39] to \u2018imagine\u2019 the image details, which leads to inevitable er-\nror/hallucination description (marked in red), Our approach involves feeding carefully designed prompts along with images directly into\nthe advanced GPT4-Vision [40] and the descriptions are more detailed and accurate (marked in blue). (b) We highlight the remarkable\nperformance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset.<br><br>Where can I get this project?<br><br>7) You can get this project from the following GitHub link: \nhttps://ShareGPT4V.github.io\nThe HD image (2550 x 3300)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**5. Conclusion**<br><br>In this paper, we propose the InternLM-Xcomposer2-\n4KHD that exceeds the performance of previous open-\nsource models on OCR-related tasks and also achieves\ncompetitive results on general-purpose LVLM benchmarks. Thanks to our dynamic resolution and automatic patch con-\nfiguration, our model supports a maximum training resolu-\ntion of up to 4K HD. We also integrate a global view patch\nto support the macro understanding and a learnable newline\ntoken to handle the various input image resolutions. Our\nmodel\u2019s performance continues to improve as the training\nresolution increases for HD-OCR tasks. Notably, we do\nnot observe any performance saturation even for the 4KHD\nsetting, and we have not explored the upper bound due to\nthe computational burden increasing with higher-resolution\ninputs. In future work, we plan to explore efficient solu-\ntions for accurate LVLM training and inference, enabling\nour model to handle even higher resolutions while main-\ntaining computational efficiency.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**5. Conclusion**<br><br>In this paper, we propose the InternLM-Xcomposer2-\n4KHD that exceeds the performance of previous open-\nsource models on OCR-related tasks and also achieves\ncompetitive results on general-purpose LVLM benchmarks. Thanks to our dynamic resolution and automatic patch con-\nfiguration, our model supports a maximum training resolu-\ntion of up to 4K HD. We also integrate a global view patch\nto support the macro understanding and a learnable newline\ntoken to handle the various input image resolutions. Our\nmodel\u2019s performance continues to improve as the training\nresolution increases for HD-OCR tasks. Notably, we do\nnot observe any performance saturation even for the 4KHD\nsetting, and we have not explored the upper bound due to\nthe computational burden increasing with higher-resolution\ninputs. In future work, we plan to explore efficient solu-\ntions for accurate LVLM training and inference, enabling\nour model to handle even higher resolutions while main-\ntaining computational efficiency.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[32] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. *arXiv preprint*\n*arXiv:2312.12436*, 2023. 2<br><br>[33] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a seed of vision in large language model. *arXiv preprint arXiv:2307.08041*, 2023. 1, 5<br><br>[34] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou. Hallusionbench: An advanced diagnostic suite for\nentangled language hallucination & visual illusion in large\nvision-language models, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[32] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. *arXiv preprint*\n*arXiv:2312.12436*, 2023. 2<br><br>[33] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a seed of vision in large language model. *arXiv preprint arXiv:2307.08041*, 2023. 1, 5<br><br>[34] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou. Hallusionbench: An advanced diagnostic suite for\nentangled language hallucination & visual illusion in large\nvision-language models, 2023. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "During the pre-training phase, the LLM is frozen while\nboth the vision encoder and Partial LoRA are fine-tuned to\nalign the visual tokens with the LLM. The pre-training data\nmainly follow the design in XComposer2 which is curated\nwith**three objectives**in mind: 1) general semantic align-\nment, 2) world knowledge alignment, 3) vision capability\nenhancement. In this paper, we focus on high-resolution\nand structural image understanding. Therefore, we have\ncollected more related data to enhance this specific capa-\nbility. As shown in Table.1, we have utilized a diverse OCR\ndataset for this purpose.<br><br>**3.4. 4KHD Supervised Fine-tuning**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "During the pre-training phase, the LLM is frozen while\nboth the vision encoder and Partial LoRA are fine-tuned to\nalign the visual tokens with the LLM. The pre-training data\nmainly follow the design in XComposer2 which is curated\nwith**three objectives**in mind: 1) general semantic align-\nment, 2) world knowledge alignment, 3) vision capability\nenhancement. In this paper, we focus on high-resolution\nand structural image understanding. Therefore, we have\ncollected more related data to enhance this specific capa-\nbility. As shown in Table.1, we have utilized a diverse OCR\ndataset for this purpose.<br><br>**3.4. 4KHD Supervised Fine-tuning**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "and Pete Florence. Palm-e: An embodied multimodal lan-\nguage model. In*arXiv preprint arXiv:2303.03378*, 2023. 2<br><br>[29] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. In\n*Proceedings of the 60th Annual Meeting of the Association*\n*for Computational Linguistics (Volume 1: Long Papers)*,\npages 320\u2013335, 2022. 1<br><br>[30] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang\nLi, and Can Huang. DocPedia: Unleashing the power\nof large multimodal model in the frequency domain\nfor versatile document understanding. *arXiv preprint*\n*arXiv:2311.11810*, 2023. 5<br><br>[31] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "and Pete Florence. Palm-e: An embodied multimodal lan-\nguage model. In*arXiv preprint arXiv:2303.03378*, 2023. 2<br><br>[29] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling. In\n*Proceedings of the 60th Annual Meeting of the Association*\n*for Computational Linguistics (Volume 1: Long Papers)*,\npages 320\u2013335, 2022. 1<br><br>[30] Hao Feng, Qi Liu, Hao Liu, Wengang Zhou, Houqiang\nLi, and Can Huang. DocPedia: Unleashing the power\nof large multimodal model in the frequency domain\nfor versatile document understanding. *arXiv preprint*\n*arXiv:2311.11810*, 2023. 5<br><br>[31] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. *arXiv preprint*\n*arXiv:2306.13394*, 2023. 1, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "In many communities, fruits and vegetables are limited/expensive and only\navailable in corner stores, convenience stores, bodegas and gas stations.<br><br>Lack of Access Can Profoundly Impact Diet\nThere are ways that you can actively support improved access to\nhigher quality foods for neighbourhoods. Here's how:<br><br>Contact your city and state\nFind ways to spread the word\nCreate a petition for more\nleaders to let them know\nabout nutrition assistance\nvariety, improve affordability\nwhat food access is like in\nprograms, such as SNAP, WIC and\nor advocate for better\nyour community\nschool meals. signage/placement.<br><br>\u00a2\nS<br><br>Meet with an after-school or\nOrganize a letter-writing\n\u2018Sign up for \u201cYou're the Cure\u201d\ndaycare program representative\n\u2018campaign and set up a meeting\nand then send a note to your\nto discuss serving more fruits and\n_\u2014_with state leaders. Congressperson advocating for\nveggies for snacks. For example, ask for funding to\nhealthier meals at school\nhost a farmers\u2019 market in an\nunderserved community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In many communities, fruits and vegetables are limited/expensive and only\navailable in corner stores, convenience stores, bodegas and gas stations.<br><br>Lack of Access Can Profoundly Impact Diet\nThere are ways that you can actively support improved access to\nhigher quality foods for neighbourhoods. Here's how:<br><br>Contact your city and state\nFind ways to spread the word\nCreate a petition for more\nleaders to let them know\nabout nutrition assistance\nvariety, improve affordability\nwhat food access is like in\nprograms, such as SNAP, WIC and\nor advocate for better\nyour community\nschool meals. signage/placement.<br><br>\u00a2\nS<br><br>Meet with an after-school or\nOrganize a letter-writing\n\u2018Sign up for \u201cYou're the Cure\u201d\ndaycare program representative\n\u2018campaign and set up a meeting\nand then send a note to your\nto discuss serving more fruits and\n_\u2014_with state leaders. Congressperson advocating for\nveggies for snacks. For example, ask for funding to\nhealthier meals at school\nhost a farmers\u2019 market in an\nunderserved community.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "After the pre-training, we empower the model to understand\nhigh-resolution images and solve diverse challenges. Dif-\nferent from previous perception tasks (*e.g*., VQAv2, GQA)\nwhich typically answer questions based on the noticeable\nobject in the image. OCR-related tasks depend on a de-\ntailed understanding of text within a high-resolution image. For instance, in InfoVQA, the length of the longer side of\n50% of the images exceeds 2000 pixels. Low-resolution\ninputs can distort the dense text information, causing the\nmodel to fail in its understanding. However, we have ob-\nserved a resolution saturation problem with the aforemen-\ntioned perception tasks, where the influence of resolution\nbecomes negligible. To address this, we introduce a mixed-resolution train-\ning strategy for more efficient training. For tasks requir-\ning high resolution, we employ the \u2018HD-55\u2019 setting during\ntraining. This allows for the input of 4K (3840*\u00d7*1600) im-\nages without necessitating additional image compression. These tasks are referred to as the HD-OCR QA tasks in Ta-\nble 2. For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the \u2018HD25\u2019 set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence. For instance, we have observed that using the \u2018HD30\u2019",
            {
                "chunk_size": "small"
            }
        ],
        [
            "For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the \u2018HD25\u2019 set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence. For instance, we have observed that using the \u2018HD30\u2019",
            {
                "chunk_size": "small"
            }
        ],
        [
            "After the pre-training, we empower the model to understand\nhigh-resolution images and solve diverse challenges. Dif-\nferent from previous perception tasks (*e.g*., VQAv2, GQA)\nwhich typically answer questions based on the noticeable\nobject in the image. OCR-related tasks depend on a de-\ntailed understanding of text within a high-resolution image. For instance, in InfoVQA, the length of the longer side of\n50% of the images exceeds 2000 pixels. Low-resolution\ninputs can distort the dense text information, causing the\nmodel to fail in its understanding. However, we have ob-\nserved a resolution saturation problem with the aforemen-\ntioned perception tasks, where the influence of resolution\nbecomes negligible. To address this, we introduce a mixed-resolution train-\ning strategy for more efficient training. For tasks requir-\ning high resolution, we employ the \u2018HD-55\u2019 setting during\ntraining. This allows for the input of 4K (3840*\u00d7*1600) im-\nages without necessitating additional image compression. These tasks are referred to as the HD-OCR QA tasks in Ta-\nble 2. For other tasks, we implement a dynamic-resolution\nstrategy. Images are resized to fall within a range between\ntheir original size and the size specified by the \u2018HD25\u2019 set-\nting. This dynamic approach enhances the robustness of the\nLVLM against differences in input resolution, thereby en-\nabling the LVLM to utilize a larger resolution during infer-\nence. For instance, we have observed that using the \u2018HD30\u2019",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "*arXiv:2209.14610*, 2022. 6<br><br>[65] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram under-\nstanding and visual language reasoning. *arXiv preprint*\n*arXiv:2110.13214*, 2021. 6<br><br>[66] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\ning Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,\nLi Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha\nZhang, and Furu Wei. Kosmos-2.5: A multimodal literate\nmodel, 2023. 2, 5<br><br>[67] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In*Proceedings*\n*of the IEEE/cvf conference on computer vision and pattern*\n*recognition*, pages 3195\u20133204, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*arXiv:2209.14610*, 2022. 6<br><br>[65] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao,\nWei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram under-\nstanding and visual language reasoning. *arXiv preprint*\n*arXiv:2110.13214*, 2021. 6<br><br>[66] Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shum-\ning Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang,\nLi Dong, Weiyao Luo, Shaoxiang Wu, Guoxin Wang, Cha\nZhang, and Furu Wei. Kosmos-2.5: A multimodal literate\nmodel, 2023. 2, 5<br><br>[67] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In*Proceedings*\n*of the IEEE/cvf conference on computer vision and pattern*\n*recognition*, pages 3195\u20133204, 2019. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**LVLMs for Document Understanding. **\nDocument un-\nderstanding involves analyzing and comprehending various\ndigital documents, such as figures, tables, and academic pa-\npers. Many document understanding tasks require models\nto handle high-resolution inputs, complex layouts, various\naspect ratios, and diverse document formats. To enhance the\ncapabilities of LVLMs for document understanding, sev-\neral works have collected and constructed high-quality doc-\nument instruction tuning data, including LLaVAR [112],\nmPLUG-DocOwl [100] and TGDoc [96]. DocPediaDoc-\nPedia [30] processes document inputs in the frequency do-\nmain. Some previous works have improved document un-\nderstanding ability by designing special modules for high-\nresolution inputs, such as HR and LR encoders [36, 97]\nor cropped image patches [59, 99, 101]. Our InternLM-\nXComposer2-4KHD first scales to 4K resolution inputs\nand demonstrates strong document understanding ability on\nOCR-related benchmarks. Also, our approach also achieves\ncomparable results on other general LVLM benchmarks like\nperception and reasoning [15, 33, 57, 61].",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**LVLMs for Document Understanding. **\nDocument un-\nderstanding involves analyzing and comprehending various\ndigital documents, such as figures, tables, and academic pa-\npers. Many document understanding tasks require models\nto handle high-resolution inputs, complex layouts, various\naspect ratios, and diverse document formats. To enhance the\ncapabilities of LVLMs for document understanding, sev-\neral works have collected and constructed high-quality doc-\nument instruction tuning data, including LLaVAR [112],\nmPLUG-DocOwl [100] and TGDoc [96]. DocPediaDoc-\nPedia [30] processes document inputs in the frequency do-\nmain. Some previous works have improved document un-\nderstanding ability by designing special modules for high-\nresolution inputs, such as HR and LR encoders [36, 97]\nor cropped image patches [59, 99, 101]. Our InternLM-\nXComposer2-4KHD first scales to 4K resolution inputs\nand demonstrates strong document understanding ability on\nOCR-related benchmarks. Also, our approach also achieves\ncomparable results on other general LVLM benchmarks like\nperception and reasoning [15, 33, 57, 61].",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Which department has the highest cost in most cases?",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Which department has the highest cost in most cases?",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[60] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-\naoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. RAR: Retrieving and ranking augmented mllms for visual\nrecognition. *arXiv preprint arXiv:2403.13805*, 2024. 2<br><br>[61] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. In*International Conference on Learning Represen-*\n*tations (ICLR)*, 2024. 5, 7<br><br>[62] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\nterpretable geometry problem solving with formal language\nand symbolic reasoning. In*The 59th Annual Meeting of the*\n*Association for Computational Linguistics (ACL)*, 2021. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[60] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xi-\naoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. RAR: Retrieving and ranking augmented mllms for visual\nrecognition. *arXiv preprint arXiv:2403.13805*, 2024. 2<br><br>[61] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. In*International Conference on Learning Represen-*\n*tations (ICLR)*, 2024. 5, 7<br><br>[62] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\nterpretable geometry problem solving with formal language\nand symbolic reasoning. In*The 59th Annual Meeting of the*\n*Association for Computational Linguistics (ACL)*, 2021. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**The Role of Global-View. **We first examine the impact\nof the global view in our Global-Local Format. As indi-\ncated in Table 7, we find that the global view is essential\nfor the LVLM to accurately comprehend the input image. When it is removed, the model performs worse across all\nbenchmarks. For instance, the model experiences a*\u2212*4*. *4%\ndrop in performance on the MMBench EN-Test without the\nglobal view. We contend that the global view offers a gen-\neral macro understanding of the image, which the model\nstruggled to derive from the large number of tokens in the\nlocal view. **The Role of the Newline Token. **We incorporate a special\nnewline token at the end of each row of the image features\nbefore the flattening operation. This token serves as an in-\ndicator of the image\u2019s 2D structure. We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM\u2019s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension. We have",
            {
                "chunk_size": "small"
            }
        ],
        [
            "We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM\u2019s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension. We have",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**The Role of Global-View. **We first examine the impact\nof the global view in our Global-Local Format. As indi-\ncated in Table 7, we find that the global view is essential\nfor the LVLM to accurately comprehend the input image. When it is removed, the model performs worse across all\nbenchmarks. For instance, the model experiences a*\u2212*4*. *4%\ndrop in performance on the MMBench EN-Test without the\nglobal view. We contend that the global view offers a gen-\neral macro understanding of the image, which the model\nstruggled to derive from the large number of tokens in the\nlocal view. **The Role of the Newline Token. **We incorporate a special\nnewline token at the end of each row of the image features\nbefore the flattening operation. This token serves as an in-\ndicator of the image\u2019s 2D structure. We examine its impact\non both the HD-9 and 4KHD strategies in Table 8. When\na fixed high-resolution strategy HD-9 is employed, we ob-\nserve that the benefit derived from the newline token is mi-\nnor. This could be attributed to the LVLM\u2019s ability to handle\nlimited differences in image ratios after training. However,\nwhen we implement a more challenging 4KHD (HD-25 +\nHD-55) strategy, which exhibits significant diversity in both\nimage ratio and token number, the LVLM demonstrates a\nnotable decline in performance on OCR-related tasks with-\nout the newline indicator. This finding supports our hypoth-\nesis that the LVLM struggles to comprehend the shape of\nthe image when the image tokens are directly flattened into\na 1D sequence. The newline token can assist the model in\nbetter understanding the structure of the image. **Influence of Token Merging Strategy. **\nIn practice, we\nemploy a simple merging strategy that concatenates four\nadjacent tokens along the channel dimension. We have",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "tical applicability in real-world scenarios. Recent advancements have aimed at enhancing the\nresolution of Large Vision-Language Models (LVLMs). Some approaches [36, 48, 66, 97] involve adapting high-\nresolution vision encoders directly. However, the Vi-\nsion Transformer (ViT) architecture falls short when deal-\ning with images of varying resolutions and aspect ratios,\nthereby restricting its ability to handle diverse inputs ef-\nfectively. Alternatively, some methods [37, 46, 50, 51, 55,\n59, 99] maintain the vision encoder\u2019s resolution, segment-\ning high-resolution images into multiple low-resolution\npatches. Yet, these methods are constrained by an inad-\nequate resolution, typically around 1500*\u00d7*1500, which\ndoes not satisfy the demands of daily content,*e.g*., website\nscreenshots [85], document pages [70], and blueprints [69]. Furthermore, they are confined to either a few predefined\nhigh-resolution settings [36, 46, 48, 50, 51, 55, 59, 66, 97]\nor a limited range of resolutions [37, 99], thereby restricting\ntheir utility across a variety of applications. In this work, we introduce InternLM-XComposer2-\n4KHD, a pioneering model that for the first time expands\nthe resolution capabilities of Large Vision-Language Mod-\nels (LVLMs) to 4K HD and even higher, thereby setting\na new standard in high-resolution vision-language under-\nstanding. Designed to handle a broad range of resolutions,\nInternLM-XComposer2-4KHD supports images with any\naspect ratio from 336 pixels up to 4K HD, facilitating its\ndeployment in real-world contexts. InternLM-XComposer2-4KHD\nfollows\npatch\ndivi-\nsion [46, 50] paradigm and enhances it by incorporating an\ninnovative extension: dynamic resolution with automatic\npatch configuration. To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*\u00d7*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2. Related Works**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*\u00d7*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2. Related Works**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "tical applicability in real-world scenarios. Recent advancements have aimed at enhancing the\nresolution of Large Vision-Language Models (LVLMs). Some approaches [36, 48, 66, 97] involve adapting high-\nresolution vision encoders directly. However, the Vi-\nsion Transformer (ViT) architecture falls short when deal-\ning with images of varying resolutions and aspect ratios,\nthereby restricting its ability to handle diverse inputs ef-\nfectively. Alternatively, some methods [37, 46, 50, 51, 55,\n59, 99] maintain the vision encoder\u2019s resolution, segment-\ning high-resolution images into multiple low-resolution\npatches. Yet, these methods are constrained by an inad-\nequate resolution, typically around 1500*\u00d7*1500, which\ndoes not satisfy the demands of daily content,*e.g*., website\nscreenshots [85], document pages [70], and blueprints [69]. Furthermore, they are confined to either a few predefined\nhigh-resolution settings [36, 46, 48, 50, 51, 55, 59, 66, 97]\nor a limited range of resolutions [37, 99], thereby restricting\ntheir utility across a variety of applications. In this work, we introduce InternLM-XComposer2-\n4KHD, a pioneering model that for the first time expands\nthe resolution capabilities of Large Vision-Language Mod-\nels (LVLMs) to 4K HD and even higher, thereby setting\na new standard in high-resolution vision-language under-\nstanding. Designed to handle a broad range of resolutions,\nInternLM-XComposer2-4KHD supports images with any\naspect ratio from 336 pixels up to 4K HD, facilitating its\ndeployment in real-world contexts. InternLM-XComposer2-4KHD\nfollows\npatch\ndivi-\nsion [46, 50] paradigm and enhances it by incorporating an\ninnovative extension: dynamic resolution with automatic\npatch configuration. To be specific, scaling the resolution\nof Large Vision-Language Models (LVLMs) to 4K HD\nand even higher standard is far beyond merely increasing\nthe number of patches. It involves a nuanced approach to\novercoming specific challenges: (1)**Dynamic Resolution**\n**and Automatic Patch Configuration**:\nAddressing the\nscarcity of high-resolution training data, our framework\nintroduces a strategy that dynamically adjusts resolution\nalongside an automatic layout configuration. During\ntraining, it maintains the original aspect ratios of images\nwhile adaptively altering patch (336*\u00d7*336) layouts and\ncounts. This results in a training resolution that exceeds\nthe original image resolutions, reaching up to 4KHD, ad-\ndressing the shortfall of high-resolution data. (2)**Handling**\n**Variability in Patch Configurations**: Despite the apparent\nsimplicity of dynamic resolution training, the variability\nin patch configurations can heavily confuse LVLMs. To\nmitigate this, we introduce a newline token after each\nrow of patch tokens to clearly delineate patch layouts,\nreducing training ambiguity and significantly boosting\nperformance. (3)**Inference Beyond 4K Resolution:**Our\nobservations reveal that, even when trained on images<br><br>**2. Related Works**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[89] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\nShu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. *arXiv preprint arXiv:2312.03818*, 2023. 2<br><br>[90] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2<br><br>[91] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities. https://",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[89] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\nShu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: A clip model focusing on wherever you want. *arXiv preprint arXiv:2312.03818*, 2023. 2<br><br>[90] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2<br><br>[91] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities. https://",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Muted Color Palettes\nMuted colors are colors that have alow\nsaturation (as opposed to vivid colors). Muted colors feel safe and secure, even\nnostalgic. They can also feel natural and\norganic. That\u2019s why many health and\nwellness brands have been using muted\ncolor palettes this year.<br><br>Simple Data\n|\nVisualizations\nThe goal of any data visualization should<br><br>be to make the complex\ndata easy to\nunderstand. We are living in a time where<br><br>a lot of data is constantly being circulated. Simple data visualizations\ncanmake\ncommunication more effective.<br><br>Geometric Shapes\nEverywhere<br><br>Last year, we saw designers usinga lot\nofflowing and abstract shapes in their\ndesigns. This year, they have been\nreplaced with rigid, hard-edged geo-\nmetric shapes and patterns. The hard\nedges of a geometric shape create a\ngreat contrast against muted colors.<br><br>Flat Icons and\nIllustrations<br><br>Many brands are using flat icons and\nillustrationsin their social media graphics,\nwebsite design, and more. Icons can be a<br><br>powerful tool for visual communication. With\na simple icon, you can communicate meaning\nin less space than words. Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Muted Color Palettes\nMuted colors are colors that have alow\nsaturation (as opposed to vivid colors). Muted colors feel safe and secure, even\nnostalgic. They can also feel natural and\norganic. That\u2019s why many health and\nwellness brands have been using muted\ncolor palettes this year.<br><br>Simple Data\n|\nVisualizations\nThe goal of any data visualization should<br><br>be to make the complex\ndata easy to\nunderstand. We are living in a time where<br><br>a lot of data is constantly being circulated. Simple data visualizations\ncanmake\ncommunication more effective.<br><br>Geometric Shapes\nEverywhere<br><br>Last year, we saw designers usinga lot\nofflowing and abstract shapes in their\ndesigns. This year, they have been\nreplaced with rigid, hard-edged geo-\nmetric shapes and patterns. The hard\nedges of a geometric shape create a\ngreat contrast against muted colors.<br><br>Flat Icons and\nIllustrations<br><br>Many brands are using flat icons and\nillustrationsin their social media graphics,\nwebsite design, and more. Icons can be a<br><br>powerful tool for visual communication. With\na simple icon, you can communicate meaning\nin less space than words. Plus, illustrations\nare way more creative than stock photos!<br><br>Classic Serif Fonts<br><br>Serif fonts are oneof the oldest font\nstyles still in use. They date all the way\nback to the 15th century, Because of\nthis, serif fonts are commonly seen as\nclassic, elegant and trustworthy. They<br><br>can evoke a feeling of nostalgia. That's\nwhy we see many financial services\ncompanies\nusing serif fonts in their\nmarketing collateral.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[38] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. *Conference on Computer Vision and*\n*Pattern Recognition (CVPR)*, 2019. 6<br><br>[39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, L \u0301elio Renard Lavaud, Marie-Anne",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[38] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. *Conference on Computer Vision and*\n*Pattern Recognition (CVPR)*, 2019. 6<br><br>[39] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, L \u0301elio Renard Lavaud, Marie-Anne",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**Large Vision-Language Models (LVLMs). **Large Lan-\nguage Models (LLMs) [6, 9, 10, 23, 39, 41, 73, 76, 78, 91\u2013\n93, 108] have gained significant attention due to their\nimpressive performance in various language-related tasks\nsuch as text generation and question answering. Follow-\ning this enthusiasm, recent Large Vision-Language Mod-\nels (LVLMs) have emerged[4, 7, 16, 18, 19, 25, 28, 32,\n47, 74, 77, 102, 110, 113], combining LLMs with vi-\nsion encoders [79, 89, 109] to leverage the complemen-\ntary strengths of language and vision modalities. By fusing\ntextual and visual representations, LVLMs can ground lan-\nguage in visual contexts, enabling a more comprehensive\nunderstanding and generation of multimodal content [5, 11,\n14, 20, 27, 51, 60, 95]. **LVLMs for High-Resolution Understanding. **\nLarge\nVision-Language Models (LVLMs) often employ CLIP-\nViT as the visual encoder for vision-dependent tasks. How-\never, the visual encoder\u2019s reliance on low resolutions,\nsuch as 224*\u00d7*224 or 336*\u00d7*336 pixels, limits its ef-\nfectiveness for high-resolution tasks like OCR and docu-\nment/chart perception. To enhance high-resolution under-\nstanding, recent works have primarily employed the fol-\nlowing strategies: (1) High-resolution (HR) visual encoders\nor dual encoders catering to HR and low-resolution (LR)\ninputs [36, 48, 66, 97]. For instance, Vary [97] intro-\nduces a new image encoder supporting HR inputs, which\nare then concatenated with LR embeddings from the orig-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**Large Vision-Language Models (LVLMs). **Large Lan-\nguage Models (LLMs) [6, 9, 10, 23, 39, 41, 73, 76, 78, 91\u2013\n93, 108] have gained significant attention due to their\nimpressive performance in various language-related tasks\nsuch as text generation and question answering. Follow-\ning this enthusiasm, recent Large Vision-Language Mod-\nels (LVLMs) have emerged[4, 7, 16, 18, 19, 25, 28, 32,\n47, 74, 77, 102, 110, 113], combining LLMs with vi-\nsion encoders [79, 89, 109] to leverage the complemen-\ntary strengths of language and vision modalities. By fusing\ntextual and visual representations, LVLMs can ground lan-\nguage in visual contexts, enabling a more comprehensive\nunderstanding and generation of multimodal content [5, 11,\n14, 20, 27, 51, 60, 95]. **LVLMs for High-Resolution Understanding. **\nLarge\nVision-Language Models (LVLMs) often employ CLIP-\nViT as the visual encoder for vision-dependent tasks. How-\never, the visual encoder\u2019s reliance on low resolutions,\nsuch as 224*\u00d7*224 or 336*\u00d7*336 pixels, limits its ef-\nfectiveness for high-resolution tasks like OCR and docu-\nment/chart perception. To enhance high-resolution under-\nstanding, recent works have primarily employed the fol-\nlowing strategies: (1) High-resolution (HR) visual encoders\nor dual encoders catering to HR and low-resolution (LR)\ninputs [36, 48, 66, 97]. For instance, Vary [97] intro-\nduces a new image encoder supporting HR inputs, which\nare then concatenated with LR embeddings from the orig-",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Figure 6. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Figure 6. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "open-source LVLMs and closed-source APIs. Here\nwe\nreport\nresults\nin\nDocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87],\nOCRBench[58],\nMMStar[15], MathVista[61], MMMU[107], AI2D[42],\nMME [31], MMBench (MMB) [57], MMBench-Chinese\n(MMB*CN*) [57], SEED-Bench Image Part (SEED*I*)[45],\nQBench-Testset (QBench*T*)[98],\nMM-Vet [105],\nHal-\nlusionBench (HallB)[34]. The evaluation is mainly\nconducted on the OpenCompass VLMEvalKit[24] for the\nunified reproduction of the results.<br><br>**Comparison with Closed-Source APIs. **As demonstrated\nin Table 3, IXC2-4KHD exhibits competitive performance\nacross a variety of benchmarks, rivaling that of Closed-\nSource APIs. Owing to its high-resolution input, IXC2-\n4KHD achieves a score of 90*. *0% on DocVQA and 81*. *0%\non ChartQA, thereby surpassing GPT-4V and Gemini-Pro\nwith a non-trivial margin. In the challenging Infograph-\nicVQA task, our model is the first open-source model that is\nclose to the performance of Closed-Source APIs, exceeding\nthe performance of previous open-source models by nearly\n20%. In addition to OCR-related tasks, IXC2-4KHD is a\ngeneral-purpose Large Vision-Language Modal that excels\nin semantic-level tasks, demonstrating competitive results.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "open-source LVLMs and closed-source APIs. Here\nwe\nreport\nresults\nin\nDocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87],\nOCRBench[58],\nMMStar[15], MathVista[61], MMMU[107], AI2D[42],\nMME [31], MMBench (MMB) [57], MMBench-Chinese\n(MMB*CN*) [57], SEED-Bench Image Part (SEED*I*)[45],\nQBench-Testset (QBench*T*)[98],\nMM-Vet [105],\nHal-\nlusionBench (HallB)[34]. The evaluation is mainly\nconducted on the OpenCompass VLMEvalKit[24] for the\nunified reproduction of the results.<br><br>**Comparison with Closed-Source APIs. **As demonstrated\nin Table 3, IXC2-4KHD exhibits competitive performance\nacross a variety of benchmarks, rivaling that of Closed-\nSource APIs. Owing to its high-resolution input, IXC2-\n4KHD achieves a score of 90*. *0% on DocVQA and 81*. *0%\non ChartQA, thereby surpassing GPT-4V and Gemini-Pro\nwith a non-trivial margin. In the challenging Infograph-\nicVQA task, our model is the first open-source model that is\nclose to the performance of Closed-Source APIs, exceeding\nthe performance of previous open-source models by nearly\n20%. In addition to OCR-related tasks, IXC2-4KHD is a\ngeneral-purpose Large Vision-Language Modal that excels\nin semantic-level tasks, demonstrating competitive results.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "switch to the HD-16 setting, we observe a performance gain\nof +10*.*2%. The performance continues to improve as the\nresolution increases, with saturation not observed even for\nthe 4KHD setting. Due to computational constraints, we\ndefer the exploration of the upper bound of improvement\nto future work. In terms of other OCR-related tasks, the\nperformance gain attributable to increased resolution is rel-\natively minor. For the perception-related benchmarks, per-\nformance is saturated on the resolution that only has negli-\ngible difference between the four settings. **Higher Inference Resolution Leads to better results on**\n**Text-related Tasks. **An intriguing observation from our ex-\nperiments is that our model, when inferring with a slightly\nhigher resolution, tends to yield improved results on text-\nrelated tasks. We present the results of HD-9, HD-16,\nand HD-25 in Table 6. For instance, IXC2-HD9 achieves\na 50*. *5% score on InfographicVQA. When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25. We posit that the dy-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25. We posit that the dy-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "switch to the HD-16 setting, we observe a performance gain\nof +10*.*2%. The performance continues to improve as the\nresolution increases, with saturation not observed even for\nthe 4KHD setting. Due to computational constraints, we\ndefer the exploration of the upper bound of improvement\nto future work. In terms of other OCR-related tasks, the\nperformance gain attributable to increased resolution is rel-\natively minor. For the perception-related benchmarks, per-\nformance is saturated on the resolution that only has negli-\ngible difference between the four settings. **Higher Inference Resolution Leads to better results on**\n**Text-related Tasks. **An intriguing observation from our ex-\nperiments is that our model, when inferring with a slightly\nhigher resolution, tends to yield improved results on text-\nrelated tasks. We present the results of HD-9, HD-16,\nand HD-25 in Table 6. For instance, IXC2-HD9 achieves\na 50*. *5% score on InfographicVQA. When we infer with\nHD16, we see a performance gain of +8*. *1%, without ad-\nditional training. Similar improvements are also observed\nwith IXC2-HD16 and IXC2-HD25. We posit that the dy-",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In*Proceedings of the International Conference on*\n*Machine learning (ICML)*, pages 8748\u20138763. PMLR, 2021. 2<br><br>[80] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. *arXiv preprint arXiv:2111.02114*, 2021. 6<br><br>[81] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In*European Conference on Computer Vision*,\npages 146\u2013162. Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In*Proceedings of the International Conference on*\n*Machine learning (ICML)*, pages 8748\u20138763. PMLR, 2021. 2<br><br>[80] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. *arXiv preprint arXiv:2111.02114*, 2021. 6<br><br>[81] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi. A-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In*European Conference on Computer Vision*,\npages 146\u2013162. Springer, 2022. 6<br><br>[82] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar. Kvqa: Knowledge-aware visual\nquestion answering. In*Proceedings of the AAAI conference*\n*on artificial intelligence*, 2019. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[35] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\njuan: A comprehensive multimodal dataset for advancing\nenglish and chinese large models. *ArXiv*, abs/2308.10755,\n2023. 6<br><br>[36] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\nWenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, et al. Cogagent: A visual language\nmodel for gui agents. *arXiv preprint arXiv:2312.08914*,\n2023. 2, 5, 7, 8<br><br>[37] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\nZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei\nHuang, et al. mplug-docowl 1.5: Unified structure learn-\ning for ocr-free document understanding. *arXiv preprint*\n*arXiv:2403.12895*, 2024. 2, 5, 7, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[35] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\njuan: A comprehensive multimodal dataset for advancing\nenglish and chinese large models. *ArXiv*, abs/2308.10755,\n2023. 6<br><br>[36] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu,\nWenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao\nDong, Ming Ding, et al. Cogagent: A visual language\nmodel for gui agents. *arXiv preprint arXiv:2312.08914*,\n2023. 2, 5, 7, 8<br><br>[37] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang\nZhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei\nHuang, et al. mplug-docowl 1.5: Unified structure learn-\ning for ocr-free document understanding. *arXiv preprint*\n*arXiv:2403.12895*, 2024. 2, 5, 7, 8",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[12] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and\nByungseok Roh. Honeybee: Locality-enhanced projec-\ntor for multimodal llm. *arXiv preprint arXiv:2312.06742*,\n2023. 9<br><br>[13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. *arXiv.org*, 2023. 6<br><br>[14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. *arXiv preprint arXiv:2311.12793*, 2023. 1, 2, 6<br><br>[15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, and Feng Zhao. Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024. 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024. 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[12] Junbum Cha, Wooyoung Kang, Jonghwan Mun, and\nByungseok Roh. Honeybee: Locality-enhanced projec-\ntor for multimodal llm. *arXiv preprint arXiv:2312.06742*,\n2023. 9<br><br>[13] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. *arXiv.org*, 2023. 6<br><br>[14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions. *arXiv preprint arXiv:2311.12793*, 2023. 1, 2, 6<br><br>[15] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang\nZang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao,\nDahua Lin, and Feng Zhao. Are we on the right way for\nevaluating large vision-language models? *arXiv preprint*\n*arXiv:2403.20330*, 2024. 5, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "In practice, we employ the OpenAI CLIP ViT-L-14-336\nas the vision encoder. Different from XComposer2, We\nkeep the ViT resolution as 336*\u00d7*336 and increase the input\nresolution with more patches. For the Dynamic Image Par-\ntition strategy, we use \u2018HD-25\u2019 for the pertaining. For each\nimage or patch, the image token number is decreased to 1*/*4\nwith a simple**merge operation**. We concatenate the nearby\n4 tokens into a new token through the channel dimension,\nthen align it with the LLM by an MLP. The \u2018separate\u2019 and\n\u2018*\\*n\u2019 token are randomly initialized. For the Partial LoRA,\nwe set a rank of 256 for all the linear layers in the LLM de-\ncoder block. Our training process involves a batch size of\n4096 and spans across 2 epochs. The learning rate linearly\nincreases to 2*\u00d7*10*\u2212*4 within the first 1% of the training\nsteps. Following this, it decreases to 0 according to a co-\nsine decay strategy. To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In practice, we employ the OpenAI CLIP ViT-L-14-336\nas the vision encoder. Different from XComposer2, We\nkeep the ViT resolution as 336*\u00d7*336 and increase the input\nresolution with more patches. For the Dynamic Image Par-\ntition strategy, we use \u2018HD-25\u2019 for the pertaining. For each\nimage or patch, the image token number is decreased to 1*/*4\nwith a simple**merge operation**. We concatenate the nearby\n4 tokens into a new token through the channel dimension,\nthen align it with the LLM by an MLP. The \u2018separate\u2019 and\n\u2018*\\*n\u2019 token are randomly initialized. For the Partial LoRA,\nwe set a rank of 256 for all the linear layers in the LLM de-\ncoder block. Our training process involves a batch size of\n4096 and spans across 2 epochs. The learning rate linearly\nincreases to 2*\u00d7*10*\u2212*4 within the first 1% of the training\nsteps. Following this, it decreases to 0 according to a co-\nsine decay strategy. To preserve the pre-existing knowledge\nof the vision encoder, we apply a layer-wise learning rate\n(LLDR) decay strategy, and the decay factor is set to 0*. *90.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[28] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[28] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "0.18 # 1 @ Payal # Support \u00a9 Engnering \u00a9 Faces \u00a9 Finance\n\u00a9 Sales",
            {
                "chunk_size": "small"
            }
        ],
        [
            "0.18 # 1 @ Payal # Support \u00a9 Engnering \u00a9 Faces \u00a9 Finance\n\u00a9 Sales",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[50] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important\nthings for large multi-modal models. *arXiv preprint*\n*arXiv:2311.06607*, 2023. 2, 5, 7<br><br>[51] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\nQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[50] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important\nthings for large multi-modal models. *arXiv preprint*\n*arXiv:2311.06607*, 2023. 2, 5, 7<br><br>[51] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian\nQiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[104] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengy-\ning Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. Metamath: Bootstrap your own\nmathematical questions for large language models. *arXiv*\n*preprint arXiv:2309.12284*, 2023. 6<br><br>[105] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. *arXiv preprint arXiv:2308.02490*, 2023. 7<br><br>[106] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in\nthe wild. *Journal of Computer Science and Technology*,\n34(3):509\u2013521, 2019. 6<br><br>[107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\nYang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\nWenhu Chen. Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023. 1, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[104] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengy-\ning Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian\nWeller, and Weiyang Liu. Metamath: Bootstrap your own\nmathematical questions for large language models. *arXiv*\n*preprint arXiv:2309.12284*, 2023. 6<br><br>[105] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. *arXiv preprint arXiv:2308.02490*, 2023. 7<br><br>[106] Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang\nMu, and Shi-Min Hu. A large chinese text dataset in\nthe wild. *Journal of Computer Science and Technology*,\n34(3):509\u2013521, 2019. 6<br><br>[107] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\nYang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\nWenhu Chen. Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. *arXiv preprint arXiv:2311.16502*, 2023. 1, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Model\nDoc Info Text Chart MMB MME SEED*\u2217*<br><br>HD9\n79.4 50.5 73.8\n78.2\n79.5\n2201\n76.6\n+ w/o global-view 78.1 47.9 71.2\n77.9\n75.1\n2019\n76.2<br><br>Table 7. **Influence of Global-View in the Input. **Global-view is\ncritical for most benchmarks.<br><br>namic image token length used in training enhances the ro-\nbustness of the LVLM, leading to better results when the\ntext in the image is more \u2018clear\u2019 in the higher resolution\ninput. Conversely, the results on ChartQA consistently de-\ngrade under this setting. This could be due to the model be-\ncoming confused about the chart structure when the resolu-\ntion is altered. Additionally, similar to the observation from\nFigure 5, the impact of resolution on perception-related\nbenchmarks appears to be quite minor. **Visualization Results. **We provide the visualization results\non ultra-high HD images in Figure 2 and Figure 3. Please\nrefer to the appendix for more results.<br><br>found this approach to be effective in reducing the num-\nber of image tokens efficiently. Here we study the influence\nof different token-merging strategies under the 4KHD set-\nting. In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3. High-Resolution Strategy Ablation**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3. High-Resolution Strategy Ablation**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Model\nDoc Info Text Chart MMB MME SEED*\u2217*<br><br>HD9\n79.4 50.5 73.8\n78.2\n79.5\n2201\n76.6\n+ w/o global-view 78.1 47.9 71.2\n77.9\n75.1\n2019\n76.2<br><br>Table 7. **Influence of Global-View in the Input. **Global-view is\ncritical for most benchmarks.<br><br>namic image token length used in training enhances the ro-\nbustness of the LVLM, leading to better results when the\ntext in the image is more \u2018clear\u2019 in the higher resolution\ninput. Conversely, the results on ChartQA consistently de-\ngrade under this setting. This could be due to the model be-\ncoming confused about the chart structure when the resolu-\ntion is altered. Additionally, similar to the observation from\nFigure 5, the impact of resolution on perception-related\nbenchmarks appears to be quite minor. **Visualization Results. **We provide the visualization results\non ultra-high HD images in Figure 2 and Figure 3. Please\nrefer to the appendix for more results.<br><br>found this approach to be effective in reducing the num-\nber of image tokens efficiently. Here we study the influence\nof different token-merging strategies under the 4KHD set-\nting. In Table 9, we study two additional strategies: Re-\nSampler[5] and C-Abstractor[12], with their default setting\nand the same compressing rate 0*. *25,*i.e*., reducing an im-\nage with 576 tokens to 144 tokens. Results show that both\nconcatenation and C-Abstractor work well and get similar\nresults on most benchmarks, this observation is also con-\nsistent with the study in MM-1[71] that the influence of\nthe connector is minor. However, the Re-Sampler performs\nworse than the other methods with a noticeable margin. We\nargue this is caused by the learnable queries used for gath-\nering information requiring a great number of data for train-\ning, our pre-training data is somewhat lightweight for it to\nconverge fully.<br><br>**4.3. High-Resolution Strategy Ablation**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>IXC2-VL</th><th>InernLM2-7B</th><th>55.4</th><th>57.6</th><th>81.2</th><th>1,712.0</th><th>530.7</th><th>80.7</th><th>79.4</th><th>74.9</th><th>72.5</th><th>46.7</th></tr>\n<tr><td>IXC2-VL</td><td>InernLM2-7B</td><td>55.4</td><td>57.6</td><td>81.2</td><td>1,712.0</td><td>530.7</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td></tr>\n<tr><td>IXC2-4KHD</td><td>InernLM2-7B</td><td>54.1</td><td>57.8</td><td>80.9</td><td>1,655.9</td><td>548.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>IXC2-VL</th><th>InernLM2-7B</th><th>55.4</th><th>57.6</th><th>81.2</th><th>1,712.0</th><th>530.7</th><th>80.7</th><th>79.4</th><th>74.9</th><th>72.5</th><th>46.7</th></tr>\n<tr><td>IXC2-VL</td><td>InernLM2-7B</td><td>55.4</td><td>57.6</td><td>81.2</td><td>1,712.0</td><td>530.7</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td></tr>\n<tr><td>IXC2-4KHD</td><td>InernLM2-7B</td><td>54.1</td><td>57.8</td><td>80.9</td><td>1,655.9</td><td>548.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>HD25\nHD25\nHD30</th><th>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</th></tr>\n<tr><td>HD25\nHD25\nHD30</td><td>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</td></tr>\n</table><br><br>Table 6. **Influence of Inference Resolution. **The model achieves\nbetter performance on text-related tasks when the inference reso-\nlution is higher than its training resolution.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>HD25\nHD25\nHD30</th><th>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</th></tr>\n<tr><td>HD25\nHD25\nHD30</td><td>87.0 63.6 76.0 80.3 78.5 2,209 74.9\n87.4 64.6 76.2 79.4 78.9 2,173 74.3</td></tr>\n</table><br><br>Table 6. **Influence of Inference Resolution. **The model achieves\nbetter performance on text-related tasks when the inference reso-\nlution is higher than its training resolution.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "\u201caencare |<br><br>Figure 4. **The illustration of processing high-resolution input.**<br><br>**3.2. High-Resolution Input. **<br><br>**Dynamic Image Partition. **\nUtilizing a static input im-\nage size for processing high-resolution images, particularly\nthose with varying aspect ratios, is neither efficient nor ef-\nfective. To overcome this limitation, we introduce a dy-\nnamic image partitioning approach, as shown in Figure 4. Our method strategically segments the image into smaller\npatches, while maintaining the integrity of the original im-\nage\u2019s aspect ratio. Given a maximum partition number*H*, the image*x*with\nsize [*h, w*] is resized and padded to the new image \u02c6*x*with\nsize [*ph \u00d7*336*, pw \u00d7*336]. This process is subject to the\nfollowing constraints:<br><br>*p_w  \\*t*im*e*s p_h \\leq \\mathcal {H}; \\; p_h = \\lceil p_w \\times h / w \\rceil*\n(1)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u201caencare |<br><br>Figure 4. **The illustration of processing high-resolution input.**<br><br>**3.2. High-Resolution Input. **<br><br>**Dynamic Image Partition. **\nUtilizing a static input im-\nage size for processing high-resolution images, particularly\nthose with varying aspect ratios, is neither efficient nor ef-\nfective. To overcome this limitation, we introduce a dy-\nnamic image partitioning approach, as shown in Figure 4. Our method strategically segments the image into smaller\npatches, while maintaining the integrity of the original im-\nage\u2019s aspect ratio. Given a maximum partition number*H*, the image*x*with\nsize [*h, w*] is resized and padded to the new image \u02c6*x*with\nsize [*ph \u00d7*336*, pw \u00d7*336]. This process is subject to the\nfollowing constraints:<br><br>*p_w  \\*t*im*e*s p_h \\leq \\mathcal {H}; \\; p_h = \\lceil p_w \\times h / w \\rceil*\n(1)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "a>\n@ HR @ IT @ Payroll @ Support @ Engineering @ Facilities @ Finance @ Sales\nConduct an in-depth analysis of the chart.<br><br>7)<br><br>The line chart presents the operations cost by department in US \ndollars for various categories from May 2018 to Mar 2019. The \ncategories include Human Resources (HR), Information \nTechnology (IT), Payroll, Support, Engineering, Facilities, \nFinance, and Sales. Each category is represented by a different \ncolor and line on the graph. The y-axis represents the cost in \nthousands of US dollars, while the x-axis represents the months \nfrom May 2018 to Mar 2019. The graph shows fluctuations in \nthe cost for each category throughout the months, with some \ncategories experiencing peaks and troughs at different times. 80xee\nSKS\n~\nZg\n=\n=\nee\n\u2014~\na,\n\u2014=\n\u201c=\nSS==a\nway 2018\nsut20t8\nsep 2018\nNow 2018\nJon 2019\nWar 20\nMonths<br><br>a><br><br>a\n0\nMay 2018\nMonths\nJan 2019\nMar 29\n7) Payroll has the highest cost in most cases, as indicated by \nthe purple line on the graph.<br><br>The HD image (1475 x 675)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\n@ HR @ IT @ Payroll @ Support @ Engineering @ Facilities @ Finance @ Sales\nConduct an in-depth analysis of the chart.<br><br>7)<br><br>The line chart presents the operations cost by department in US \ndollars for various categories from May 2018 to Mar 2019. The \ncategories include Human Resources (HR), Information \nTechnology (IT), Payroll, Support, Engineering, Facilities, \nFinance, and Sales. Each category is represented by a different \ncolor and line on the graph. The y-axis represents the cost in \nthousands of US dollars, while the x-axis represents the months \nfrom May 2018 to Mar 2019. The graph shows fluctuations in \nthe cost for each category throughout the months, with some \ncategories experiencing peaks and troughs at different times. 80xee\nSKS\n~\nZg\n=\n=\nee\n\u2014~\na,\n\u2014=\n\u201c=\nSS==a\nway 2018\nsut20t8\nsep 2018\nNow 2018\nJon 2019\nWar 20\nMonths<br><br>a><br><br>a\n0\nMay 2018\nMonths\nJan 2019\nMar 29\n7) Payroll has the highest cost in most cases, as indicated by \nthe purple line on the graph.<br><br>The HD image (1475 x 675)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "wid\u00a9a4\u201c<\nie\nete|\nlate<br><br>Social Media\nSlide Decks\nAlot of people have been sharing slide decks<br><br>on Instagram and Linkedin. They're a visual\nway of communicating longer messages than\na single image post. That\u2019s why we've seen<br><br>so many social justice, political and wellness\naccounts sharing slide decks. On top of that,\nthe algorithms\non Instagram and Linkedin. seem to promote these slide decks a lot\nmore thana single image.<br><br>Text Heavy Videos\nPeople will likely continue to work\nremotely well into 2021. That means\nshooting new video content is going to\nbe difficult. Videos that make use of text\non-screen to communicate messages\nare a way of getting around that.<br><br>Instead of needing a whole production\nteam to create a video, brands can\ncreate a simple text heavy video in\na fraction ofthe time.<br><br>Read the full guide: venngage.com/blog/graphic-design-trends",
            {
                "chunk_size": "small"
            }
        ],
        [
            "wid\u00a9a4\u201c<\nie\nete|\nlate<br><br>Social Media\nSlide Decks\nAlot of people have been sharing slide decks<br><br>on Instagram and Linkedin. They're a visual\nway of communicating longer messages than\na single image post. That\u2019s why we've seen<br><br>so many social justice, political and wellness\naccounts sharing slide decks. On top of that,\nthe algorithms\non Instagram and Linkedin. seem to promote these slide decks a lot\nmore thana single image.<br><br>Text Heavy Videos\nPeople will likely continue to work\nremotely well into 2021. That means\nshooting new video content is going to\nbe difficult. Videos that make use of text\non-screen to communicate messages\nare a way of getting around that.<br><br>Instead of needing a whole production\nteam to create a video, brands can\ncreate a simple text heavy video in\na fraction ofthe time.<br><br>Read the full guide: venngage.com/blog/graphic-design-trends",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Amputee\npaneer\n7<br><br>initialize\namoot\n= feat\neumemtoy O04)<br><br>goo (ins ian si44)<br><br>\u00e9\nere\nainda aren\na quuatontion\nhoe\n'\n2<br><br>3\nmort= cummed\n7\nab\nith, Yndew\n:\nund of gor WP<br><br>Se<br><br>The HD image (1888 x 3172)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Amputee\npaneer\n7<br><br>initialize\namoot\n= feat\neumemtoy O04)<br><br>goo (ins ian si44)<br><br>\u00e9\nere\nainda aren\na quuatontion\nhoe\n'\n2<br><br>3\nmort= cummed\n7\nab\nith, Yndew\n:\nund of gor WP<br><br>Se<br><br>The HD image (1888 x 3172)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br><table border=\"1\">\n<tr><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>) creme, e -\u00a7 SHES\" paw\nEncode Resize :\nSEEETON | cercemesaaes | Seale, A TREE\niliNg\nDon\u2019t send\n& Merge EDUCATE r ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW curs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc",
            {
                "chunk_size": "small"
            }
        ],
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br><table border=\"1\">\n<tr><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>) creme, e -\u00a7 SHES\" paw\nEncode Resize :\nSEEETON | cercemesaaes | Seale, A TREE\niliNg\nDon\u2019t send\n& Merge EDUCATE r ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW curs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "a>\nDetect all the word in the given image<br><br>7)\nAmerican Heart Association. VennGAGE. <br><br>How to Develop Healthy Eating Habits. And Reduce the Risk of Heart Disease. <br><br>Your best defense is a healthy lifestyle filled with physical activity and \nnutritious meals, while cutting down or avoiding drinking and smoking. <br><br>The Current Trend<br><br>75% of the US population is low in its fruit and vegetables consumption. 1/10 \nadults meets the recommendation for fruit and vegetable consumption. Men and \nyoung adults in general eat the fewest fruits and vegetables compared to any \nother group. <br><br>3 Steps to Eating Healthy 1. Add produce of various colors to your diet to \ndiversify your produce consumption. 2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nDetect all the word in the given image<br><br>7)\nAmerican Heart Association. VennGAGE. <br><br>How to Develop Healthy Eating Habits. And Reduce the Risk of Heart Disease. <br><br>Your best defense is a healthy lifestyle filled with physical activity and \nnutritious meals, while cutting down or avoiding drinking and smoking. <br><br>The Current Trend<br><br>75% of the US population is low in its fruit and vegetables consumption. 1/10 \nadults meets the recommendation for fruit and vegetable consumption. Men and \nyoung adults in general eat the fewest fruits and vegetables compared to any \nother group. <br><br>3 Steps to Eating Healthy 1. Add produce of various colors to your diet to \ndiversify your produce consumption. 2. Use hacks to incorporate more veggies \nand fruits to your meal, and throughout the day. 3. Take steps to improve access \nto fruits and vegetables for all.<br><br>1. Adding Color to Your Diet<br><br>Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables may reduce \nyour risk for cardiovascular disease, including heart attack and stroke. <br><br>Examples of a single serving of fruit: 1 medium apple, 1 small banana, 1 \nmedium avocado, 4 large strawberries. Examples of a single serving of \nvegetables: 5 - 8 broccoli florets, 2/3 large sweet potato, 6 baby carrots, 1 cup of \nraw spinach.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</th><th>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</th><th>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</th></tr>\n<tr><td>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</td><td>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</td><td>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</th><th>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</th><th>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</th></tr>\n<tr><td>Qwen-VL-Chat\nShareGPT4V\nMonkey\nCogVLM-17B\nLLaVA-XTuner\nLLaVA-1.5\nLLaVA-Next\nInternLM-XC</td><td>Qwen-7B\nVicuna-7B\nQwen-7B\nVicuna-7B\nInernLM2-20B\nVicuna-13B\nVicuna-13B\nInernLM-7B</td><td>37.5 33.8 63.0 1,487.5 360.7 60.6 56.7 58.2 61.7 47.3\n33.0 25.8 58.0 1,567.4 376.4 68.8 62.2 69.7 - 37.6\n38.3 34.8 62.5 1,522.4 401.4 72.4 67.5 68.9 - 33.0\n36.5 34.7 63.3 - - 65.8 55.9 68.8 - 54.5\n- 24.6 65.4 - - 75.1 73.7 70.2 - 37.2\n32.8 26.1 61.1 1,531.3 295.4 67.7 63.6 68.2 61.4 35.4\n38.3 32.4 72.2 1,445.0 296.0 70.0 68.5 71.4 - 44.9\n- 29.5 56.9 1,528.4 391.1 74.4 72.4 66.1 64.4 35.2</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>GPT-4V\nGemini-Pro</th><th>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</th></tr>\n<tr><td>GPT-4V\nGemini-Pro</td><td>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>GPT-4V\nGemini-Pro</th><th>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</th></tr>\n<tr><td>GPT-4V\nGemini-Pro</td><td>88.4 78.5 75.1 78.0 51.6 57.1 47.8 75.5 56.8 1,926.5 77.0 74.4 69.1 74.1 56.8 46.5\n88.1 74.1 75.2 74.6 68.0 42.6 45.8 70.2 47.9 1,933.3 73.6 74.3 70.7 70.6 59.2 45.2</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "75%\n1/10\n(*)\nMen and young adults\nof the US population is\nadults meets the\nin general eat the fewest\nlow in its fruit and\nrecommendation for fruit and\nfruits and vegetables,\nvegetables consumption. vegetable consumption. compared to any other group.<br><br>3 Steps to Eating Healthy\n1\n2\n3\n\u2018Add produce of varying colors\nUse hacks to incorporate more\nTake steps to improve\nproduce consumption.to your diet to diversify your\nveggies and fruits to your meals,\naccessto fruits and\nand throughout the day. vegetables forall",
            {
                "chunk_size": "small"
            }
        ],
        [
            "75%\n1/10\n(*)\nMen and young adults\nof the US population is\nadults meets the\nin general eat the fewest\nlow in its fruit and\nrecommendation for fruit and\nfruits and vegetables,\nvegetables consumption. vegetable consumption. compared to any other group.<br><br>3 Steps to Eating Healthy\n1\n2\n3\n\u2018Add produce of varying colors\nUse hacks to incorporate more\nTake steps to improve\nproduce consumption.to your diet to diversify your\nveggies and fruits to your meals,\naccessto fruits and\nand throughout the day. vegetables forall",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Source: https://ww.empoweredtoserve.rg/-/media/ETS-files/Community Resources/Health-Lessons\nAmgen Branded/Fruits-and-Veggies/DS18477_ETS_Amgen_Presentation_FruitVeg_OTkk_\nNOTES. pdf<br><br>The HD image (816 x 3813)<br><br>Figure 7. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.<br><br>16",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Source: https://ww.empoweredtoserve.rg/-/media/ETS-files/Community Resources/Health-Lessons\nAmgen Branded/Fruits-and-Veggies/DS18477_ETS_Amgen_Presentation_FruitVeg_OTkk_\nNOTES. pdf<br><br>The HD image (816 x 3813)<br><br>Figure 7. **Chat with InternLM-XComposer2-4KHD**. Please refer to the link for better visualization.<br><br>16",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In*ICDAR*, 2019. 6<br><br>[73] OpenAI. Chatgpt. https://openai.com/blog/\nchatgpt, 2022. 1, 2<br><br>[74] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[75] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text:\nDescribing images using 1 million captioned\nphotographs. In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730\u201327744, 2022. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730\u201327744, 2022. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[72] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and\nAnirban Chakraborty. Ocr-vqa: Visual question answering\nby reading text in images. In*ICDAR*, 2019. 6<br><br>[73] OpenAI. Chatgpt. https://openai.com/blog/\nchatgpt, 2022. 1, 2<br><br>[74] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[75] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text:\nDescribing images using 1 million captioned\nphotographs. In*Neural Information Processing Systems*\n*(NIPS)*, 2011. 6<br><br>[76] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. *Advances in Neural Information Processing Systems*\n*(NeurIPS)*, 35:27730\u201327744, 2022. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Overall, the infographic provides a \ncomprehensive overview of the 7 graphic design \ntrends that are expected to shape the design \nlandscape in 2021.<br><br>=\nWhat is the detailed explanation of the \nthird part?<br><br>*<br><br>The third part of the image is titled \"Geometric \nShapes Everywhere\". It explains that last year, \ndesigners used a lot of flowing and abstract \nshapes in their designs. However, this year, they \nhave been replaced with rigid, hard-edged \ngeometric shapes and patterns. The hard edges of \na geometric shape create a great contrast against \nmuted colors.<br><br>\u00ae\n(@)<br><br>=<br><br>*\nYou can find the full guide on \nvennage.com/blog/graphic-design-trends.<br><br>The HD image (816 x 5133)<br><br>Figure 3. **Chat with InternLM-XComposer2-4KHD on ultra-high HD image with the 816***\u00d7***5133 resolution**. Some regions of the\ninput HD images are zoomed in for better visualization.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Overall, the infographic provides a \ncomprehensive overview of the 7 graphic design \ntrends that are expected to shape the design \nlandscape in 2021.<br><br>=\nWhat is the detailed explanation of the \nthird part?<br><br>*<br><br>The third part of the image is titled \"Geometric \nShapes Everywhere\". It explains that last year, \ndesigners used a lot of flowing and abstract \nshapes in their designs. However, this year, they \nhave been replaced with rigid, hard-edged \ngeometric shapes and patterns. The hard edges of \na geometric shape create a great contrast against \nmuted colors.<br><br>\u00ae\n(@)<br><br>=<br><br>*\nYou can find the full guide on \nvennage.com/blog/graphic-design-trends.<br><br>The HD image (816 x 5133)<br><br>Figure 3. **Chat with InternLM-XComposer2-4KHD on ultra-high HD image with the 816***\u00d7***5133 resolution**. Some regions of the\ninput HD images are zoomed in for better visualization.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[63] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal rea-\nsoning via thought chains for science question answer-\ning. *Advances in Neural Information Processing Systems*,\n35:2507\u20132521, 2022. 6<br><br>[64] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning. *arXiv preprint*",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[63] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan. Learn to explain: Multimodal rea-\nsoning via thought chains for science question answer-\ning. *Advances in Neural Information Processing Systems*,\n35:2507\u20132521, 2022. 6<br><br>[64] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-\nChun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin\nKalyan. Dynamic prompt learning via policy gradient for\nsemi-structured mathematical reasoning. *arXiv preprint*",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "github.com/InternLM/InternLM, 2023. 1, 2, 6<br><br>[92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth \u0301ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. *arXiv.org*, 2023.<br><br>[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models,\n2023. 1, 2<br><br>[94] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. *arXiv preprint*\n*arXiv:2311.07574*, 2023. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "github.com/InternLM/InternLM, 2023. 1, 2, 6<br><br>[92] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth \u0301ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. *arXiv.org*, 2023.<br><br>[93] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models,\n2023. 1, 2<br><br>[94] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. *arXiv preprint*\n*arXiv:2311.07574*, 2023. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[111] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\nLi, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\nMingkun Yang, et al. Icdar 2019 robust reading challenge\non reading chinese text on signboard. In*2019 international*\n*conference on document analysis and recognition (ICDAR)*,\npages 1577\u20131581. IEEE, 2019. 6<br><br>[112] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: En-\nhanced visual instruction tuning for text-rich image under-\nstanding. *arXiv preprint arXiv:2306.17107*, 2023. 5<br><br>[113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. Minigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language\nmodels. *arXiv.org*, 2023. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[111] Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan\nLi, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao,\nMingkun Yang, et al. Icdar 2019 robust reading challenge\non reading chinese text on signboard. In*2019 international*\n*conference on document analysis and recognition (ICDAR)*,\npages 1577\u20131581. IEEE, 2019. 6<br><br>[112] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou,\nNedim Lipka, Diyi Yang, and Tong Sun. LLaVAR: En-\nhanced visual instruction tuning for text-rich image under-\nstanding. *arXiv preprint arXiv:2306.17107*, 2023. 5<br><br>[113] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. Minigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language\nmodels. *arXiv.org*, 2023. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In*International*\n*Conference on Computer Vision (ICCV)*, 2015. 6<br><br>[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt. Openflamingo:\nAn open-\nsource framework for training large autoregressive vision-\nlanguage models. *arXiv.org*, 2023. 2<br><br>[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. *arXiv.org*, 2023. 2, 9",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In*International*\n*Conference on Computer Vision (ICCV)*, 2015. 6<br><br>[4] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt. Openflamingo:\nAn open-\nsource framework for training large autoregressive vision-\nlanguage models. *arXiv.org*, 2023. 2<br><br>[5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. *arXiv.org*, 2023. 2, 9",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Find ways to spread the word about nutrition assistance programs, such as \nSNAP, WIC and school meals. <br><br>Create a petition for more variety, improve affordability and advocate for better \nsignage/placement. <br><br>Meet with an after-school or daycare program representative to discuss serving \nmore fruits and vegetables for snacks. Organize a letter-writing campaign and \nset up a meeting with state leaders. <br><br>For example, ask for funding to host a farmers market in an underserved \ncommunity. Sign up for \"You're the Cure\" and send a note to your \nCongressperson advocating for healthier meals at school. <br><br>It's American Heart Month! Share this infographic with your family, friends and \nnetwork today. You can carry out all of these actions or just a few. But \nwhichever actions you choose, they pave the way for greater access to nutritious \nfood for your community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Find ways to spread the word about nutrition assistance programs, such as \nSNAP, WIC and school meals. <br><br>Create a petition for more variety, improve affordability and advocate for better \nsignage/placement. <br><br>Meet with an after-school or daycare program representative to discuss serving \nmore fruits and vegetables for snacks. Organize a letter-writing campaign and \nset up a meeting with state leaders. <br><br>For example, ask for funding to host a farmers market in an underserved \ncommunity. Sign up for \"You're the Cure\" and send a note to your \nCongressperson advocating for healthier meals at school. <br><br>It's American Heart Month! Share this infographic with your family, friends and \nnetwork today. You can carry out all of these actions or just a few. But \nwhichever actions you choose, they pave the way for greater access to nutritious \nfood for your community.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "\u2018Adding a variety of produce to your diet has a number of health benefits. It'll help you:<br><br>Manage your weight\nControl your blood pressure\nSupport healthy digestion<br><br>Reduce the risk of some\nReduce the risk of chronic health\ncancers, such as colon cancer\nproblems, such as diabetes",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u2018Adding a variety of produce to your diet has a number of health benefits. It'll help you:<br><br>Manage your weight\nControl your blood pressure\nSupport healthy digestion<br><br>Reduce the risk of some\nReduce the risk of chronic health\ncancers, such as colon cancer\nproblems, such as diabetes",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**1. Introduction**<br><br>In recent years, the progress in Large Language Models\n(LLMs) [10, 21, 29, 39, 73, 78, 91\u201393] has provoked the\ndevelopment of Large Vision-Language Models (LVLMs). These models have demonstrated proficiency in tasks such\nas image captioning [14, 17] and visual-question-answering\n(VQA) [31, 33, 57, 107]. Nevertheless, due to their limited\nresolution, they struggle with processing images containing\nfine details, such as charts [68], tables [87], documents [70],\nand infographics [69]. This limitation constrains their prac-",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. Introduction**<br><br>In recent years, the progress in Large Language Models\n(LLMs) [10, 21, 29, 39, 73, 78, 91\u201393] has provoked the\ndevelopment of Large Vision-Language Models (LVLMs). These models have demonstrated proficiency in tasks such\nas image captioning [14, 17] and visual-question-answering\n(VQA) [31, 33, 57, 107]. Nevertheless, due to their limited\nresolution, they struggle with processing images containing\nfine details, such as charts [68], tables [87], documents [70],\nand infographics [69]. This limitation constrains their prac-",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth \u0301ee Lacroix, and William El Sayed. Mistral 7b, 2023. 1, 2<br><br>[40] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. Dvqa: Understanding data visualizations via ques-\ntion answering. In*Proceedings of the IEEE conference on*\n*computer vision and pattern recognition*, pages 5648\u20135656,\n2018. 6<br><br>[41] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. *arXiv preprint arXiv:2001.08361*,\n2020. 2<br><br>[42] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In*Computer Vision\u2013ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11\u201314, 2016, Proceedings, Part IV 14*, pages 235\u2013\n251. Springer, 2016. 6, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In*Computer Vision\u2013ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11\u201314, 2016, Proceedings, Part IV 14*, pages 235\u2013\n251. Springer, 2016. 6, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth \u0301ee Lacroix, and William El Sayed. Mistral 7b, 2023. 1, 2<br><br>[40] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. Dvqa: Understanding data visualizations via ques-\ntion answering. In*Proceedings of the IEEE conference on*\n*computer vision and pattern recognition*, pages 5648\u20135656,\n2018. 6<br><br>[41] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. *arXiv preprint arXiv:2001.08361*,\n2020. 2<br><br>[42] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In*Computer Vision\u2013ECCV 2016:*\n*14th European Conference, Amsterdam, The Netherlands,*\n*October 11\u201314, 2016, Proceedings, Part IV 14*, pages 235\u2013\n251. Springer, 2016. 6, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Figure 2. **Chat with InternLM-XComposer2-4KHD**. Some regions of the input HD images are zoomed in for better visualization.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Figure 2. **Chat with InternLM-XComposer2-4KHD**. Some regions of the input HD images are zoomed in for better visualization.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "\u00a7} ShareGPT4\u00a5: Improving Large Multi-Modal Models with Better Captions<br><br>Lin Chen**! ?, Jinsong Li*\u00ae?, Xiaoyi Dong?, Pan Zhang\u201d, Conghui He?, Jiaqi Wang?,\nFeng Zhao! ', Dahua Lin\u2019? 1University of Science and Technology of China\nShanghai AI Laboratory",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u00a7} ShareGPT4\u00a5: Improving Large Multi-Modal Models with Better Captions<br><br>Lin Chen**! ?, Jinsong Li*\u00ae?, Xiaoyi Dong?, Pan Zhang\u201d, Conghui He?, Jiaqi Wang?,\nFeng Zhao! ', Dahua Lin\u2019? 1University of Science and Technology of China\nShanghai AI Laboratory",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**3. Method**<br><br>**3.1. Model Architecture. **<br><br>The model architecture of InternLM-XComposer2-4KHD\nmainly follows the design of InternLM-XComposer2[27]\n(XComposer2 in the following for simplicity. ), including a\nlight-weight Vision Encoder OpenAI ViT-Large/14, Large\nLanguage Model InternLM2-7B, and Partial LoRA for effi-\ncient alignment. We recommend the readers to the XCom-\nposer2 paper for more details.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3. Method**<br><br>**3.1. Model Architecture. **<br><br>The model architecture of InternLM-XComposer2-4KHD\nmainly follows the design of InternLM-XComposer2[27]\n(XComposer2 in the following for simplicity. ), including a\nlight-weight Vision Encoder OpenAI ViT-Large/14, Large\nLanguage Model InternLM2-7B, and Partial LoRA for effi-\ncient alignment. We recommend the readers to the XCom-\nposer2 paper for more details.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "And Reduce the Risk of Heart Disease\na\n. *\n%\nw\nRs\n=\n~~\nY<br><br>Your best defense is a healthy lifestyle filled with physical activity and\nnutritious meals, while cutting down or avoiding drinking and smoking.<br><br>The Current Trend",
            {
                "chunk_size": "small"
            }
        ],
        [
            "And Reduce the Risk of Heart Disease\na\n. *\n%\nw\nRs\n=\n~~\nY<br><br>Your best defense is a healthy lifestyle filled with physical activity and\nnutritious meals, while cutting down or avoiding drinking and smoking.<br><br>The Current Trend",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "a>\nWhat is the algorithm of this flow chart?<br><br>7)\nThe flow chart in the image is an algorithm for finding the maximum element in an array. Here is a \ndetailed explanation of the algorithm:<br><br>1. Declare an integer array having size m.\n2. Input the m numbers into the array. 3. Initialize max as the first element of the array. 4. For each element in the array, starting from the second element:\nIf the current element is greater than max, set max to the current element. 5. At the end of the loop, max will hold the maximum element in the array. 6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \nelement is stored in the variable max, and it is then outputted",
            {
                "chunk_size": "small"
            }
        ],
        [
            "6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \nelement is stored in the variable max, and it is then outputted",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nWhat is the algorithm of this flow chart?<br><br>7)\nThe flow chart in the image is an algorithm for finding the maximum element in an array. Here is a \ndetailed explanation of the algorithm:<br><br>1. Declare an integer array having size m.\n2. Input the m numbers into the array. 3. Initialize max as the first element of the array. 4. For each element in the array, starting from the second element:\nIf the current element is greater than max, set max to the current element. 5. At the end of the loop, max will hold the maximum element in the array. 6. Output the maximum element.<br><br>This algorithm iterates through the array, comparing each element to the current maximum. If a \nlarger element is found, it becomes the new maximum. At the end of the loop, the maximum \nelement is stored in the variable max, and it is then outputted",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Adding a variety of produce to your diet has a number of health benefits. It \nhelps you: Manage your weight, Control your blood pressure, Support healthy \ndigestion, Reduce the risk of some cancers, such as colon cancer, Reduce the \nrisk of chronic health problems, such as diabetes. <br><br>2. Use Healthy Eating Hacks <br><br>1. Fresh, frozen, canned and dried all count toward your daily servings, so you \nhave plenty of options. 2. Always reserve half of your plate for fruits and \nvegetables. 3. Choose whole fruits and vegetables. 4. Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Adding a variety of produce to your diet has a number of health benefits. It \nhelps you: Manage your weight, Control your blood pressure, Support healthy \ndigestion, Reduce the risk of some cancers, such as colon cancer, Reduce the \nrisk of chronic health problems, such as diabetes. <br><br>2. Use Healthy Eating Hacks <br><br>1. Fresh, frozen, canned and dried all count toward your daily servings, so you \nhave plenty of options. 2. Always reserve half of your plate for fruits and \nvegetables. 3. Choose whole fruits and vegetables. 4. Try to eat a variety of \nvegetables, instead of the same thing all the time. 5. Look for fruit packed in its \nown fruit juice and 100% vegetable juice, with no added sugar. 6. Look for \nlow/no-sodium options for canned vegetables, and 100% vegetable juice. <br><br>3. Take Action for Better Access <br><br>In many communities, fruits and vegetables are limited/expensive and only \navailable in corner stores, convenience stores, bodegas and gas stations. <br><br>Lack of access can profoundly impact diet. Here's how: Contact your city and \nstate leaders to let them know what food access is in your community.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[18] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut. Pali-3 vision language models: Smaller, faster,\nstronger, 2023. 2<br><br>[19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[18] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut. Pali-3 vision language models: Smaller, faster,\nstronger, 2023. 2<br><br>[19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>)\ncreme, e -\u00a7\nSHES\" paw\n:\nSEEETON | cercemesaaes | Seale, A TREE\nili Ng\nDon\u2019t send\nEDUCATE\nr ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW\ncurs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc<br><br>Dynamic Image \nPartition\nR PLANET STARTS PROTECTING OU<br><br><table border=\"1\">\n<tr><th></th><th></th><th></th><th></th><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>Dalve vess\n\u00a2 Freocte<br><br>Encode\nWA\n= ane\n& Merge<br><br>ae<br><br>ae\n\u00ab(1\nEDUCATE.<br><br>es \u00e9nour\nee Y\nVolunteer! | 2\nin",
            {
                "chunk_size": "small"
            }
        ],
        [
            "PROTECTING OUR PLANET STARTS WITH\nad<br><br>BIKE MORE | @B reduce\n{choose sustainable\u2019 rueyon\neR | Gi recy\n=oGeil\noy'>)\ncreme, e -\u00a7\nSHES\" paw\n:\nSEEETON | cercemesaaes | Seale, A TREE\nili Ng\nDon\u2019t send\nEDUCATE\nr ee A=\nSiocon\n|=\nae\nmoet\n=\nTavcinoeeel\nee aren\nom\nnd ee. WW\ncurs. | joes\na\naes\n=a\neen\na\n@ IDEA Secs Vv\nrc<br><br>Dynamic Image \nPartition\nR PLANET STARTS PROTECTING OU<br><br><table border=\"1\">\n<tr><th></th><th></th><th></th><th></th><th></th><th></th><th>\\n</th></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n<tr><td> </td><td> </td><td> </td><td> </td><td> </td><td> </td><td>\\n</td></tr>\n</table><br><br>Dalve vess\n\u00a2 Freocte<br><br>Encode\nWA\n= ane\n& Merge<br><br>ae<br><br>ae\n\u00ab(1\nEDUCATE.<br><br>es \u00e9nour\nee Y\nVolunteer! | 2\nin",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Chen, et al. Sphinx: The joint mixing of weights, tasks, and\nvisual embeddings for multi-modal large language models. *arXiv preprint arXiv:2311.07575*, 2023. 2, 5<br><br>[52] Adam Dahlgren Lindstr \u0308om and Savitha Sam Abra-\nham. Clevr-math:\nA dataset for compositional lan-\nguage, visual and mathematical reasoning. *arXiv preprint*\n*arXiv:2208.05358*, 2022. 6<br><br>[53] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. *Transactions of the Association*\n*for Computational Linguistics*, 2023. 6<br><br>[54] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu. Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024. 2, 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024. 2, 5, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Chen, et al. Sphinx: The joint mixing of weights, tasks, and\nvisual embeddings for multi-modal large language models. *arXiv preprint arXiv:2311.07575*, 2023. 2, 5<br><br>[52] Adam Dahlgren Lindstr \u0308om and Savitha Sam Abra-\nham. Clevr-math:\nA dataset for compositional lan-\nguage, visual and mathematical reasoning. *arXiv preprint*\n*arXiv:2208.05358*, 2022. 6<br><br>[53] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. *Transactions of the Association*\n*for Computational Linguistics*, 2023. 6<br><br>[54] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu. Mmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning. *arXiv preprint*\n*arXiv:2311.10774*, 2023. 6<br><br>[55] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan\nZhang, Sheng Shen, and Yong Jae Lee. Llava-next: Im-\nproved reasoning, ocr, and world knowledge, January 2024. 2, 5, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2023. 2<br><br>[20] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and\nJifeng Dai. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. *arXiv*\n*preprint arXiv:2312.14238*, 2023. 2, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2023. 2<br><br>[20] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo\nChen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou\nZhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and\nJifeng Dai. Internvl: Scaling up vision foundation mod-\nels and aligning for generic visual-linguistic tasks. *arXiv*\n*preprint arXiv:2312.14238*, 2023. 2, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**References**<br><br>[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. Nocaps: Novel object\ncaptioning at scale. In*Proceedings of the IEEE/CVF inter-*\n*national conference on computer vision*, pages 8948\u20138957,\n2019. 6<br><br>[2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,\nGe Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jian-\nqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu,\nShawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen\nXie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yux-\nuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi:\nOpen foundation models by 01.ai, 2024. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**References**<br><br>[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. Nocaps: Novel object\ncaptioning at scale. In*Proceedings of the IEEE/CVF inter-*\n*national conference on computer vision*, pages 8948\u20138957,\n2019. 6<br><br>[2] 01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,\nGe Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jian-\nqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu,\nShawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen\nXie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu,\nPengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yux-\nuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi:\nOpen foundation models by 01.ai, 2024. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Table 3. **Comparison with closed-source APIs and previous open-source SOTAs. **Our InternLM-XComposer2-4KHD gets SOTA\nresults in 6 of the 16 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs. The best results\nare**bold**and the second-best results are underlined.<br><br><table border=\"1\">\n<tr><th>Method</th><th>LLM</th><th>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</th></tr>\n<tr><td>Method</td><td>LLM</td><td>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Table 3. **Comparison with closed-source APIs and previous open-source SOTAs. **Our InternLM-XComposer2-4KHD gets SOTA\nresults in 6 of the 16 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs. The best results\nare**bold**and the second-best results are underlined.<br><br><table border=\"1\">\n<tr><th>Method</th><th>LLM</th><th>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</th></tr>\n<tr><td>Method</td><td>LLM</td><td>MMStar MathVista AI2D MMEP MMEC MMB MMBCN SEEDI QBenchT MM-Vet</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[99] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\nNi, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\nSun, and Gao Huang. Llava-uhd: an lmm perceiving any\naspect ratio and high-resolution images. *arXiv preprint*\n*arXiv:2403.11703*, 2024. 2, 5, 8<br><br>[100] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Jun-\nfeng Tian, et al. mPLUG-DocOwl: Modularized multi-\nmodal large language model for document understanding. *arXiv preprint arXiv:2307.02499*, 2023. 5<br><br>[101] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming\nYan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji\nZhang, et al. Ureader: Universal ocr-free visually-situated\nlanguage understanding with multimodal large language\nmodel. *arXiv preprint arXiv:2310.05126*, 2023. 5, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[99] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin\nNi, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, Maosong\nSun, and Gao Huang. Llava-uhd: an lmm perceiving any\naspect ratio and high-resolution images. *arXiv preprint*\n*arXiv:2403.11703*, 2024. 2, 5, 8<br><br>[100] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Jun-\nfeng Tian, et al. mPLUG-DocOwl: Modularized multi-\nmodal large language model for document understanding. *arXiv preprint arXiv:2307.02499*, 2023. 5<br><br>[101] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming\nYan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji\nZhang, et al. Ureader: Universal ocr-free visually-situated\nlanguage understanding with multimodal large language\nmodel. *arXiv preprint arXiv:2310.05126*, 2023. 5, 8",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[77] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world. *arXiv.org*,\n2023. 2<br><br>[78] Qwen. Introducing qwen-7b: Open foundation and human-\naligned models (of the state-of-the-arts), 2023. 1, 2<br><br>[79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[77] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world. *arXiv.org*,\n2023. 2<br><br>[78] Qwen. Introducing qwen-7b: Open foundation and human-\naligned models (of the state-of-the-arts), 2023. 1, 2<br><br>[79] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,\nYang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia\nGuo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang,\nTao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiax-\ning Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\ning Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-\nwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun\nLv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang\nNing, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,\nYunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[10] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu\nChen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei\nChu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei,\nYang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia\nGuo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang,\nTao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiax-\ning Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yin-\ning Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kai-\nwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun\nLv, Kai Lv, Li Ma, Runyuan Ma, Zerun Ma, Wenchang\nNing, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang,\nYunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%*chatgpt quality, March 2023. 1, 6<br><br>[22] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\nNg, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\nZhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\ning challenge on arbitrary-shaped text-rrc-art. In*2019 In-*\n*ternational Conference on Document Analysis and Recog-*\n*nition (ICDAR)*, pages 1571\u20131576. IEEE, 2019. 6<br><br>[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. *arXiv.org*, 2022. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[21] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%*chatgpt quality, March 2023. 1, 6<br><br>[22] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet\nNg, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao\nZhang, Junyu Han, Errui Ding, et al. Icdar2019 robust read-\ning challenge on arbitrary-shaped text-rrc-art. In*2019 In-*\n*ternational Conference on Document Analysis and Recog-*\n*nition (ICDAR)*, pages 1571\u20131576. IEEE, 2019. 6<br><br>[23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. *arXiv.org*, 2022. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Train Eval</th><th>Doc Info Text Chart MMB MME SEED\u2217</th></tr>\n<tr><td>Train Eval</td><td>Doc Info Text Chart MMB MME SEED\u2217</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD9\nHD9\nHD16</th><th>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</th></tr>\n<tr><td>HD9\nHD9\nHD16</td><td>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD16\nHD16\nHD25</th><th>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</th></tr>\n<tr><td>HD16\nHD16\nHD25</td><td>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Train Eval</th><th>Doc Info Text Chart MMB MME SEED\u2217</th></tr>\n<tr><td>Train Eval</td><td>Doc Info Text Chart MMB MME SEED\u2217</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD9\nHD9\nHD16</th><th>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</th></tr>\n<tr><td>HD9\nHD9\nHD16</td><td>79.4 50.5 73.8 78.2 79.5 2,201 76.6\n83.0 58.6 74.3 75.8 79.3 2,198 76.7</td></tr>\n</table><br><br><table border=\"1\">\n<tr><th>HD16\nHD16\nHD25</th><th>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</th></tr>\n<tr><td>HD16\nHD16\nHD25</td><td>84.9 60.8 75.7 80.1 80.2 2,129 75.7\n85.9 62.1 75.8 79.1 80.1 2,100 75.4</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Gemini-Pro</th><th>88.1</th><th>74.1</th><th>75.2</th><th>74.6</th><th>68.0</th><th>42.6</th><th>45.8</th><th>70.2</th><th>47.9</th><th>1,933.3</th><th>73.6</th><th>74.3</th><th>70.7</th><th>70.6</th><th>59.2</th><th>45.2</th></tr>\n<tr><td>IXC2-VL</td><td>57.7</td><td>72.6</td><td>34.4</td><td>70.1</td><td>53.2</td><td>55.4</td><td>57.6</td><td>81.2</td><td>41.4</td><td>2,220.4</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td><td>41.0</td></tr>\n<tr><td>IXC2-4KHD</td><td>90.0</td><td>81.0</td><td>68.6</td><td>77.2</td><td>67.5</td><td>54.1</td><td>57.8</td><td>80.9</td><td>39.7</td><td>2,204.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td><td>40.9</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Gemini-Pro</th><th>88.1</th><th>74.1</th><th>75.2</th><th>74.6</th><th>68.0</th><th>42.6</th><th>45.8</th><th>70.2</th><th>47.9</th><th>1,933.3</th><th>73.6</th><th>74.3</th><th>70.7</th><th>70.6</th><th>59.2</th><th>45.2</th></tr>\n<tr><td>IXC2-VL</td><td>57.7</td><td>72.6</td><td>34.4</td><td>70.1</td><td>53.2</td><td>55.4</td><td>57.6</td><td>81.2</td><td>41.4</td><td>2,220.4</td><td>80.7</td><td>79.4</td><td>74.9</td><td>72.5</td><td>46.7</td><td>41.0</td></tr>\n<tr><td>IXC2-4KHD</td><td>90.0</td><td>81.0</td><td>68.6</td><td>77.2</td><td>67.5</td><td>54.1</td><td>57.8</td><td>80.9</td><td>39.7</td><td>2,204.9</td><td>80.2</td><td>77.7</td><td>74.7</td><td>71.8</td><td>54.9</td><td>40.9</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[16] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali-x: On scaling up a multilingual vision\nand language model, 2023. 2<br><br>[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server, 2015. 1, 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[16] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali-x: On scaling up a multilingual vision\nand language model, 2023. 2<br><br>[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server, 2015. 1, 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "multi-modal benchmarks. This. project\nis available at\nhttps: //ShareGPT4V. github. io to serve asa",
            {
                "chunk_size": "small"
            }
        ],
        [
            "multi-modal benchmarks. This. project\nis available at\nhttps: //ShareGPT4V. github. io to serve asa",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "& shareGPTWV: Improving Large Mul-Modal Me with BeterCaptions\nsca onme ee\nnye\nnc gg\naa aay<br><br>Zz\ni\n\u2014\u2014\u2014 ee\nS\nES\nSSeeeass) 4oes<br><br>5 spmamilcamees ene\nvoemooee",
            {
                "chunk_size": "small"
            }
        ],
        [
            "& shareGPTWV: Improving Large Mul-Modal Me with BeterCaptions\nsca onme ee\nnye\nnc gg\naa aay<br><br>Zz\ni\n\u2014\u2014\u2014 ee\nS\nES\nSSeeeass) 4oes<br><br>5 spmamilcamees ene\nvoemooee",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "up to 4K resolution, the model can achieve additional\nperformance improvements during inference by processing\nimages at higher resolutions. Furthermore, scaling the training resolution up to 4K\nstandard results in a consistent improvement in perfor-\nmance, highlighting the potential for training even beyond\n4K resolution. This underscores the capacity for further en-\nhancing model capabilities and suggests a promising tra-\njectory for advancing the frontiers of high-resolution image\nprocessing within the domain of large vision-language mod-\nels. We evaluate our InternLM-XComposer2-4KHD on\n16 diverse benchmarks spanning various domains, in-\ncluding 5 challenging HD-OCR datasets (DocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87] and\nOCRBench[58]). Compared to previous open-source\nLVLM models and closed-source APIs, our approach\nachieves SOTA results in 6 of 16 benchmarks, demon-\nstrating competitive performance despite only 7B parame-\nters. As shown in Figure 1, InternLM-XComposer2-4KHD\neven surpasses the performance of GPT4V [74] and Gemini\nPro [90] across ten benchmarks. Notably, our method ex-\nhibits excellent performance on 5 HD-OCR datasets, over\nexisting open-source LVLMs by a substantial margin.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "up to 4K resolution, the model can achieve additional\nperformance improvements during inference by processing\nimages at higher resolutions. Furthermore, scaling the training resolution up to 4K\nstandard results in a consistent improvement in perfor-\nmance, highlighting the potential for training even beyond\n4K resolution. This underscores the capacity for further en-\nhancing model capabilities and suggests a promising tra-\njectory for advancing the frontiers of high-resolution image\nprocessing within the domain of large vision-language mod-\nels. We evaluate our InternLM-XComposer2-4KHD on\n16 diverse benchmarks spanning various domains, in-\ncluding 5 challenging HD-OCR datasets (DocVQA[70],\nChartQA[68],\nInfographicVQA[69],\nTextVQA[87] and\nOCRBench[58]). Compared to previous open-source\nLVLM models and closed-source APIs, our approach\nachieves SOTA results in 6 of 16 benchmarks, demon-\nstrating competitive performance despite only 7B parame-\nters. As shown in Figure 1, InternLM-XComposer2-4KHD\neven surpasses the performance of GPT4V [74] and Gemini\nPro [90] across ten benchmarks. Notably, our method ex-\nhibits excellent performance on 5 HD-OCR datasets, over\nexisting open-source LVLMs by a substantial margin.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "\u2019 Recovery\n&\ntime<br><br>Finally, the infographic emphasizes the \nimportance of recovery time, as contact displays \nno symptoms of coronavirus (COVID-19) the \npatient makes a full recovery, they can go back \nto daily lives as normal and pose no risk to \nothers. The HD image (986 x 2560)<br><br>@\nContact displays no symptoms\nof coronavirus (COVID-19)\nthe patient makesa full recovery,\nthey can go back to daily lives as\nnormal and pose no risk to others",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\u2019 Recovery\n&\ntime<br><br>Finally, the infographic emphasizes the \nimportance of recovery time, as contact displays \nno symptoms of coronavirus (COVID-19) the \npatient makes a full recovery, they can go back \nto daily lives as normal and pose no risk to \nothers. The HD image (986 x 2560)<br><br>@\nContact displays no symptoms\nof coronavirus (COVID-19)\nthe patient makesa full recovery,\nthey can go back to daily lives as\nnormal and pose no risk to others",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "DocVQA\n**Abstract** OCRBench ChartQA<br><br>81.0\n90.0 67.5\nMathVista TextVQA<br><br>57.8\n77.2<br><br>AI2D InfoVQA\n80.9\n68.6<br><br>2204.9 74.7\nMME\nSEEDI<br><br>71.8\n77.7<br><br>QBenchT MMBenchCN\n54.9 39.9<br><br>40.9 80.2<br><br>MM-Vet MMBench\n54.1<br><br>HallB MMMU\nMMStar<br><br>GPT-4V\nGemini-Pro\nIXC2-4KHD\nPrevious open-source SOTA",
            {
                "chunk_size": "small"
            }
        ],
        [
            "DocVQA\n**Abstract** OCRBench ChartQA<br><br>81.0\n90.0 67.5\nMathVista TextVQA<br><br>57.8\n77.2<br><br>AI2D InfoVQA\n80.9\n68.6<br><br>2204.9 74.7\nMME\nSEEDI<br><br>71.8\n77.7<br><br>QBenchT MMBenchCN\n54.9 39.9<br><br>40.9 80.2<br><br>MM-Vet MMBench\n54.1<br><br>HallB MMMU\nMMStar<br><br>GPT-4V\nGemini-Pro\nIXC2-4KHD\nPrevious open-source SOTA",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "a>\nGive an in-depth description of the visual.<br><br>7)<br><br>The image is an infographic from Public Health \nEngland that explains the concept of contact \ntracing in the context of preventing the spread of \ncoronavirus (COVID-19). Contact tracing is a \nfundamental part of outbreak control used by \npublic health professionals around the world to \nprevent the spread of infections.<br><br>The infographic is divided into several sections, \neach providing a different aspect of the contact \ntracing process. The first section explains that a \ncontact is assessed to see how close they were to \na confirmed case, which could include someone \nliving with the case, or someone who has been \nin direct contact with them or their bodily fluids. Following this assessment, the contacts are \ncategorised into low or high risk and provided \nwith advice on what they should do.<br><br>The second section explains that if a contact is at \nhigh risk of infection, they may be asked to self-\nisolate, remaining in their home for 14 days and \nstaying away from work, school or public \nplaces. If a contact is at low risk, they don't \nrequire self-isolation.<br><br>The third section explains that if the person \ndevelops symptoms, they would be tested and \nprovided with specialist care if they have the \nnovel coronavirus (COVID-19).",
            {
                "chunk_size": "small"
            }
        ],
        [
            "a>\nGive an in-depth description of the visual.<br><br>7)<br><br>The image is an infographic from Public Health \nEngland that explains the concept of contact \ntracing in the context of preventing the spread of \ncoronavirus (COVID-19). Contact tracing is a \nfundamental part of outbreak control used by \npublic health professionals around the world to \nprevent the spread of infections.<br><br>The infographic is divided into several sections, \neach providing a different aspect of the contact \ntracing process. The first section explains that a \ncontact is assessed to see how close they were to \na confirmed case, which could include someone \nliving with the case, or someone who has been \nin direct contact with them or their bodily fluids. Following this assessment, the contacts are \ncategorised into low or high risk and provided \nwith advice on what they should do.<br><br>The second section explains that if a contact is at \nhigh risk of infection, they may be asked to self-\nisolate, remaining in their home for 14 days and \nstaying away from work, school or public \nplaces. If a contact is at low risk, they don't \nrequire self-isolation.<br><br>The third section explains that if the person \ndevelops symptoms, they would be tested and \nprovided with specialist care if they have the \nnovel coronavirus (COVID-19).",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Table 5. **High-resolution Evaluation. **IntenrLM-XComposer2-4KHD has the largest input resolution and outperforms open-source\nLVLMs which are specifically tuned for document understanding.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Table 5. **High-resolution Evaluation. **IntenrLM-XComposer2-4KHD has the largest input resolution and outperforms open-source\nLVLMs which are specifically tuned for document understanding.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "=\nIllustrate the fine details present in the image.<br><br>*<br><br>The image is a vibrant and colorful infographic \nthat provides an overview of 7 graphic design \ntrends that are expected to dominate in 2021. The \ninfographic is divided into 7 sections, each \nrepresenting a different trend.<br><br>At the top, there's a section titled \"Muted Color \nPalettes\", which introduces the concept of muted \ncolors as a trend for 2021. <br><br>This is followed by a section on \"Simple Data \nVisualizations\", highlighting the goal of making \ncomplex data easy to understand.<br><br>The third section focuses on \"Geometric Shapes \nEverywhere\", showcasing how designers are \nusing geometric shapes in their designs. <br><br>The fourth section delves into \"Flat Icons and \nIllustrations\", explaining how brands are using \nflat icons and illustrations in their social media \ngraphics, website design, and more.<br><br>The fifth section introduces \"Classic Serif Fonts\", \ndiscussing how these fonts are seen as classic, \nelegant, and trustworthy. <br><br>The sixth section explores \"Social Media Slide \nDecks\", illustrating how slide decks have become \na visual way of communicating longer messages \non Instagram and LinkedIn.<br><br>Finally, the seventh section discusses \"Text \nHeavy Videos\", predicting that people will likely \ncontinue to work remotely well into 2021, leading \nto an increase in video content that uses text on-\nscreen to communicate messages.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "=\nIllustrate the fine details present in the image.<br><br>*<br><br>The image is a vibrant and colorful infographic \nthat provides an overview of 7 graphic design \ntrends that are expected to dominate in 2021. The \ninfographic is divided into 7 sections, each \nrepresenting a different trend.<br><br>At the top, there's a section titled \"Muted Color \nPalettes\", which introduces the concept of muted \ncolors as a trend for 2021. <br><br>This is followed by a section on \"Simple Data \nVisualizations\", highlighting the goal of making \ncomplex data easy to understand.<br><br>The third section focuses on \"Geometric Shapes \nEverywhere\", showcasing how designers are \nusing geometric shapes in their designs. <br><br>The fourth section delves into \"Flat Icons and \nIllustrations\", explaining how brands are using \nflat icons and illustrations in their social media \ngraphics, website design, and more.<br><br>The fifth section introduces \"Classic Serif Fonts\", \ndiscussing how these fonts are seen as classic, \nelegant, and trustworthy. <br><br>The sixth section explores \"Social Media Slide \nDecks\", illustrating how slide decks have become \na visual way of communicating longer messages \non Instagram and LinkedIn.<br><br>Finally, the seventh section discusses \"Text \nHeavy Videos\", predicting that people will likely \ncontinue to work remotely well into 2021, leading \nto an increase in video content that uses text on-\nscreen to communicate messages.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "**InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model**\n**Handling Resolutions from 336 Pixels to 4K HD**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model**\n**Handling Resolutions from 336 Pixels to 4K HD**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[46] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang,\nFanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-\nmodality model, 2023. 2, 5<br><br>[47] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. *arXiv.org*, 2023. 2<br><br>[48] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-Gemini: Mining the potential of multi-modality\nvision language models. *arXiv preprint arXiv:2403.18814*,\n2024. 2, 5<br><br>[49] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam\nKortylewski, Wufei Ma, Benjamin Van Durme, and Alan L\nYuille. Super-clevr: A virtual benchmark to diagnose do-\nmain robustness in visual reasoning. In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963\u201314973, 2023. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963\u201314973, 2023. 6",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[46] Bo Li, Peiyuan Zhang, Jingkang Yang, Yuanhan Zhang,\nFanyi Pu, and Ziwei Liu. Otterhd: A high-resolution multi-\nmodality model, 2023. 2, 5<br><br>[47] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. *arXiv.org*, 2023. 2<br><br>[48] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng\nZhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya\nJia. Mini-Gemini: Mining the potential of multi-modality\nvision language models. *arXiv preprint arXiv:2403.18814*,\n2024. 2, 5<br><br>[49] Zhuowan Li, Xingrui Wang, Elias Stengel-Eskin, Adam\nKortylewski, Wufei Ma, Benjamin Van Durme, and Alan L\nYuille. Super-clevr: A virtual benchmark to diagnose do-\nmain robustness in visual reasoning. In*Proceedings of*\n*the IEEE/CVF Conference on Computer Vision and Pattern*\n*Recognition*, pages 14963\u201314973, 2023. 6",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[68] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. *arXiv preprint arXiv:2203.10244*, 2022. 1, 2, 6, 7<br><br>[69] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In*Proceedings of the IEEE/CVF Winter Conference on Ap-*\n*plications of Computer Vision*, pages 1697\u20131706, 2022. 1,\n2, 6, 7<br><br>[70] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In*Pro-*\n*ceedings of the IEEE/CVF winter conference on applica-*\n*tions of computer vision*, pages 2200\u20132209, 2021. 1, 2, 6,\n7<br><br>[71] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024. 9",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024. 9",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[68] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning. *arXiv preprint arXiv:2203.10244*, 2022. 1, 2, 6, 7<br><br>[69] Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis\nKaratzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In*Proceedings of the IEEE/CVF Winter Conference on Ap-*\n*plications of Computer Vision*, pages 1697\u20131706, 2022. 1,\n2, 6, 7<br><br>[70] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on document images. In*Pro-*\n*ceedings of the IEEE/CVF winter conference on applica-*\n*tions of computer vision*, pages 2200\u20132209, 2021. 1, 2, 6,\n7<br><br>[71] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier,\nSam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah,\nXianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Meth-\nods, analysis & insights from multimodal llm pre-training. *arXiv preprint arXiv:2403.09611*, 2024. 9",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "=<br><br>If the person does\n|(@)a\nwe would test\nanaes\n\u2014\nthem and provide them with\n=== Gas\n|\nspecialist care if they have\nwe O|e=\n|\nthe novel coronavirus\n\u2014\u2014\u2014_\n(Cow\noch ea",
            {
                "chunk_size": "small"
            }
        ],
        [
            "=<br><br>If the person does\n|(@)a\nwe would test\nanaes\n\u2014\nthem and provide them with\n=== Gas\n|\nspecialist care if they have\nwe O|e=\n|\nthe novel coronavirus\n\u2014\u2014\u2014_\n(Cow\noch ea",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "*The Large Vision-Language Model (LVLM) field has*\n*seen significant advancements, yet its progression has been*\n*hindered by challenges in comprehending fine-grained vi-*\n*sual content due to limited resolution. Recent efforts have*\n*aimed to enhance the high-resolution understanding ca-*\n*pabilities of LVLMs, yet they remain capped at approxi-*\n*mately 1500 \u00d7 1500 pixels and constrained to a relatively*\n*narrow resolution range. This paper represents InternLM-*\n*XComposer2-4KHD, a groundbreaking exploration into el-*\n*evating LVLM resolution capabilities up to 4K HD (3840*\n*\u00d7 1600) and beyond. Concurrently, considering the ultra-*\n*high resolution may not be necessary in all scenarios, it*\n*supports a wide range of diverse resolutions from 336 pix-*\n*els to 4K standard, significantly broadening its scope of ap-*\n*plicability. Specifically, this research advances the patch*\n*division paradigm by introducing a novel extension: dy-*\n*namic resolution with automatic patch configuration. *\n*It*\n*maintains the training image aspect ratios while automati-*\n*cally varying patch counts and configuring layouts based on*\n*a pre-trained Vision Transformer (ViT) (336 \u00d7 336), lead-*\n*ing to dynamic training resolution from 336 pixels to 4K*\n*standard. *\n*Our research demonstrates that scaling train-*\n*ing resolution up to 4K HD leads to consistent perfor-*\n*mance enhancements without hitting the ceiling of poten-*\n*tial improvements. *\n*InternLM-XComposer2-4KHD shows*\n*superb capability that matches or even surpasses GPT-*\n*4V and Gemini Pro in 10 of the 16 benchmarks. *\n*The*\n*InternLM-XComposer2-4KHD model series with 7B pa-*\n*rameters are publicly available at https://github. *\n*com/InternLM/InternLM-XComposer. *<br><br>*indicates equal contribution.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*<br><br>*indicates equal contribution.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*The Large Vision-Language Model (LVLM) field has*\n*seen significant advancements, yet its progression has been*\n*hindered by challenges in comprehending fine-grained vi-*\n*sual content due to limited resolution. Recent efforts have*\n*aimed to enhance the high-resolution understanding ca-*\n*pabilities of LVLMs, yet they remain capped at approxi-*\n*mately 1500 \u00d7 1500 pixels and constrained to a relatively*\n*narrow resolution range. This paper represents InternLM-*\n*XComposer2-4KHD, a groundbreaking exploration into el-*\n*evating LVLM resolution capabilities up to 4K HD (3840*\n*\u00d7 1600) and beyond. Concurrently, considering the ultra-*\n*high resolution may not be necessary in all scenarios, it*\n*supports a wide range of diverse resolutions from 336 pix-*\n*els to 4K standard, significantly broadening its scope of ap-*\n*plicability. Specifically, this research advances the patch*\n*division paradigm by introducing a novel extension: dy-*\n*namic resolution with automatic patch configuration. *\n*It*\n*maintains the training image aspect ratios while automati-*\n*cally varying patch counts and configuring layouts based on*\n*a pre-trained Vision Transformer (ViT) (336 \u00d7 336), lead-*\n*ing to dynamic training resolution from 336 pixels to 4K*\n*standard. *\n*Our research demonstrates that scaling train-*\n*ing resolution up to 4K HD leads to consistent perfor-*\n*mance enhancements without hitting the ceiling of poten-*\n*tial improvements. *\n*InternLM-XComposer2-4KHD shows*\n*superb capability that matches or even surpasses GPT-*\n*4V and Gemini Pro in 10 of the 16 benchmarks. *\n*The*\n*InternLM-XComposer2-4KHD model series with 7B pa-*\n*rameters are publicly available at https://github. *\n*com/InternLM/InternLM-XComposer. *<br><br>*indicates equal contribution.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[95] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models, 2023. 2, 7<br><br>[96] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and\nHouqiang Li. Towards improving document understanding:\nAn exploration on text-grounding via mllms. *arXiv preprint*\n*arXiv:2311.13194*, 2023. 5<br><br>[97] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang. Vary:\nScaling up the vision vocab-\nulary for large vision-language models. *arXiv preprint*\n*arXiv:2312.06109*, 2023. 2, 5<br><br>[98] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, et al. Q-bench: A benchmark for\ngeneral-purpose foundation models on low-level vision. *arXiv preprint arXiv:2309.14181*, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*arXiv preprint arXiv:2309.14181*, 2023. 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[95] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models, 2023. 2, 7<br><br>[96] Yonghui Wang, Wengang Zhou, Hao Feng, Keyi Zhou, and\nHouqiang Li. Towards improving document understanding:\nAn exploration on text-grounding via mllms. *arXiv preprint*\n*arXiv:2311.13194*, 2023. 5<br><br>[97] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang. Vary:\nScaling up the vision vocab-\nulary for large vision-language models. *arXiv preprint*\n*arXiv:2312.06109*, 2023. 2, 5<br><br>[98] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, et al. Q-bench: A benchmark for\ngeneral-purpose foundation models on low-level vision. *arXiv preprint arXiv:2309.14181*, 2023. 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Open-Source\nPrevious SOTA</th><th>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</th></tr>\n<tr><td>Open-Source\nPrevious SOTA</td><td>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Open-Source\nPrevious SOTA</th><th>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</th></tr>\n<tr><td>Open-Source\nPrevious SOTA</td><td>[37] [37] [37] [36] [36] [55] [55] [55] [20] [2] [55] [55] [55] [110] [95] [50]\n8B 8B 8B 18B 18B 35B 35B 35B 40B 34B 35B 35B 35B 8B 17B 10B\n82.2 70.2 44.5 76.1 59.0 52.1 39.0 78.9 51.6 2050.2 81.1 79.0 75.7 64.4 54.5 39.3</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Table 4. **Comparison with open-source SOTA methods. **IXC2-4KHD outperforms competitors in most benchmarks. The best results\nare**bold**and the second-best results are underlined.<br><br>setting yields better results on most OCR-related tasks when\nthe LVLM is trained under the \u2018HD25\u2019 setting. In practice, we jointly train all the components with a\nbatch size of 2048 over 3500 steps. Data from multiple\nsources are sampled in a weighted manner, with the weights\nbased on the number of data from each source. As the \u2018HD-\n55\u2019 setting has double image tokens than the \u2018HD-25\u2019, we\nadjust the data loader to enable different batch sizes for\nthem and adjust their weight accordingly. The maximum\nlearning rate is set to 5*\u00d7*10*\u2212*5, and each component has its\nown unique learning strategy. For the vision encoder, we set\nthe LLDR to 0*. *9, which aligns with the pretraining strategy. For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Table 4. **Comparison with open-source SOTA methods. **IXC2-4KHD outperforms competitors in most benchmarks. The best results\nare**bold**and the second-best results are underlined.<br><br>setting yields better results on most OCR-related tasks when\nthe LVLM is trained under the \u2018HD25\u2019 setting. In practice, we jointly train all the components with a\nbatch size of 2048 over 3500 steps. Data from multiple\nsources are sampled in a weighted manner, with the weights\nbased on the number of data from each source. As the \u2018HD-\n55\u2019 setting has double image tokens than the \u2018HD-25\u2019, we\nadjust the data loader to enable different batch sizes for\nthem and adjust their weight accordingly. The maximum\nlearning rate is set to 5*\u00d7*10*\u2212*5, and each component has its\nown unique learning strategy. For the vision encoder, we set\nthe LLDR to 0*. *9, which aligns with the pretraining strategy. For the LLM, we employ a fixed learning rate scale factor\nof 0*.*2. This slows down the update of the LLM, achieving\na balance between preserving its original capabilities and\naligning it with vision knowledge.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables each may\nreduce your risk for cardiovascular disease, including heart attack and stroke. Examples of a single serving of fruit:\nExamples of a single serving of vegetables:\n\u00a9\n1 medium apple\n\u00a9\n5-8 broccoli florets\n\u00a9\n1small banana\n+\nYa\nlarge sweet potato\n*% medium avocado\n\u00a9\n6 baby carrots\n\u00ab\u00a9 large strawberries\n\u00a9\n1 cup of raw spinach",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Eating 2 to 2% cups (4 to 5 servings) daily of fruits and vegetables each may\nreduce your risk for cardiovascular disease, including heart attack and stroke. Examples of a single serving of fruit:\nExamples of a single serving of vegetables:\n\u00a9\n1 medium apple\n\u00a9\n5-8 broccoli florets\n\u00a9\n1small banana\n+\nYa\nlarge sweet potato\n*% medium avocado\n\u00a9\n6 baby carrots\n\u00ab\u00a9 large strawberries\n\u00a9\n1 cup of raw spinach",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "<table border=\"1\">\n<tr><th>Method</th><th>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</th></tr>\n<tr><td>Method</td><td>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "<table border=\"1\">\n<tr><th>Method</th><th>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</th></tr>\n<tr><td>Method</td><td>Doc Chart Info Text OCR MM Math MMB MMB SEED QBench MM- Hall\nAI2D MMMU MME\nVQA QA VQA VQA Bench Star Vista EN CN Image Test Vet Bench</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "RA a\nos\n@)<br><br>5= 9)<br><br>a\n:\nfe >",
            {
                "chunk_size": "small"
            }
        ],
        [
            "RA a\nos\n@)<br><br>5= 9)<br><br>a\n:\nfe >",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Operations Cost by Department by Month<br><br>160K<br><br>120K",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Operations Cost by Department by Month<br><br>160K<br><br>120K",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "Model\nModel Size\nMax Resolution\nDocVQA*T est*\nChartQA*T est*\nInfoVQA*T est*\nTextVQA*V al*\nOCRBench<br><br>TextMonkey[59]\n9B\n896x896\n73.0\n66.9\n28.6\n65.6\n55.8\nLLaVA-UHD [99]\n13B\n1008x672\n\u2014\n\u2014\n\u2014\n67.7\n\u2014\nCogAgent [36]\n17B\n1024x1024\n81.6\n68.4\n44.5\n76.1\n59.0\nUReader [101]\n7B\n2240x2240\n65.4\n59.3\n42.2\n57.6\n\u2014\nDocOwl 1.5 [37]\n8B\n1344x1344\n82.2\n70.2\n50.7\n68.6\n\u2014<br><br><table border=\"1\">\n<tr><th>IXC2-4KHD</th><th>8B</th><th>3840x1600</th><th>90.0 (+7.8)</th><th>81.0 (+10.8)</th><th>68.6 (+17.9)</th><th>77.2 (+1.2)</th><th>67.5 (+8.5)</th></tr>\n<tr><td>IXC2-4KHD</td><td>8B</td><td>3840x1600</td><td>90.0 (+7.8)</td><td>81.0 (+10.8)</td><td>68.6 (+17.9)</td><td>77.2 (+1.2)</td><td>67.5 (+8.5)</td></tr>\n</table>",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Model\nModel Size\nMax Resolution\nDocVQA*T est*\nChartQA*T est*\nInfoVQA*T est*\nTextVQA*V al*\nOCRBench<br><br>TextMonkey[59]\n9B\n896x896\n73.0\n66.9\n28.6\n65.6\n55.8\nLLaVA-UHD [99]\n13B\n1008x672\n\u2014\n\u2014\n\u2014\n67.7\n\u2014\nCogAgent [36]\n17B\n1024x1024\n81.6\n68.4\n44.5\n76.1\n59.0\nUReader [101]\n7B\n2240x2240\n65.4\n59.3\n42.2\n57.6\n\u2014\nDocOwl 1.5 [37]\n8B\n1344x1344\n82.2\n70.2\n50.7\n68.6\n\u2014<br><br><table border=\"1\">\n<tr><th>IXC2-4KHD</th><th>8B</th><th>3840x1600</th><th>90.0 (+7.8)</th><th>81.0 (+10.8)</th><th>68.6 (+17.9)</th><th>77.2 (+1.2)</th><th>67.5 (+8.5)</th></tr>\n<tr><td>IXC2-4KHD</td><td>8B</td><td>3840x1600</td><td>90.0 (+7.8)</td><td>81.0 (+10.8)</td><td>68.6 (+17.9)</td><td>77.2 (+1.2)</td><td>67.5 (+8.5)</td></tr>\n</table>",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[6] Baichuan. Baichuan 2: Open large-scale language models. *arXiv.org*, 2023. 2<br><br>[7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa \u0306gnak Tas \u0327\u0131rlar. Introducing our multimodal models, 2023. 2<br><br>[8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc \u0327al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\n*Proceedings of the IEEE/CVF international conference on*\n*computer vision*, pages 4291\u20134301, 2019. 6<br><br>[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877\u2013\n1901, 2020. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877\u2013\n1901, 2020. 2",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[6] Baichuan. Baichuan 2: Open large-scale language models. *arXiv.org*, 2023. 2<br><br>[7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa \u0306gnak Tas \u0327\u0131rlar. Introducing our multimodal models, 2023. 2<br><br>[8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez,\nMarc \u0327al Rusinol, Ernest Valveny, CV Jawahar, and Dimos-\nthenis Karatzas. Scene text visual question answering. In\n*Proceedings of the IEEE/CVF international conference on*\n*computer vision*, pages 4291\u20134301, 2019. 6<br><br>[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. *Advances in Neu-*\n*ral Information Processing Systems (NeurIPS)*, 33:1877\u2013\n1901, 2020. 2",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. *arXiv.org*, 2023. 6<br><br>[57] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player? *arXiv:2307.06281*, 2023. 1, 5, 7<br><br>[58] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\nYin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the\nhidden mystery of ocr in large multimodal models, 2024. 2,\n7<br><br>[59] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,\nShuo Zhang, and Xiang Bai. Textmonkey: An ocr-free\nlarge multimodal model for understanding document. *arXiv*\n*preprint arXiv:2403.04473*, 2024. 2, 5, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "*arXiv*\n*preprint arXiv:2403.04473*, 2024. 2, 5, 8",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[56] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. *arXiv.org*, 2023. 6<br><br>[57] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player? *arXiv:2307.06281*, 2023. 1, 5, 7<br><br>[58] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng\nYin, Cheng lin Liu, Lianwen Jin, and Xiang Bai. On the\nhidden mystery of ocr in large multimodal models, 2024. 2,\n7<br><br>[59] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma,\nShuo Zhang, and Xiang Bai. Textmonkey: An ocr-free\nlarge multimodal model for understanding document. *arXiv*\n*preprint arXiv:2403.04473*, 2024. 2, 5, 8",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283109,
        "modified_at": 1732283109,
        "owner": null,
        "path": "../test/InternLM-4kimages.pdf",
        "seen_at": 1733134124
    },
    "rets": [
        [
            "[108] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\nZhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130b: An open bilingual pre-\ntrained model. In*The Eleventh International Conference*\n*on Learning Representations (ICLR)*, 2023. 2<br><br>[109] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang,\nand Jiaqi Wang. Long-CLIP: Unlocking the long-text capa-\nbility of clip. *arXiv preprint arXiv:2403.15378*, 2024. 2<br><br>[110] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al. Internlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. *arXiv preprint*\n*arXiv:2309.15112*, 2023. 2, 6, 7",
            {
                "chunk_size": "small"
            }
        ],
        [
            "[108] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\nZhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130b: An open bilingual pre-\ntrained model. In*The Eleventh International Conference*\n*on Learning Representations (ICLR)*, 2023. 2<br><br>[109] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang,\nand Jiaqi Wang. Long-CLIP: Unlocking the long-text capa-\nbility of clip. *arXiv preprint arXiv:2403.15378*, 2024. 2<br><br>[110] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al. Internlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. *arXiv preprint*\n*arXiv:2309.15112*, 2023. 2, 6, 7",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Once a firm has decided the investment project it wants to \nundertake, it has to figure out ways and means of financing them.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Once a firm has decided the investment project it wants to \nundertake, it has to figure out ways and means of financing them.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**4. Relationship to Production Department :**<br><br>Any decision of production department will affect the financial\nposition of the firm.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**4. Relationship to Production Department :**<br><br>Any decision of production department will affect the financial\nposition of the firm.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**FUNCTIONS OF FINANCE**<br><br>The functions of finance involve three major decisions a company \nmust make \u2013 the investment decisions, the financing decisions, \nand the dividend / share repurchase decisions.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**FUNCTIONS OF FINANCE**<br><br>The functions of finance involve three major decisions a company \nmust make \u2013 the investment decisions, the financing decisions, \nand the dividend / share repurchase decisions.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Thus profit maximisation fails to be an operationally feasible \nobjective of financial management.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Thus profit maximisation fails to be an operationally feasible \nobjective of financial management.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Because of difference in interest between owners (shareholders)\nand  managers, chances of conflict is there.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Because of difference in interest between owners (shareholders)\nand  managers, chances of conflict is there.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- current assets (inventories, debtors, short term holdings<br><br>of marketable securities and cash)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- current assets (inventories, debtors, short term holdings<br><br>of marketable securities and cash)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**2.Transitional Phase (1940 \u2013 1950)**<br><br>Here though the nature of financial management was similar to\ntraditional phase but greater emphasis was placed on day-to-day\nactivities.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**2.Transitional Phase (1940 \u2013 1950)**<br><br>Here though the nature of financial management was similar to\ntraditional phase but greater emphasis was placed on day-to-day\nactivities.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Related to day-to-day financing activities and deals with \u2013",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Related to day-to-day financing activities and deals with \u2013",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- it should be clear and unambiguous<br><br>- time frame should be there to evaluate success and failure<br><br>of any decision",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- it should be clear and unambiguous<br><br>- time frame should be there to evaluate success and failure<br><br>of any decision",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**1. Relationship to Economics :**<br><br>There are two important linkages between economics & finance -",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. Relationship to Economics :**<br><br>There are two important linkages between economics & finance -",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Specifically  it is defined as procurement of funds and their \neffective utilisation.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Specifically  it is defined as procurement of funds and their \neffective utilisation.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "The primary goal of finance is to create shareholder value by \ninvesting in positive NPV projects and minimising the cost of \nfinancing.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The primary goal of finance is to create shareholder value by \ninvesting in positive NPV projects and minimising the cost of \nfinancing.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Price of a product can only be set after consultation with finance \nmanager.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Price of a product can only be set after consultation with finance \nmanager.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**2. Maximisation of Shareholder\u2019s Wealth :**<br><br>This objective is generally expressed in terms of maximisation of\na value of a share of the firm.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**2. Maximisation of Shareholder\u2019s Wealth :**<br><br>This objective is generally expressed in terms of maximisation of\na value of a share of the firm.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**CONFLICT OF GOAL BETWEEN MANAGEMENT AND**\n**OWNERS : AGENCY PROBLEMS**<br><br>In companies, owners and management are separated from each \nother.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**CONFLICT OF GOAL BETWEEN MANAGEMENT AND**\n**OWNERS : AGENCY PROBLEMS**<br><br>In companies, owners and management are separated from each \nother.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "A clear understanding of objective is very essential because it \nprovides a frame work for optimum financial decision making.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "A clear understanding of objective is very essential because it \nprovides a frame work for optimum financial decision making.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- there should be an efficient capital market wherein the <br><br>effect of a decision is truly reflected in the market price\nof the share.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- there should be an efficient capital market wherein the <br><br>effect of a decision is truly reflected in the market price\nof the share.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**EVOLUTION OF FINANCE**<br><br>It may be divided into three broad categories, i.e., traditional \nphase, transitional phase and modern phase.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**EVOLUTION OF FINANCE**<br><br>It may be divided into three broad categories, i.e., traditional \nphase, transitional phase and modern phase.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "The primary objective of accounting is to measure the performance\nof the firm, assess its financial condition, and determine the base\nfor tax payment.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The primary objective of accounting is to measure the performance\nof the firm, assess its financial condition, and determine the base\nfor tax payment.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "All business decisions have financial implications and therefore\nfinancial management is inevitably related with every aspect of \nbusiness operations.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "All business decisions have financial implications and therefore\nfinancial management is inevitably related with every aspect of \nbusiness operations.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Characteristics of this phase were :<br><br>- finance function was**episodic**in nature.<br><br>- funds were arranged mainly from financial institutions or<br><br>through shares/ debentures.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Characteristics of this phase were :<br><br>- finance function was**episodic**in nature.<br><br>- funds were arranged mainly from financial institutions or<br><br>through shares/ debentures.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- it should be consistent with the long term objective of the <br><br>firm",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- it should be consistent with the long term objective of the <br><br>firm",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**Cost Acc**<br><br>**Manager**<br><br>**Data Proces**<br><br>**Manager**",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**Cost Acc**<br><br>**Manager**<br><br>**Data Proces**<br><br>**Manager**",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**1. Maximisation of the profit of the Firm :**<br><br>It is often considered as the implied objective.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. Maximisation of the profit of the Firm :**<br><br>It is often considered as the implied objective.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- the market price of shares should not be influenced by<br><br>speculative activities.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- the market price of shares should not be influenced by<br><br>speculative activities.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "It also leads to the welfare of the society. Hence it will result in \nefficient allocation of resources not only from the point of view of\nthe firm but also for the society.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "It also leads to the welfare of the society. Hence it will result in \nefficient allocation of resources not only from the point of view of\nthe firm but also for the society.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**1.The Traditional Phase (Up to 1940)**<br><br>Initially finance was a part of economic activities and business \nowners were more concerned with the operational activities.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1.The Traditional Phase (Up to 1940)**<br><br>Initially finance was a part of economic activities and business \nowners were more concerned with the operational activities.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "The final decision will depend upon the preference of the \nshareholders and investment opportunities available before the\nfirm.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The final decision will depend upon the preference of the \nshareholders and investment opportunities available before the\nfirm.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**FINANCIAL DECISIONS IN A FIRM**<br><br>The decisions of raising funds  investing them in assets and distributing returns \nearned from assets  to shareholders are respectively known as financing decision, \ninvestment decisions and dividend decision.<br><br>A firm attempts to balance cash inflows and outflows while performing these \ndecisions.<br><br>There are three broad areas of financial decision making \u2013 capital\nbudgeting, capital structure and working capital management.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**FINANCIAL DECISIONS IN A FIRM**<br><br>The decisions of raising funds  investing them in assets and distributing returns \nearned from assets  to shareholders are respectively known as financing decision, \ninvestment decisions and dividend decision.<br><br>A firm attempts to balance cash inflows and outflows while performing these \ndecisions.<br><br>There are three broad areas of financial decision making \u2013 capital\nbudgeting, capital structure and working capital management.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Management of a company is consisting of professionals who are \nmore qualified and having technical expertise to run the business.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Management of a company is consisting of professionals who are \nmore qualified and having technical expertise to run the business.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "The decisions regarding long term assets are known as**capital**\n**budgeting**and regarding short term assets as**working capital**\n**management**.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The decisions regarding long term assets are known as**capital**\n**budgeting**and regarding short term assets as**working capital**\n**management**.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**5. Relationship to Personnel Management :**<br><br>The recruitment, training and placement of staff is the \nresponsibility of personnel department and all of these requires\nfinance. Thus the decision regarding these aspects can not be \ntaken in isolation of finance department.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**5. Relationship to Personnel Management :**<br><br>The recruitment, training and placement of staff is the \nresponsibility of personnel department and all of these requires\nfinance. Thus the decision regarding these aspects can not be \ntaken in isolation of finance department.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "With this objective, the management will allocate the resources\nin the best possible ways within the given constraints of risk.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "With this objective, the management will allocate the resources\nin the best possible ways within the given constraints of risk.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "CFO supervises the work of treasurer and controller.<br><br>Treasurer \u2013 obtaining finance, banking relationship, cash<br><br>management, credit admin, C.B.<br><br>Controller \u2013 financial accounting, internal auditing, taxation,<br><br>management accounting and control",
            {
                "chunk_size": "small"
            }
        ],
        [
            "CFO supervises the work of treasurer and controller.<br><br>Treasurer \u2013 obtaining finance, banking relationship, cash<br><br>management, credit admin, C.B.<br><br>Controller \u2013 financial accounting, internal auditing, taxation,<br><br>management accounting and control",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- what should be the optimal debt-equity ratio?<br><br>- which specific instruments should be employed?<br><br>- which capital market should the firm access?<br><br>- when should the firm raise finances?<br><br>- what price the firm should offer its securities?",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- what should be the optimal debt-equity ratio?<br><br>- which specific instruments should be employed?<br><br>- which capital market should the firm access?<br><br>- when should the firm raise finances?<br><br>- what price the firm should offer its securities?",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**2. Capital Structure :**<br><br>It involves determining the best mix of debt, equity and hybrid \nsecurities to employ.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**2. Capital Structure :**<br><br>It involves determining the best mix of debt, equity and hybrid \nsecurities to employ.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Therefore economic value of shareholders wealth is the market\nprice of the share.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Therefore economic value of shareholders wealth is the market\nprice of the share.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**ii) short term assets \u2013**those assets which in normal course of <br><br>business are convertible into cash\nwithout any loss in value, usually within\na year.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**ii) short term assets \u2013**those assets which in normal course of <br><br>business are convertible into cash\nwithout any loss in value, usually within\na year.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**2. Accrual Method Vs Cash Flow Method**<br><br>Accounting is mainly concerned with the cash-flow while finance is\nconcerned about magnitude, timing and risk of cash-flows.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**2. Accrual Method Vs Cash Flow Method**<br><br>Accounting is mainly concerned with the cash-flow while finance is\nconcerned about magnitude, timing and risk of cash-flows.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Major characteristics of modern phase are :<br><br>- the firm (i.e., insider) is more important<br><br>- rational matching of funds to their uses so as to maximise<br><br>the wealth of the current shareholders.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Major characteristics of modern phase are :<br><br>- the firm (i.e., insider) is more important<br><br>- rational matching of funds to their uses so as to maximise<br><br>the wealth of the current shareholders.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**1. Record-Keeping Vs Value Maximising**<br><br>Accounting is mainly concerned with record keeping, whereas \nfinance is aimed at value maximising.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. Record-Keeping Vs Value Maximising**<br><br>Accounting is mainly concerned with record keeping, whereas \nfinance is aimed at value maximising.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Thus, all financial decisions are evaluated in terms of the firm\u2019s \nfuture cash flows.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Thus, all financial decisions are evaluated in terms of the firm\u2019s \nfuture cash flows.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- micro economic theory provides the conceptual <br><br>understanding of the tools of financial decision making.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- micro economic theory provides the conceptual <br><br>understanding of the tools of financial decision making.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "The titles used to designate the key finance officer are also differs\nfrom company to company like \u2013**Vice President (Finance),**\n**Chief Executive (Finance), General Manager (Finance),**\n**Director (Finance), Chief Finance Officer (CFO)**.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The titles used to designate the key finance officer are also differs\nfrom company to company like \u2013**Vice President (Finance),**\n**Chief Executive (Finance), General Manager (Finance),**\n**Director (Finance), Chief Finance Officer (CFO)**.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**GOALS/OBJECTIVES OF FINANCIAL MANAGEMENT**<br><br>A goal of a firm may be defined as a target against which the firm\u2019s\noperating performance can be measured.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**GOALS/OBJECTIVES OF FINANCIAL MANAGEMENT**<br><br>A goal of a firm may be defined as a target against which the firm\u2019s\noperating performance can be measured.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Besides these \u2013 what should be the optimal dividend payout ratio?",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Besides these \u2013 what should be the optimal dividend payout ratio?",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "It differs from company to company depending on their respective \nneeds.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "It differs from company to company depending on their respective \nneeds.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- current liabilities (short-term debt, trade creditors, <br><br>accruals and provisions)",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- current liabilities (short-term debt, trade creditors, <br><br>accruals and provisions)",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Funds analysis and control on a regular basis, rather than on a \ncasual basis.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Funds analysis and control on a regular basis, rather than on a \ncasual basis.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**1. The Investment Decisions :**<br><br>Capital investment is the allocation of capital to investment \nproposals whose benefits are to be realized in the future.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. The Investment Decisions :**<br><br>Capital investment is the allocation of capital to investment \nproposals whose benefits are to be realized in the future.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**ORGANISATION OF THE FINANCE FUNCTION**<br><br>A firm should give proper attention to the structure and \norganisation of its finance department.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**ORGANISATION OF THE FINANCE FUNCTION**<br><br>A firm should give proper attention to the structure and \norganisation of its finance department.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**3. Dividend Policy Decisions :**<br><br>The dividend should be analysed in relation to the financing \ndecision of a firm.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3. Dividend Policy Decisions :**<br><br>The dividend should be analysed in relation to the financing \ndecision of a firm.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**2. Financing Decisions :**<br><br>It relates to the choice of the proportion of sources to finance the\nselected investment proposals.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**2. Financing Decisions :**<br><br>It relates to the choice of the proportion of sources to finance the\nselected investment proposals.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**2. Relationship to Accounting :**<br><br>They are closely related but differ with each other primarily in \nthree aspects -",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**2. Relationship to Accounting :**<br><br>They are closely related but differ with each other primarily in \nthree aspects -",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "The market price of share reflects its present value.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "The market price of share reflects its present value.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**3. Relationship to Marketing :**<br><br>Marketing is one of the most important area on which the success\nor failure of a firm depends.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3. Relationship to Marketing :**<br><br>Marketing is one of the most important area on which the success\nor failure of a firm depends.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**3. Certainty Vs Uncertainty**<br><br>Accounting deals with the past data hence it is relatively certain\nwhile finance is concerned mainly with the future hence it is having\na high degree of uncertainty.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3. Certainty Vs Uncertainty**<br><br>Accounting deals with the past data hence it is relatively certain\nwhile finance is concerned mainly with the future hence it is having\na high degree of uncertainty.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**INTRODUCTION TO FINANCE**<br><br>In general, finance is defined as the provision of money at the time\nIt is required.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**INTRODUCTION TO FINANCE**<br><br>In general, finance is defined as the provision of money at the time\nIt is required.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Two alternatives are available in dealing with the profits of the \nfirm \u2013 they can be distributed to the shareholders in the form of \ndividends or they can be retained in the business itself.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Two alternatives are available in dealing with the profits of the \nfirm \u2013 they can be distributed to the shareholders in the form of \ndividends or they can be retained in the business itself.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**3.The Modern Phase (After 1950)**<br><br>Started since mid 1950\u2019s.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3.The Modern Phase (After 1950)**<br><br>Started since mid 1950\u2019s.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Financial management is defined as the management of flow of\nfunds in a firm.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Financial management is defined as the management of flow of\nfunds in a firm.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**1. Capital Budgeting :**<br><br>Means allocation of funds to different long term assets like \ninvesting in lands, machineries, infrastructures, distribution \nnetworks etc.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**1. Capital Budgeting :**<br><br>Means allocation of funds to different long term assets like \ninvesting in lands, machineries, infrastructures, distribution \nnetworks etc.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "Various types of financial decisions be taken with a view to \nmaximise the profit of the firm.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "Various types of financial decisions be taken with a view to \nmaximise the profit of the firm.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**3. Working Capital Management :**<br><br>It is also referred as**short-term financing decisions**.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**3. Working Capital Management :**<br><br>It is also referred as**short-term financing decisions**.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "To mitigate agency problem, effective monitoring has to be done\nand proper incentives have to be offered.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "To mitigate agency problem, effective monitoring has to be done\nand proper incentives have to be offered.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "But wealth maximisation is considered as superior objective than\nthat of profit maximisation.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "But wealth maximisation is considered as superior objective than\nthat of profit maximisation.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "In this case the objective is to minimise the cost of financing \nwithout impairing the ability of the firm to raise finances.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "In this case the objective is to minimise the cost of financing \nwithout impairing the ability of the firm to raise finances.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "**i) long term assets \u2013**which yield a return over a period of time<br><br>in future.",
            {
                "chunk_size": "small"
            }
        ],
        [
            "**i) long term assets \u2013**which yield a return over a period of time<br><br>in future.",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/finance.pdf",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "- macro economic environment defines the setting within <br><br>which a firm operates, and",
            {
                "chunk_size": "small"
            }
        ],
        [
            "- macro economic environment defines the setting within <br><br>which a firm operates, and",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
{
    "metadata": {
        "created_at": 1732283451,
        "modified_at": 1732283451,
        "owner": null,
        "path": "data/input.txt",
        "seen_at": 1733136016
    },
    "rets": [
        [
            "\"content\" : \"numpy is a math library in python, you can do a bunch of operations in it and use this for making python faster for arrays too. \"",
            {
                "chunk_size": "small"
            }
        ],
        [
            "\"content\" : \"numpy is a math library in python, you can do a bunch of operations in it and use this for making python faster for arrays too. \"",
            {
                "chunk_size": "large"
            }
        ]
    ]
}
